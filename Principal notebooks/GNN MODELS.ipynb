{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le fichier CSV\n",
    "file_path = \"data/istio_request_2.2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df['grpc_response_status'].fillna(0, inplace=True)\n",
    "\n",
    "df['response_flags'] = df['response_flags'].astype(str).str.strip()  # Convertir en string et enlever espaces\n",
    "\n",
    "# Ajouter une colonne 'result' avec 'success' ou 'error'\n",
    "df['result'] = df.apply(\n",
    "    lambda row: 'success' if row['response_code'] == 200 and row['grpc_response_status'] == 0 and row['response_flags'] == '-' else 'error',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Réorganiser les données par 'source_workload', 'destination_workload' et 'timestamp'\n",
    "df_sorted = df.sort_values(by=['source_workload', 'destination_workload', 'timestamp'])\n",
    "\n",
    "# todas las solicitudes entre cada par origen-destino están ordenadas en el tiempo.\n",
    "# Esto es útil para cálculos secuenciales, como determinar el número de solicitudes\n",
    "# en un período de tiempo o calcular métricas basadas en la secuencia temporal de eventos.\n",
    "\n",
    "# Sauvegarder le fichier résultant\n",
    "df_sorted.to_csv(\"results sahra2/aggregated_istio_data.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Charger le fichier\n",
    "file_path = \"results sahra2/aggregated_istio_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convertir timestamp en datetime pour le tri\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Trier avant la séparation\n",
    "df.sort_values(by=['source_workload', 'destination_workload', 'timestamp'], inplace=True)\n",
    "\n",
    "# Séparer les succès\n",
    "df_success = df[df['result'] == 'success'].copy()\n",
    "#Separamos para obtener solo las solicitudes exitosas\n",
    "\n",
    "# Calculer new_request, new_istio_request_bytes et new_istio_request_duration_milliseconds avec 0 pour la première ligne\n",
    "df_success['new_request'] = df_success.groupby(['source_workload', 'destination_workload'])['total_request'].diff().fillna(0)\n",
    "df_success['new_istio_request_bytes'] = df_success.groupby(['source_workload', 'destination_workload'])['istio_request_bytes_sum'].diff().fillna(0)\n",
    "df_success['new_istio_request_duration_milliseconds'] = df_success.groupby(['source_workload', 'destination_workload'])['istio_request_duration_milliseconds_sum'].diff().fillna(0)\n",
    "\n",
    "# diff() calcula la diferencia entre una fila y la anterior en cada grupo de source_workload y destination_workload.\n",
    "# Para la primera fila de cada grupo, no hay una fila anterior, así que diff() devuelve NaN.\n",
    "# Con fillna(0) reemplaza esos NaN con 0, indicando que no hubo solicitudes nuevas en ese primer intervalo.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Appliquer la condition si new_request == 0\n",
    "df_success.loc[df_success['new_request'] == 0, ['new_istio_request_bytes', 'new_istio_request_duration_milliseconds']] = 0\n",
    "#Si no hubo nuevas solicitudes en un intervalo, asegura que los bytes y la duración también se registren como cero.\n",
    "\n",
    "\n",
    "\n",
    "# Calculer latency\n",
    "df_success['latency'] = df_success['new_istio_request_duration_milliseconds'] / df_success['new_request']\n",
    "df_success['latency'].fillna(0, inplace=True)\n",
    "# Calcula la latencia dividiendo el tiempo total de solicitudes por el número de solicitudes nuevas en cada intervalo.\n",
    "\n",
    "# Sauvegarder les succès dans un fichier\n",
    "df_success.to_csv(\"results sahra2/success_istio_data.csv\", index=False)\n",
    "# Si no hubo nuevas solicitudes, la latencia se rellena con cero\n",
    "\n",
    "# Séparer les erreurs HTTP et gRPC\n",
    "df_http_errors = df[(df['result'] == 'error') & (df['request_protocol'] == 'http')].copy()\n",
    "df_grpc_errors = df[(df['result'] == 'error') & (df['request_protocol'] == 'grpc')].copy()\n",
    "\n",
    "error_files = []  # Liste des fichiers d'erreur générés\n",
    "\n",
    "# Traitement des erreurs HTTP\n",
    "http_groups = df_http_errors.groupby(['request_protocol', 'response_code', 'grpc_response_status', 'response_flags'])\n",
    "\n",
    "for (request_protocol, response_code, grpc_status, response_flags), df_error in http_groups:\n",
    "    df_error = df_error.copy()\n",
    "    \n",
    "    # Calculer new_request, new_istio_request_bytes et new_istio_request_duration_milliseconds avec 0 pour la première ligne\n",
    "    df_error['new_request'] = df_error.groupby(['source_workload', 'destination_workload'])['total_request'].diff().fillna(0)\n",
    "    df_error['new_istio_request_bytes'] = df_error.groupby(['source_workload', 'destination_workload'])['istio_request_bytes_sum'].diff().fillna(0)\n",
    "    df_error['new_istio_request_duration_milliseconds'] = df_error.groupby(['source_workload', 'destination_workload'])['istio_request_duration_milliseconds_sum'].diff().fillna(0)\n",
    "    \n",
    "    # Appliquer la condition si new_request == 0\n",
    "    df_error.loc[df_error['new_request'] == 0, ['new_istio_request_bytes', 'new_istio_request_duration_milliseconds']] = 0\n",
    "    \n",
    "    # Calculer latency\n",
    "    df_error['latency'] = df_error['new_istio_request_duration_milliseconds'] / df_error['new_request']\n",
    "    df_error['latency'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Nommer le fichier selon l'erreur\n",
    "    file_name = f\"results sahra2/error_{request_protocol}_{response_code}_{response_flags}.csv\"\n",
    "    df_error.to_csv(file_name, index=False)\n",
    "    error_files.append(df_error)\n",
    "\n",
    "# Traitement des erreurs gRPC\n",
    "grpc_groups = df_grpc_errors.groupby(['request_protocol', 'response_code', 'grpc_response_status', 'response_flags'])\n",
    "\n",
    "for (request_protocol, response_code, grpc_status, response_flags), df_error in grpc_groups:\n",
    "    df_error = df_error.copy()\n",
    "    \n",
    "    # Calculer new_request, new_istio_request_bytes et new_istio_request_duration_milliseconds avec 0 pour la première ligne\n",
    "    df_error['new_request'] = df_error.groupby(['source_workload', 'destination_workload'])['total_request'].diff().fillna(0)\n",
    "    df_error['new_istio_request_bytes'] = df_error.groupby(['source_workload', 'destination_workload'])['istio_request_bytes_sum'].diff().fillna(0)\n",
    "    df_error['new_istio_request_duration_milliseconds'] = df_error.groupby(['source_workload', 'destination_workload'])['istio_request_duration_milliseconds_sum'].diff().fillna(0)\n",
    "    \n",
    "    # Appliquer la condition si new_request == 0\n",
    "    df_error.loc[df_error['new_request'] == 0, ['new_istio_request_bytes', 'new_istio_request_duration_milliseconds']] = 0\n",
    "    \n",
    "    # Calculer latency\n",
    "    df_error['latency'] = df_error['new_istio_request_duration_milliseconds'] / df_error['new_request']\n",
    "    df_error['latency'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Nommer le fichier selon l'erreur\n",
    "    file_name = f\"results sahra2/error_{request_protocol}_{response_code}_{grpc_status}_{response_flags}.csv\"\n",
    "    df_error.to_csv(file_name, index=False)\n",
    "    error_files.append(df_error)\n",
    "\n",
    "# Fusionner tous les fichiers (success + errors)\n",
    "df_final = pd.concat([df_success] + error_files).sort_values(by=['source_workload', 'destination_workload', 'timestamp'])\n",
    "\n",
    "# Sauvegarder le fichier final\n",
    "df_final.to_csv(\"results sahra2/new_request_istio_data.csv\", index=False)\n",
    "\n",
    "print(\"Traitement terminé. Fichier sauvegardé sous 'new_request_istio_data.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "\n",
    "# Charger le fichier\n",
    "file_path = \"results sahra2/new_request_istio_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convertir timestamp en datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Trier les données\n",
    "df.sort_values(by=['source_workload', 'destination_workload', 'timestamp'], inplace=True)\n",
    "\n",
    "# Calculer success rate, error rate, success count, error count, duration success request, duration error request et average latency par timestamp\n",
    "grouped = df.groupby(['source_workload', 'destination_workload', 'timestamp'])\n",
    "aggregated_rows = []\n",
    "\n",
    "for (src, dst, ts), group in grouped:\n",
    "    total_new_request = group['new_request'].sum()\n",
    "    success_count = group[group['result'] == 'success']['new_request'].sum()\n",
    "    error_count = total_new_request - success_count\n",
    "    \n",
    "    if total_new_request > 0:\n",
    "        success_rate = success_count / total_new_request\n",
    "        error_rate = 1 - success_rate\n",
    "    else:\n",
    "        success_rate = float('nan')\n",
    "        error_rate = float('nan')\n",
    "    \n",
    "    # Calculer la durée des requêtes réussies et erronées\n",
    "    duration_success_request = group[group['result'] == 'success']['latency'].sum()\n",
    "    duration_error_request = group[group['result'] == 'error']['latency'].sum()\n",
    "    average_latency = duration_success_request + duration_error_request\n",
    "\n",
    "    # Séparer new_istio_request_bytes en success et error\n",
    "    new_istio_request_bytes_success = group[group['result'] == 'success']['new_istio_request_bytes'].sum()\n",
    "    new_istio_request_bytes_error = group[group['result'] == 'error']['new_istio_request_bytes'].sum()\n",
    "    istio_request_bytes = new_istio_request_bytes_success+new_istio_request_bytes_error\n",
    "    aggregated_rows.append([ts, src, dst, group['total_request'].max(), total_new_request, success_count, error_count, success_rate, error_rate, duration_success_request, duration_error_request, average_latency, new_istio_request_bytes_success, new_istio_request_bytes_error,istio_request_bytes])\n",
    "\n",
    "# Créer un DataFrame final\n",
    "df_final = pd.DataFrame(aggregated_rows, columns=['timestamp', 'source_workload', 'destination_workload', 'total_request', 'new_request', 'success_count', 'error_count', 'success_rate', 'error_rate', 'duration_success_request', 'duration_error_request', 'average_latency', 'new_istio_request_bytes_success', 'new_istio_request_bytes_error','istio_request_bytes'])\n",
    "\n",
    "# Sauvegarder le fichier\n",
    "output_file = \"results sahra2/aggregated_istio_rates.csv\"\n",
    "df_final.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Traitement terminé. Fichier sauvegardé sous {output_file}.\") #Procesamiento completado. Archivo guardado como ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger le fichier\n",
    "file_path = \"results sahra2/aggregated_istio_rates.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convertir timestamp en datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Définir les fenêtres de temps\n",
    "time_windows = ['15S', '30S', '1min', '5min', '10min']\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les résultats\n",
    "kpi_results = []\n",
    "\n",
    "for window in time_windows:\n",
    "    df_resampled = (df\n",
    "        .groupby(['source_workload', 'destination_workload'])  # Regrouper par workload\n",
    "        .resample(window, on='timestamp', label='right', closed='right')  # Aligner sur la fin de la fenêtre\n",
    "        .agg({\n",
    "            'total_request': 'max',\n",
    "            'new_request': 'sum',\n",
    "            'success_count': 'sum',\n",
    "            'error_count': 'sum',\n",
    "            'success_rate': 'mean',\n",
    "            'error_rate': 'mean',\n",
    "            'average_latency': 'sum',\n",
    "            'istio_request_bytes': 'sum'\n",
    "        })\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Calcul du throughput\n",
    "    df_resampled['throughput'] = df_resampled['istio_request_bytes'] / pd.to_timedelta(window).total_seconds()\n",
    "    \n",
    "    # Calcul du request rate\n",
    "    df_resampled['request_rate'] = df_resampled['new_request'] / pd.to_timedelta(window).total_seconds()\n",
    "    \n",
    "    df_resampled['time_window'] = window\n",
    "    kpi_results.append(df_resampled)\n",
    "\n",
    "# Concaténer tous les résultats\n",
    "df_final = pd.concat(kpi_results)\n",
    "\n",
    "# Supprimer la ligne où timestamp == \"2025-03-10 16:09:00\"\n",
    "starting_point = pd.Timestamp(\"2025-03-10 16:09:00\")\n",
    "df_final = df_final[df_final['timestamp'] != starting_point]\n",
    "\n",
    "# Sauvegarder dans un fichier CSV\n",
    "df_final.to_csv(\"results sahra2/kiali_kpi_metrics.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Charger le fichier\n",
    "df = pd.read_csv(\"results sahra2/kiali_kpi_metrics.csv\")\n",
    "\n",
    "# Convertir timestamp en datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Définir les fenêtres de temps et leurs intervalles respectifs\n",
    "interval_mapping = {\n",
    "    '15S': '1min',  # 15 secondes -> 1 minute\n",
    "    '30S': '2min',  # 30 secondes -> 2 minutes\n",
    "    '1min': '4min',  # 1 minute -> 4 minutes\n",
    "    '5min': '10min',  # 5 minutes -> 10 minutes\n",
    "    '10min': '10min'  # 10 minutes -> 10 minutes\n",
    "}\n",
    "\n",
    "latency_results = []\n",
    "\n",
    "for window, interval in interval_mapping.items():\n",
    "    df_filtered = df[df['time_window'] == window].copy()\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No data for time window: {window}\")\n",
    "        continue\n",
    "    \n",
    "    df_filtered.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    try:\n",
    "        df_grouped = (df_filtered.groupby(['source_workload', 'destination_workload'])\n",
    "                      .resample(interval)\n",
    "                      .agg({col: list for col in df.columns if col not in ['timestamp', 'source_workload', 'destination_workload', 'time_window']})\n",
    "                      .reset_index())\n",
    "    except Exception as e:\n",
    "        print(f\"Error during resampling for window {window}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    for idx, row in df_grouped.iterrows():\n",
    "        values = row.get('average_latency', [])\n",
    "        frequencies = row.get('new_request', [])\n",
    "        \n",
    "        if not values or not frequencies or len(values) != len(frequencies):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            data = np.repeat(values, frequencies)\n",
    "            if data.size == 0:\n",
    "                continue\n",
    "            \n",
    "            percentiles = {\n",
    "                'p50_latency': np.percentile(data, 50),\n",
    "                'p90_latency': np.percentile(data, 90),\n",
    "                'p95_latency': np.percentile(data, 95),\n",
    "                'p99_latency': np.percentile(data, 99)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating percentiles for row {idx} in window {window}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        result = {**row.to_dict(), **percentiles, 'time_window': window}\n",
    "        latency_results.append(result)\n",
    "\n",
    "# Créer un DataFrame final\n",
    "df_latency = pd.DataFrame(latency_results)\n",
    "\n",
    "# Sauvegarder dans un fichier CSV\n",
    "df_latency.to_csv(\"results sahra2/kiali_latency_percentiles.csv\", index=False)\n",
    "\n",
    "print(\"Traitement terminé. Fichier sauvegardé sous kiali_latency_percentiles.csv.\") #Procesamiento completado. Archivo guardado como ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"results sahra2/kiali_kpi_metrics.csv\"  # Updated file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "# Define function to classify edge color based on success_rate\n",
    "def get_edge_color(success_rate):\n",
    "    if (success_rate > 0.95):\n",
    "        return \"green\"\n",
    "    elif (success_rate > 0.80):\n",
    "        return \"yellow\"\n",
    "    else:\n",
    "        return \"red\"\n",
    "\n",
    "# Define time windows\n",
    "start_date = data['timestamp'].min()\n",
    "end_date = data['timestamp'].max()\n",
    "window_sizes = [\"1T\", \"5T\", \"10T\", \"30T\"]  # Added more time windows\n",
    "\n",
    "# Generate graphs for each time window\n",
    "for window_size in window_sizes:\n",
    "    date_generated = pd.date_range(start_date, end_date, freq=window_size)\n",
    "    for i in range(len(date_generated) - 1):\n",
    "        start = date_generated[i]\n",
    "        end = date_generated[i + 1]\n",
    "        \n",
    "        # Filter data for the current time window\n",
    "        window_data = data[(data['timestamp'] >= start) & (data['timestamp'] < end)]\n",
    "        \n",
    "        # Create a directed graph\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        # Add edges and nodes with attributes\n",
    "        for _, row in window_data.iterrows():\n",
    "            src = row['source_workload']\n",
    "            dst = row['destination_workload']\n",
    "            success_rate = row['success_rate']\n",
    "            success_count = row['success_count']  # Added success_count\n",
    "            \n",
    "            # Add nodes\n",
    "            G.add_node(src)\n",
    "            G.add_node(dst)\n",
    "            \n",
    "            # Add edge with color and success_count attributes\n",
    "            G.add_edge(src, dst, weight=row['new_request'], color=get_edge_color(success_rate), success_count=success_count)\n",
    "        \n",
    "        # Get edge colors\n",
    "        edge_colors = [G.edges[edge]['color'] for edge in G.edges]\n",
    "        \n",
    "        # Plot the graph\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        pos = nx.spring_layout(G, seed=70,k=20)\n",
    "        nx.draw(G, pos, with_labels=True, node_color=\"lightblue\", edge_color=edge_colors, width=2, alpha=0.7,node_size=2800, font_size=8)\n",
    "        \n",
    "        # Add edge labels for success_count\n",
    "        edge_labels = {(u, v): f\"{d['success_count']}\" for u, v, d in G.edges(data=True)}\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=7)\n",
    "        \n",
    "        plt.title(f\"Graph from {start} to {end} (Window: {window_size})\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1ª (completa)\t\n",
    "#### MODELOS: GCN, GAT, SAGE\t \n",
    "#### VALIDACIÓN CRUZADA: ✅ Sí\t\n",
    "#### MODULARIDAD: ✅ Alta\n",
    "#### COMPLEJIDAD: 🔥 Alta\n",
    "#### IDEAL PARA: Comparar modelos, evaluar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Cargar datos\n",
    "data = pd.read_csv(\"results sahra2/kiali_latency_percentiles.csv\")\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "# Preprocesar columnas\n",
    "selected_columns = [\n",
    "    \"p50_latency\", \"p90_latency\", \"p95_latency\", \"p99_latency\",\n",
    "    \"istio_request_bytes\", \"success_rate\", \"throughput\", \"average_latency\"\n",
    "]\n",
    "for col in selected_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# Agrupar y normalizar\n",
    "grouped = data.groupby('source_workload')[selected_columns].mean()\n",
    "grouped.dropna(axis=1, how='all', inplace=True)  # Elimina columnas completamente vacías\n",
    "\n",
    "# Escalar características\n",
    "scaler = StandardScaler()\n",
    "node_features_tensor = torch.tensor(scaler.fit_transform(grouped.values), dtype=torch.float)\n",
    "\n",
    "# Mapear nodos\n",
    "node_mapping = {node: idx for idx, node in enumerate(grouped.index)}\n",
    "\n",
    "# Crear edge_index (sin FutureWarning)\n",
    "edge_df = data[['source_workload', 'destination_workload']].copy()\n",
    "edge_df = edge_df[edge_df['source_workload'].isin(node_mapping) & edge_df['destination_workload'].isin(node_mapping)]\n",
    "edge_df['source_workload'] = edge_df['source_workload'].map(node_mapping)\n",
    "edge_df['destination_workload'] = edge_df['destination_workload'].map(node_mapping)\n",
    "edge_index_tensor = torch.tensor(edge_df.values.T, dtype=torch.long)\n",
    "\n",
    "# Etiquetas de anomalía por latencia p99\n",
    "anomaly_series = data.groupby('source_workload')[\"p99_latency\"].mean()\n",
    "anomaly_series = anomaly_series.loc[grouped.index]\n",
    "threshold = anomaly_series.quantile(0.95)\n",
    "anomaly_labels = (anomaly_series > threshold).astype(int)\n",
    "anomaly_labels_tensor = torch.tensor(anomaly_labels.values, dtype=torch.float)\n",
    "\n",
    "# Objeto PyG\n",
    "data = Data(x=node_features_tensor, edge_index=edge_index_tensor)\n",
    "\n",
    "# Definición de modelo\n",
    "def get_model(model_type, in_channels, hidden_channels, out_channels):\n",
    "    class GNNModel(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            if model_type == \"GCN\":\n",
    "                self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "                self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "            elif model_type == \"GAT\":\n",
    "                self.conv1 = GATConv(in_channels, hidden_channels, heads=4, concat=False)\n",
    "                self.conv2 = GATConv(hidden_channels, out_channels, heads=4, concat=False)\n",
    "            elif model_type == \"GraphSAGE\":\n",
    "                self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "                self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "            else:\n",
    "                raise ValueError(\"Modelo no soportado\")\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            x = self.conv1(x, edge_index).relu()\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return x\n",
    "\n",
    "    return GNNModel()\n",
    "\n",
    "# Entrenamiento y evaluación\n",
    "def train_and_evaluate(model_type, train_idx, test_idx, epochs=100, patience=10):\n",
    "    model = get_model(model_type, data.x.size(1), 32, 1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index).squeeze()\n",
    "        loss = F.binary_cross_entropy_with_logits(out[train_idx], anomaly_labels_tensor[train_idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index).squeeze()\n",
    "        probs = torch.sigmoid(out[test_idx])\n",
    "        preds = (probs > 0.5).int().numpy()\n",
    "        true = anomaly_labels_tensor[test_idx].int().numpy()\n",
    "\n",
    "        f1 = f1_score(true, preds)\n",
    "        acc = accuracy_score(true, preds)\n",
    "        try:\n",
    "            auc = roc_auc_score(true, probs.numpy())\n",
    "        except ValueError:\n",
    "            auc = float('nan')\n",
    "\n",
    "    return f1, acc, auc\n",
    "\n",
    "# Validación cruzada\n",
    "y_np = anomaly_labels_tensor.numpy()\n",
    "indices = np.arange(len(y_np))\n",
    "\n",
    "minority_class_count = min(np.bincount(y_np.astype(int)))\n",
    "if minority_class_count < 2:\n",
    "    warnings.warn(\n",
    "        f\"No hay suficientes ejemplos en la clase minoritaria ({minority_class_count}) \"\n",
    "        \"para realizar validación cruzada. Se omite esta fase.\"\n",
    "    )\n",
    "else:\n",
    "    n_splits = min(5, minority_class_count)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    print(\"\\n--- Validación cruzada ---\")\n",
    "    for model_type in [\"GCN\", \"GAT\", \"GraphSAGE\"]:\n",
    "        f1s, accs, aucs = [], [], []\n",
    "        for train_idx, test_idx in skf.split(indices, y_np):\n",
    "            train_idx = torch.tensor(train_idx, dtype=torch.long)\n",
    "            test_idx = torch.tensor(test_idx, dtype=torch.long)\n",
    "            f1, acc, auc = train_and_evaluate(model_type, train_idx, test_idx)\n",
    "            f1s.append(f1)\n",
    "            accs.append(acc)\n",
    "            aucs.append(auc)\n",
    "\n",
    "        print(f\"\\nModelo: {model_type}\")\n",
    "        print(f\"F1 promedio: {np.mean(f1s):.4f}\")\n",
    "        print(f\"Accuracy promedio: {np.mean(accs):.4f}\")\n",
    "        print(f\"AUC-ROC promedio: {np.mean(aucs):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2ª (media)\n",
    "#### MODELO: Solo GCN\n",
    "#### VALIDACIÓN CRUZADA: ❌ No\n",
    "#### MODULARIDAD: ✅ Media\n",
    "#### COMPLEJIDAD: ⭐ Media\t\n",
    "#### IDEAL PARA: Prueba base de GCN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos\n",
    "data = pd.read_csv(\"results sahra2/kiali_latency_percentiles.csv\")\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "# Seleccionar métricas relevantes\n",
    "selected_columns = [\n",
    "    \"p50_latency\", \"p90_latency\", \"p95_latency\", \"p99_latency\",\n",
    "    \"istio_request_bytes\", \"success_rate\", \"throughput\", \"average_latency\"\n",
    "]\n",
    "for col in selected_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# Agrupar por 'source_workload' y calcular la media\n",
    "grouped = data.groupby('source_workload')[selected_columns].mean()\n",
    "grouped.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "# Normalizar características\n",
    "scaler = StandardScaler()\n",
    "node_features = scaler.fit_transform(grouped.values)\n",
    "node_features_tensor = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "# Crear mapeo de nodos\n",
    "node_mapping = {node: idx for idx, node in enumerate(grouped.index)}\n",
    "\n",
    "# Crear edge_index\n",
    "edge_df = data[['source_workload', 'destination_workload']].dropna()\n",
    "edge_df = edge_df[edge_df['source_workload'].isin(node_mapping) & edge_df['destination_workload'].isin(node_mapping)]\n",
    "edge_df['source_workload'] = edge_df['source_workload'].map(node_mapping)\n",
    "edge_df['destination_workload'] = edge_df['destination_workload'].map(node_mapping)\n",
    "edge_index_tensor = torch.tensor(edge_df.values.T, dtype=torch.long)\n",
    "\n",
    "# Etiquetar anomalías basadas en p99_latency\n",
    "anomaly_series = data.groupby('source_workload')[\"p99_latency\"].mean()\n",
    "anomaly_series = anomaly_series.loc[grouped.index]\n",
    "threshold = anomaly_series.quantile(0.95)\n",
    "anomaly_labels = (anomaly_series > threshold).astype(int)\n",
    "anomaly_labels_tensor = torch.tensor(anomaly_labels.values, dtype=torch.float)\n",
    "\n",
    "# Crear objeto Data de PyG\n",
    "graph_data = Data(x=node_features_tensor, edge_index=edge_index_tensor)\n",
    "\n",
    "# Definir modelo GCN (similar a AddGraph)\n",
    "class AddGraphModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Dividir datos en entrenamiento y prueba\n",
    "train_ratio = 0.8\n",
    "num_nodes = len(anomaly_labels_tensor)\n",
    "train_size = int(train_ratio * num_nodes)\n",
    "indices = np.random.permutation(num_nodes)\n",
    "train_idx = torch.tensor(indices[:train_size], dtype=torch.long)\n",
    "test_idx = torch.tensor(indices[train_size:], dtype=torch.long)\n",
    "\n",
    "# Crear modelo\n",
    "model = AddGraphModel(\n",
    "    in_channels=graph_data.x.size(1),\n",
    "    hidden_channels=32,\n",
    "    out_channels=1\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(graph_data.x, graph_data.edge_index).squeeze()\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(out[train_idx], anomaly_labels_tensor[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluar el modelo\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(graph_data.x, graph_data.edge_index).squeeze()\n",
    "    probs = torch.sigmoid(out[test_idx])\n",
    "    preds = (probs > 0.5).int().numpy()\n",
    "    true = anomaly_labels_tensor[test_idx].int().numpy()\n",
    "\n",
    "    f1 = f1_score(true, preds)\n",
    "    acc = accuracy_score(true, preds)\n",
    "    auc = roc_auc_score(true, probs.numpy()) if len(np.unique(true)) > 1 else float('nan')\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}, Accuracy: {acc:.4f}, AUC-ROC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module builds an AddGraph-style anomaly detection model using PyTorch Geometric (PyG). It processes microservice telemetry data (e.g., latency percentiles, throughput, success rate) collected over time.\n",
    "\n",
    "Key steps:\n",
    "\n",
    "Temporal Graph Construction: The system constructs dynamic graph snapshots using 1-minute time windows. Each node represents a microservice (source_workload), and edges represent calls between services.\n",
    "\n",
    "Feature Engineering: Metrics like p50–p99 latency, success rate, and throughput are averaged and normalized for each node in each snapshot.\n",
    "\n",
    "Anomaly Labeling: Nodes are labeled as anomalous if their p99_latency exceeds the 95th percentile in the global dataset during that window.\n",
    "\n",
    "GCN Model: A 2-layer Graph Convolutional Network (GCN) is trained to detect anomalous nodes based on their temporal behavior and structural context.\n",
    "\n",
    "Evaluation: The model is evaluated using F1 score, accuracy, and ROC AUC on the test snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instalar dependencias necesarias\n",
    "!pip install torch torchvision torchaudio torch-geometric pandas scikit-learn matplotlib tqdm --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "# 3. Cargar el dataset\n",
    "df = pd.read_csv(\"results sahra2/kiali_latency_percentiles.csv\")\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# 4. Procesar columnas con listas como strings\n",
    "def parse_and_average(col):\n",
    "    return df[col].apply(lambda x: np.mean(ast.literal_eval(x)) if pd.notnull(x) and isinstance(x, str) else np.nan)\n",
    "\n",
    "list_columns = [\"average_latency\", \"throughput\", \"success_rate\", \"istio_request_bytes\"]\n",
    "for col in list_columns:\n",
    "    df[col + \"_mean\"] = parse_and_average(col)\n",
    "\n",
    "# 5. Definir las features finales\n",
    "features = [\n",
    "    \"p50_latency\", \"p90_latency\", \"p95_latency\", \"p99_latency\",\n",
    "    \"average_latency_mean\", \"throughput_mean\", \"success_rate_mean\", \"istio_request_bytes_mean\"\n",
    "]\n",
    "\n",
    "# Eliminar filas con valores nulos\n",
    "df = df.dropna(subset=[\"source_workload\", \"destination_workload\", \"timestamp\"] + features)\n",
    "\n",
    "# 6. Normalizar las features\n",
    "scaler = StandardScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "# 7. Crear nodos únicos e índice\n",
    "nodes = pd.unique(df[[\"source_workload\", \"destination_workload\"]].values.ravel())\n",
    "node_map = {n: i for i, n in enumerate(nodes)}\n",
    "\n",
    "# 8. Crear snapshots por ventana de tiempo\n",
    "window_size = timedelta(minutes=1)\n",
    "start = df[\"timestamp\"].min()\n",
    "end = df[\"timestamp\"].max()\n",
    "\n",
    "snapshots = []\n",
    "labels = []\n",
    "\n",
    "while start + window_size <= end:\n",
    "    window = df[(df[\"timestamp\"] >= start) & (df[\"timestamp\"] < start + window_size)]\n",
    "    if len(window) == 0:\n",
    "        start += window_size\n",
    "        continue\n",
    "\n",
    "    src = window[\"source_workload\"].map(node_map).values\n",
    "    dst = window[\"destination_workload\"].map(node_map).values\n",
    "    edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "\n",
    "    grouped = window.groupby(\"source_workload\")[features].mean()\n",
    "    grouped = grouped.reindex(nodes).fillna(0)\n",
    "    x = torch.tensor(grouped.values, dtype=torch.float)\n",
    "\n",
    "    p99_series = window.groupby(\"source_workload\")[\"p99_latency\"].mean()\n",
    "    p99_series = p99_series.reindex(nodes).fillna(0)\n",
    "    threshold = df[\"p99_latency\"].quantile(0.95)\n",
    "    y = (p99_series > threshold).astype(int).values\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "    snapshots.append(Data(x=x, edge_index=edge_index))\n",
    "    labels.append(y)\n",
    "\n",
    "    start += window_size\n",
    "\n",
    "print(f\"Snapshots creados: {len(snapshots)}\")\n",
    "\n",
    "# 9. Definir el modelo GCN\n",
    "class AddGraphGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x.squeeze()\n",
    "\n",
    "model = AddGraphGCN(in_channels=len(features), hidden_channels=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 10. Entrenamiento\n",
    "split = int(0.8 * len(snapshots))\n",
    "train_snapshots = snapshots[:split]\n",
    "test_snapshots = snapshots[split:]\n",
    "test_labels = labels[split:]\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, g in enumerate(train_snapshots):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(g.x, g.edge_index)\n",
    "        target = labels[i]\n",
    "        loss = loss_fn(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")\n",
    "\n",
    "# 11. Evaluación\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "for i, g in enumerate(test_snapshots):\n",
    "    with torch.no_grad():\n",
    "        out = model(g.x, g.edge_index)\n",
    "        probs = torch.sigmoid(out)\n",
    "        pred = (probs > 0.5).int().numpy()\n",
    "        true = test_labels[i].int().numpy()\n",
    "\n",
    "        all_preds.extend(pred.tolist())\n",
    "        all_true.extend(true.tolist())\n",
    "\n",
    "f1 = f1_score(all_true, all_preds)\n",
    "acc = accuracy_score(all_true, all_preds)\n",
    "auc = roc_auc_score(all_true, all_preds) if len(np.unique(all_true)) > 1 else float('nan')\n",
    "print(f\"\\nF1 Score: {f1:.4f} | Accuracy: {acc:.4f} | AUC-ROC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-geometric in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (3.11.16)\n",
      "Requirement already satisfied: fsspec in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (2024.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (2.1.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\marti\\appdata\\roaming\\python\\python310\\site-packages (from torch-geometric) (7.0.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (3.2.3)\n",
      "Requirement already satisfied: requests in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->torch-geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->torch-geometric) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->torch-geometric) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->torch-geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->torch-geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->torch-geometric) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->torch-geometric) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->torch-geometric) (1.19.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch-geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torch-geometric) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torch-geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torch-geometric) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torch-geometric) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\marti\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->torch-geometric) (0.4.6)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\marti\\appdata\\roaming\\python\\python310\\site-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.13.1)\n",
      "Requirement already satisfied: torch-geometric-temporal in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric-temporal) (3.4.2)\n",
      "Requirement already satisfied: six in c:\\users\\marti\\appdata\\roaming\\python\\python310\\site-packages (from torch-geometric-temporal) (1.17.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\marti\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\marti\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\marti\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\marti\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Instalar PyTorch (elige la versión según tu sistema; esta es para CPU)\n",
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# Instalar PyTorch Geometric y Torch-Geometric-Temporal\n",
    "!pip install torch-geometric\n",
    "!pip install torch-geometric-temporal\n",
    "\n",
    "# Instalar otras dependencias útiles\n",
    "!pip install pandas scikit-learn matplotlib tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric_temporal.nn import TGNMemory, TemporalConv\n",
    "from torch_geometric_temporal.signal import StaticGraphTemporalSignal, temporal_signal_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Cargar datos\n",
    "df = pd.read_csv(\"results sahra2/kiali_latency_percentiles.csv\")\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# 2. Detectar anomalías por percentil 95 de p99_latency\n",
    "threshold = df[\"p99_latency\"].quantile(0.95)\n",
    "df[\"anomaly\"] = (df[\"p99_latency\"] > threshold).astype(int)\n",
    "\n",
    "# 3. Crear snapshots por minuto\n",
    "snapshots = []\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for ts, group in df.groupby(df['timestamp'].dt.floor(\"min\")):\n",
    "    nodes = pd.concat([group[\"source_workload\"], group[\"destination_workload\"]]).dropna().unique()\n",
    "    node_map = {n: i for i, n in enumerate(nodes)}\n",
    "    \n",
    "    # Crear edge_index\n",
    "    edges = group[[\"source_workload\", \"destination_workload\"]].dropna()\n",
    "    if edges.empty: continue\n",
    "    edge_idx = edges.applymap(node_map.get).dropna().astype(int).values.T\n",
    "    edge_index = torch.tensor(edge_idx, dtype=torch.long)\n",
    "\n",
    "    # edge_attr: usar p99_latency o throughput como atributos\n",
    "    edge_attr_values = group[\"p99_latency\"].fillna(0).values[:edge_index.shape[1]]\n",
    "    edge_attr = torch.tensor(edge_attr_values.reshape(-1, 1), dtype=torch.float)\n",
    "\n",
    "    # Atributos de nodo\n",
    "    node_df = group.groupby(\"source_workload\")[\"p99_latency\"].mean().reindex(nodes).fillna(0).values\n",
    "    x = torch.tensor(scaler.fit_transform(node_df.reshape(-1, 1)), dtype=torch.float)\n",
    "\n",
    "    # Etiquetas (1 si hubo anomalía en nodo)\n",
    "    y_df = group.groupby(\"source_workload\")[\"anomaly\"].max().reindex(nodes).fillna(0).values\n",
    "    y = torch.tensor(y_df.astype(int), dtype=torch.long)\n",
    "\n",
    "    snapshots.append((x, edge_index, edge_attr, y))\n",
    "\n",
    "# 4. Crear dataset temporal\n",
    "dataset = StaticGraphTemporalSignal(\n",
    "    edge_indices=[s[1] for s in snapshots],\n",
    "    edge_features=[s[2] for s in snapshots],\n",
    "    features=[s[0] for s in snapshots],\n",
    "    targets=[s[3] for s in snapshots]\n",
    ")\n",
    "\n",
    "# 5. Dividir en entrenamiento y prueba\n",
    "train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)\n",
    "\n",
    "# 6. Definir modelo temporal\n",
    "class TGN(torch.nn.Module):\n",
    "    def __init__(self, node_features, edge_features, memory_dim, time_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.memory = TGNMemory(\n",
    "            node_features=node_features,\n",
    "            edge_features=edge_features,\n",
    "            memory_dimension=memory_dim,\n",
    "            time_dimension=time_dim,\n",
    "            message_dimension=memory_dim,\n",
    "        )\n",
    "        self.temporal_conv = TemporalConv(in_channels=memory_dim, out_channels=output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, time=None):\n",
    "        mem = self.memory(x, edge_index, edge_attr, time)\n",
    "        return self.temporal_conv(mem, edge_index)\n",
    "\n",
    "# 7. Instanciar modelo\n",
    "model = TGN(node_features=1, edge_features=1, memory_dim=32, time_dim=16, output_dim=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 8. Entrenamiento\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for snapshot in train_dataset:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
    "        loss = F.binary_cross_entropy_with_logits(out.squeeze(), snapshot.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# 9. Evaluación\n",
    "model.eval()\n",
    "f1s, accs, aucs = [], [], []\n",
    "\n",
    "for snapshot in test_dataset:\n",
    "    with torch.no_grad():\n",
    "        out = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
    "        probs = torch.sigmoid(out.squeeze())\n",
    "        preds = (probs > 0.5).int()\n",
    "        y_true = snapshot.y.cpu().numpy()\n",
    "        y_pred = preds.cpu().numpy()\n",
    "        y_prob = probs.cpu().numpy()\n",
    "\n",
    "        f1s.append(f1_score(y_true, y_pred, zero_division=0))\n",
    "        accs.append(accuracy_score(y_true, y_pred))\n",
    "        if len(np.unique(y_true)) > 1:\n",
    "            aucs.append(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "print(f\"\\nF1 Score: {np.mean(f1s):.4f}\")\n",
    "print(f\"Accuracy: {np.mean(accs):.4f}\")\n",
    "print(f\"AUC-ROC: {np.mean(aucs) if aucs else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_geometric_temporal.nn.recurrent'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch_geometric_temporal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecurrent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DCRNN\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch_geometric_temporal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m temporal_signal_split\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m f1_score, accuracy_score, roc_auc_score\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric_temporal.nn.recurrent'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric_temporal.nn.recurrent import DCRNN\n",
    "from torch_geometric_temporal.signal import temporal_signal_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Definir el modelo ST-GNN\n",
    "class STGNN(torch.nn.Module):\n",
    "    def __init__(self, node_features, hidden_channels, output_dim):\n",
    "        super(STGNN, self).__init__()\n",
    "        self.recurrent = DCRNN(node_features, hidden_channels, K=2)  # Diffusion Convolutional Recurrent Network\n",
    "        self.linear = torch.nn.Linear(hidden_channels, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        h = self.recurrent(x, edge_index, edge_weight)\n",
    "        out = self.linear(h)\n",
    "        return out\n",
    "\n",
    "# Cargar datos\n",
    "data = pd.read_csv(\"results sahra2/kiali_latency_percentiles.csv\")\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "# Seleccionar métricas relevantes\n",
    "selected_columns = [\n",
    "    \"p50_latency\", \"p90_latency\", \"p95_latency\", \"p99_latency\",\n",
    "    \"istio_request_bytes\", \"success_rate\", \"throughput\", \"average_latency\"\n",
    "]\n",
    "for col in selected_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# Agrupar por 'source_workload' y calcular la media\n",
    "grouped = data.groupby(['timestamp', 'source_workload'])[selected_columns].mean().reset_index()\n",
    "\n",
    "# Crear nodos y aristas para cada snapshot temporal\n",
    "timestamps = grouped['timestamp'].unique()\n",
    "edge_indices, edge_weights, node_features, labels = [], [], [], []\n",
    "\n",
    "for timestamp in timestamps:\n",
    "    snapshot = grouped[grouped['timestamp'] == timestamp]\n",
    "    \n",
    "    # Crear nodos (características)\n",
    "    node_features.append(torch.tensor(snapshot[selected_columns].values, dtype=torch.float))\n",
    "    \n",
    "    # Crear aristas (relaciones entre microservicios)\n",
    "    edges = data[data['timestamp'] == timestamp][['source_workload', 'destination_workload']].dropna()\n",
    "    edge_index = torch.tensor(edges.values.T, dtype=torch.long)\n",
    "    edge_indices.append(edge_index)\n",
    "    \n",
    "    # Crear pesos de aristas (por ejemplo, tasa de éxito)\n",
    "    edge_weight = torch.tensor(edges['success_rate'].values, dtype=torch.float)\n",
    "    edge_weights.append(edge_weight)\n",
    "    \n",
    "    # Etiquetas de anomalías (por ejemplo, basadas en p99_latency)\n",
    "    anomaly_labels = (snapshot['p99_latency'] > snapshot['p99_latency'].quantile(0.95)).astype(int)\n",
    "    labels.append(torch.tensor(anomaly_labels.values, dtype=torch.float))\n",
    "\n",
    "# Crear dataset temporal\n",
    "from torch_geometric_temporal.signal import StaticGraphTemporalSignal\n",
    "dataset = StaticGraphTemporalSignal(edge_indices, edge_weights, node_features, timestamps, labels)\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)\n",
    "\n",
    "# Entrenar el modelo\n",
    "model = STGNN(node_features=len(selected_columns), hidden_channels=32, output_dim=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    for snapshot in train_dataset:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(snapshot.x, snapshot.edge_index, snapshot.edge_weight)\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(out, snapshot.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluar el modelo\n",
    "model.eval()\n",
    "f1_scores, accuracies, aucs = [], [], []\n",
    "for snapshot in test_dataset:\n",
    "    with torch.no_grad():\n",
    "        out = model(snapshot.x, snapshot.edge_index, snapshot.edge_weight)\n",
    "        preds = (torch.sigmoid(out) > 0.5).int()\n",
    "        f1_scores.append(f1_score(snapshot.y, preds))\n",
    "        accuracies.append(accuracy_score(snapshot.y, preds))\n",
    "        aucs.append(roc_auc_score(snapshot.y, torch.sigmoid(out)))\n",
    "\n",
    "print(f\"F1 Score: {np.mean(f1_scores):.4f}, Accuracy: {np.mean(accuracies):.4f}, AUC-ROC: {np.mean(aucs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DCRNN +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric_temporal.nn.recurrent import DCRNN\n",
    "from torch_geometric_temporal.signal import temporal_signal_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Definir el modelo ST-GNN\n",
    "class STGNN(torch.nn.Module):\n",
    "    def __init__(self, node_features, hidden_channels, output_dim):\n",
    "        super(STGNN, self).__init__()\n",
    "        self.recurrent = DCRNN(node_features, hidden_channels, K=2)  # Diffusion Convolutional Recurrent Network\n",
    "        self.linear = torch.nn.Linear(hidden_channels, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        h = self.recurrent(x, edge_index, edge_weight)\n",
    "        out = self.linear(h)\n",
    "        return out\n",
    "\n",
    "# Cargar datos\n",
    "data = pd.read_csv(\"results sahra2/kiali_latency_percentiles.csv\")\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "# Seleccionar métricas relevantes\n",
    "selected_columns = [\n",
    "    \"p50_latency\", \"p90_latency\", \"p95_latency\", \"p99_latency\",\n",
    "    \"istio_request_bytes\", \"success_rate\", \"throughput\", \"average_latency\"\n",
    "]\n",
    "for col in selected_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# Agrupar por 'source_workload' y calcular la media\n",
    "grouped = data.groupby(['timestamp', 'source_workload'])[selected_columns].mean().reset_index()\n",
    "\n",
    "# Crear nodos y aristas para cada snapshot temporal\n",
    "timestamps = grouped['timestamp'].unique()\n",
    "edge_indices, edge_weights, node_features, labels = [], [], [], []\n",
    "\n",
    "for timestamp in timestamps:\n",
    "    snapshot = grouped[grouped['timestamp'] == timestamp]\n",
    "    \n",
    "    # Crear nodos (características)\n",
    "    node_features.append(torch.tensor(snapshot[selected_columns].values, dtype=torch.float))\n",
    "    \n",
    "    # Crear aristas (relaciones entre microservicios)\n",
    "    edges = data[data['timestamp'] == timestamp][['source_workload', 'destination_workload']].dropna()\n",
    "    edge_index = torch.tensor(edges.values.T, dtype=torch.long)\n",
    "    edge_indices.append(edge_index)\n",
    "    \n",
    "    # Crear pesos de aristas (por ejemplo, tasa de éxito)\n",
    "    edge_weight = torch.tensor(edges['success_rate'].values, dtype=torch.float)\n",
    "    edge_weights.append(edge_weight)\n",
    "    \n",
    "    # Etiquetas de anomalías (basadas en p99 y p95)\n",
    "    anomaly_labels_p99 = (snapshot['p99_latency'] > snapshot['p99_latency'].quantile(0.95)).astype(int)\n",
    "    anomaly_labels_p95 = (snapshot['p95_latency'] > snapshot['p95_latency'].quantile(0.95)).astype(int)\n",
    "    combined_labels = (anomaly_labels_p99 | anomaly_labels_p95).astype(int)  # Etiqueta combinada\n",
    "    labels.append(torch.tensor(combined_labels.values, dtype=torch.float))\n",
    "\n",
    "# Crear dataset temporal\n",
    "from torch_geometric_temporal.signal import StaticGraphTemporalSignal\n",
    "dataset = StaticGraphTemporalSignal(edge_indices, edge_weights, node_features, timestamps, labels)\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)\n",
    "\n",
    "# Entrenar el modelo\n",
    "model = STGNN(node_features=len(selected_columns), hidden_channels=32, output_dim=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    for snapshot in train_dataset:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(snapshot.x, snapshot.edge_index, snapshot.edge_weight)\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(out, snapshot.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluar el modelo\n",
    "model.eval()\n",
    "f1_scores, accuracies, aucs = [], [], []\n",
    "for snapshot in test_dataset:\n",
    "    with torch.no_grad():\n",
    "        out = model(snapshot.x, snapshot.edge_index, snapshot.edge_weight)\n",
    "        preds = (torch.sigmoid(out) > 0.5).int()\n",
    "        f1_scores.append(f1_score(snapshot.y, preds))\n",
    "        accuracies.append(accuracy_score(snapshot.y, preds))\n",
    "        aucs.append(roc_auc_score(snapshot.y, torch.sigmoid(out)))\n",
    "\n",
    "print(f\"F1 Score: {np.mean(f1_scores):.4f}, Accuracy: {np.mean(accuracies):.4f}, AUC-ROC: {np.mean(aucs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KGROOT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install dependencies\n",
    "!pip install dgl torch pandas scikit-learn tqdm --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dgl.nn import GraphConv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import timedelta\n",
    "\n",
    "# 3. Load and preprocess dataset\n",
    "df = pd.read_csv(\"istio_request_2.2.csv\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.sort_values(\"timestamp\")\n",
    "\n",
    "# Calculate avg latency\n",
    "df[\"avg_latency\"] = df[\"istio_request_duration_milliseconds_sum\"] / df[\"total_request\"]\n",
    "df[\"avg_latency\"].fillna(0, inplace=True)\n",
    "\n",
    "# Label relation type\n",
    "def classify_relation(row, latency_thres):\n",
    "    if row[\"avg_latency\"] > latency_thres:\n",
    "        return \"high_latency\"\n",
    "    elif row[\"response_code\"] >= 500:\n",
    "        return \"error_5xx\"\n",
    "    elif row[\"response_code\"] >= 400:\n",
    "        return \"error_4xx\"\n",
    "    else:\n",
    "        return \"success\"\n",
    "\n",
    "lat_thres = df[\"avg_latency\"].quantile(0.95)\n",
    "df[\"relation\"] = df.apply(lambda r: classify_relation(r, lat_thres), axis=1)\n",
    "\n",
    "# 4. Encode services and relations\n",
    "all_services = pd.unique(df[[\"source_workload\", \"destination_workload\"]].values.ravel())\n",
    "service2id = {svc: i for i, svc in enumerate(all_services)}\n",
    "relation2id = {\"success\": 0, \"error_4xx\": 1, \"error_5xx\": 2, \"high_latency\": 3}\n",
    "\n",
    "df[\"src_id\"] = df[\"source_workload\"].map(service2id)\n",
    "df[\"dst_id\"] = df[\"destination_workload\"].map(service2id)\n",
    "df[\"rel_id\"] = df[\"relation\"].map(relation2id)\n",
    "\n",
    "# Normalize latency\n",
    "scaler = MinMaxScaler()\n",
    "df[\"latency_norm\"] = scaler.fit_transform(df[[\"avg_latency\"]])\n",
    "\n",
    "# 5. Create graph snapshots (historical and current)\n",
    "snapshots = []\n",
    "labels = []\n",
    "\n",
    "window_size = timedelta(minutes=1)\n",
    "start_time = df[\"timestamp\"].min()\n",
    "end_time = df[\"timestamp\"].max()\n",
    "timepoints = []\n",
    "\n",
    "while start_time + window_size < end_time:\n",
    "    window_df = df[(df[\"timestamp\"] >= start_time) & (df[\"timestamp\"] < start_time + window_size)]\n",
    "    if len(window_df) > 0:\n",
    "        snapshots.append(window_df.copy())\n",
    "        timepoints.append(start_time)\n",
    "    start_time += window_size\n",
    "\n",
    "# Use last snapshot as current graph, previous as historical\n",
    "historical_graphs = snapshots[:-1]\n",
    "current_graph_df = snapshots[-1]\n",
    "\n",
    "# 6. GCN model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hidden_feats)\n",
    "        self.conv2 = GraphConv(hidden_feats, 1)\n",
    "\n",
    "    def forward(self, g, feat):\n",
    "        h = F.relu(self.conv1(g, feat))\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "# 7. Prepare historical training data\n",
    "graphs = []\n",
    "edge_labels = []\n",
    "\n",
    "for snapshot in historical_graphs:\n",
    "    g = dgl.graph((snapshot[\"src_id\"], snapshot[\"dst_id\"]), num_nodes=len(service2id))\n",
    "    edge_feat = torch.tensor(snapshot[\"latency_norm\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "    rel_feat = torch.tensor(snapshot[\"rel_id\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "    g.edata[\"feat\"] = torch.cat([edge_feat, rel_feat], dim=1)\n",
    "    graphs.append(g)\n",
    "\n",
    "# Create training graph as average over history\n",
    "def average_edge_features(graphs):\n",
    "    edge_feats = [g.edata[\"feat\"] for g in graphs]\n",
    "    stacked = torch.stack([F.pad(e, (0, 0, 0, max([g.num_edges() for g in graphs]) - e.size(0))) for e in edge_feats])\n",
    "    return stacked.mean(0)\n",
    "\n",
    "avg_feat = average_edge_features(graphs)\n",
    "train_g = dgl.graph((current_graph_df[\"src_id\"], current_graph_df[\"dst_id\"]), num_nodes=len(service2id))\n",
    "edge_feat = torch.tensor(current_graph_df[\"latency_norm\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "rel_feat = torch.tensor(current_graph_df[\"rel_id\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "train_g.edata[\"feat\"] = torch.cat([edge_feat, rel_feat], dim=1)\n",
    "\n",
    "# 8. Train GCN to reconstruct historical pattern\n",
    "model = GCN(in_feats=2, hidden_feats=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    pred = model(train_g, train_g.edata[\"feat\"])\n",
    "    loss = loss_fn(pred, avg_feat[:len(pred)])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 9. Anomaly score: deviation from historical pattern\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(train_g, train_g.edata[\"feat\"])\n",
    "    scores = torch.abs(pred.squeeze() - avg_feat[:len(pred)].squeeze())\n",
    "    topk = torch.topk(scores, k=10)\n",
    "    print(\"\\nTop-10 Anomalous Interactions:\")\n",
    "    for i in topk.indices:\n",
    "        src = current_graph_df.iloc[i][\"source_workload\"]\n",
    "        dst = current_graph_df.iloc[i][\"destination_workload\"]\n",
    "        r = current_graph_df.iloc[i][\"relation\"]\n",
    "        print(f\"{src} → {dst} | relation: {r} | anomaly score: {scores[i].item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización con NetworkX y Matplotlib\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear grafo dirigido\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Añadir nodos\n",
    "for node_id in service2id.values():\n",
    "    G.add_node(node_id)\n",
    "\n",
    "# Añadir aristas con anomalía como atributo\n",
    "for i, row in current_graph_df.iterrows():\n",
    "    src = service2id[row[\"source_workload\"]]\n",
    "    dst = service2id[row[\"destination_workload\"]]\n",
    "    score = scores[i].item()\n",
    "    G.add_edge(src, dst, score=score)\n",
    "\n",
    "# Posiciones para layout\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "# Crear lista de colores para las aristas según score\n",
    "edge_colors = []\n",
    "edge_widths = []\n",
    "threshold = torch.quantile(scores, 0.90).item()\n",
    "\n",
    "for u, v in G.edges():\n",
    "    s = G[u][v]['score']\n",
    "    if s > threshold:\n",
    "        edge_colors.append('red')\n",
    "        edge_widths.append(2.5)\n",
    "    else:\n",
    "        edge_colors.append('gray')\n",
    "        edge_widths.append(0.8)\n",
    "\n",
    "# Etiquetas de nodos\n",
    "id2service = {v: k for k, v in service2id.items()}\n",
    "node_labels = {node: id2service[node] for node in G.nodes()}\n",
    "\n",
    "# Dibujar grafo\n",
    "plt.figure(figsize=(14, 10))\n",
    "nx.draw_networkx_nodes(G, pos, node_size=700, node_color='skyblue')\n",
    "nx.draw_networkx_edges(G, pos, edge_color=edge_colors, width=edge_widths, arrows=True)\n",
    "nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=10)\n",
    "\n",
    "plt.title(\"KGroot-based Anomaly Graph\\n(Red edges are anomalous)\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
