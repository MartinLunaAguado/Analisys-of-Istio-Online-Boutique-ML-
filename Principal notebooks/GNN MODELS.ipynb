{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le fichier CSV\n",
    "file_path = \"data/istio_request_2.2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df['grpc_response_status'].fillna(0, inplace=True)\n",
    "\n",
    "df['response_flags'] = df['response_flags'].astype(str).str.strip()  # Convertir en string et enlever espaces\n",
    "\n",
    "# Ajouter une colonne 'result' avec 'success' ou 'error'\n",
    "df['result'] = df.apply(\n",
    "    lambda row: 'success' if row['response_code'] == 200 and row['grpc_response_status'] == 0 and row['response_flags'] == '-' else 'error',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# R√©organiser les donn√©es par 'source_workload', 'destination_workload' et 'timestamp'\n",
    "df_sorted = df.sort_values(by=['source_workload', 'destination_workload', 'timestamp'])\n",
    "\n",
    "# todas las solicitudes entre cada par origen-destino est√°n ordenadas en el tiempo.\n",
    "# Esto es √∫til para c√°lculos secuenciales, como determinar el n√∫mero de solicitudes\n",
    "# en un per√≠odo de tiempo o calcular m√©tricas basadas en la secuencia temporal de eventos.\n",
    "\n",
    "# Sauvegarder le fichier r√©sultant\n",
    "df_sorted.to_csv(\"results sahra2/aggregated_istio_data.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Charger le fichier\n",
    "file_path = \"results sahra2/aggregated_istio_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convertir timestamp en datetime pour le tri\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Trier avant la s√©paration\n",
    "df.sort_values(by=['source_workload', 'destination_workload', 'timestamp'], inplace=True)\n",
    "\n",
    "# S√©parer les succ√®s\n",
    "df_success = df[df['result'] == 'success'].copy()\n",
    "#Separamos para obtener solo las solicitudes exitosas\n",
    "\n",
    "# Calculer new_request, new_istio_request_bytes et new_istio_request_duration_milliseconds avec 0 pour la premi√®re ligne\n",
    "df_success['new_request'] = df_success.groupby(['source_workload', 'destination_workload'])['total_request'].diff().fillna(0)\n",
    "df_success['new_istio_request_bytes'] = df_success.groupby(['source_workload', 'destination_workload'])['istio_request_bytes_sum'].diff().fillna(0)\n",
    "df_success['new_istio_request_duration_milliseconds'] = df_success.groupby(['source_workload', 'destination_workload'])['istio_request_duration_milliseconds_sum'].diff().fillna(0)\n",
    "\n",
    "# diff() calcula la diferencia entre una fila y la anterior en cada grupo de source_workload y destination_workload.\n",
    "# Para la primera fila de cada grupo, no hay una fila anterior, as√≠ que diff() devuelve NaN.\n",
    "# Con fillna(0) reemplaza esos NaN con 0, indicando que no hubo solicitudes nuevas en ese primer intervalo.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Appliquer la condition si new_request == 0\n",
    "df_success.loc[df_success['new_request'] == 0, ['new_istio_request_bytes', 'new_istio_request_duration_milliseconds']] = 0\n",
    "#Si no hubo nuevas solicitudes en un intervalo, asegura que los bytes y la duraci√≥n tambi√©n se registren como cero.\n",
    "\n",
    "\n",
    "\n",
    "# Calculer latency\n",
    "df_success['latency'] = df_success['new_istio_request_duration_milliseconds'] / df_success['new_request']\n",
    "df_success['latency'].fillna(0, inplace=True)\n",
    "# Calcula la latencia dividiendo el tiempo total de solicitudes por el n√∫mero de solicitudes nuevas en cada intervalo.\n",
    "\n",
    "# Sauvegarder les succ√®s dans un fichier\n",
    "df_success.to_csv(\"results sahra2/success_istio_data.csv\", index=False)\n",
    "# Si no hubo nuevas solicitudes, la latencia se rellena con cero\n",
    "\n",
    "# S√©parer les erreurs HTTP et gRPC\n",
    "df_http_errors = df[(df['result'] == 'error') & (df['request_protocol'] == 'http')].copy()\n",
    "df_grpc_errors = df[(df['result'] == 'error') & (df['request_protocol'] == 'grpc')].copy()\n",
    "\n",
    "error_files = []  # Liste des fichiers d'erreur g√©n√©r√©s\n",
    "\n",
    "# Traitement des erreurs HTTP\n",
    "http_groups = df_http_errors.groupby(['request_protocol', 'response_code', 'grpc_response_status', 'response_flags'])\n",
    "\n",
    "for (request_protocol, response_code, grpc_status, response_flags), df_error in http_groups:\n",
    "    df_error = df_error.copy()\n",
    "    \n",
    "    # Calculer new_request, new_istio_request_bytes et new_istio_request_duration_milliseconds avec 0 pour la premi√®re ligne\n",
    "    df_error['new_request'] = df_error.groupby(['source_workload', 'destination_workload'])['total_request'].diff().fillna(0)\n",
    "    df_error['new_istio_request_bytes'] = df_error.groupby(['source_workload', 'destination_workload'])['istio_request_bytes_sum'].diff().fillna(0)\n",
    "    df_error['new_istio_request_duration_milliseconds'] = df_error.groupby(['source_workload', 'destination_workload'])['istio_request_duration_milliseconds_sum'].diff().fillna(0)\n",
    "    \n",
    "    # Appliquer la condition si new_request == 0\n",
    "    df_error.loc[df_error['new_request'] == 0, ['new_istio_request_bytes', 'new_istio_request_duration_milliseconds']] = 0\n",
    "    \n",
    "    # Calculer latency\n",
    "    df_error['latency'] = df_error['new_istio_request_duration_milliseconds'] / df_error['new_request']\n",
    "    df_error['latency'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Nommer le fichier selon l'erreur\n",
    "    file_name = f\"results sahra2/error_{request_protocol}_{response_code}_{response_flags}.csv\"\n",
    "    df_error.to_csv(file_name, index=False)\n",
    "    error_files.append(df_error)\n",
    "\n",
    "# Traitement des erreurs gRPC\n",
    "grpc_groups = df_grpc_errors.groupby(['request_protocol', 'response_code', 'grpc_response_status', 'response_flags'])\n",
    "\n",
    "for (request_protocol, response_code, grpc_status, response_flags), df_error in grpc_groups:\n",
    "    df_error = df_error.copy()\n",
    "    \n",
    "    # Calculer new_request, new_istio_request_bytes et new_istio_request_duration_milliseconds avec 0 pour la premi√®re ligne\n",
    "    df_error['new_request'] = df_error.groupby(['source_workload', 'destination_workload'])['total_request'].diff().fillna(0)\n",
    "    df_error['new_istio_request_bytes'] = df_error.groupby(['source_workload', 'destination_workload'])['istio_request_bytes_sum'].diff().fillna(0)\n",
    "    df_error['new_istio_request_duration_milliseconds'] = df_error.groupby(['source_workload', 'destination_workload'])['istio_request_duration_milliseconds_sum'].diff().fillna(0)\n",
    "    \n",
    "    # Appliquer la condition si new_request == 0\n",
    "    df_error.loc[df_error['new_request'] == 0, ['new_istio_request_bytes', 'new_istio_request_duration_milliseconds']] = 0\n",
    "    \n",
    "    # Calculer latency\n",
    "    df_error['latency'] = df_error['new_istio_request_duration_milliseconds'] / df_error['new_request']\n",
    "    df_error['latency'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Nommer le fichier selon l'erreur\n",
    "    file_name = f\"results sahra2/error_{request_protocol}_{response_code}_{grpc_status}_{response_flags}.csv\"\n",
    "    df_error.to_csv(file_name, index=False)\n",
    "    error_files.append(df_error)\n",
    "\n",
    "# Fusionner tous les fichiers (success + errors)\n",
    "df_final = pd.concat([df_success] + error_files).sort_values(by=['source_workload', 'destination_workload', 'timestamp'])\n",
    "\n",
    "# Sauvegarder le fichier final\n",
    "df_final.to_csv(\"results sahra2/new_request_istio_data.csv\", index=False)\n",
    "\n",
    "print(\"Traitement termin√©. Fichier sauvegard√© sous 'new_request_istio_data.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "\n",
    "# Charger le fichier\n",
    "file_path = \"results sahra2/new_request_istio_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convertir timestamp en datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Trier les donn√©es\n",
    "df.sort_values(by=['source_workload', 'destination_workload', 'timestamp'], inplace=True)\n",
    "\n",
    "# Calculer success rate, error rate, success count, error count, duration success request, duration error request et average latency par timestamp\n",
    "grouped = df.groupby(['source_workload', 'destination_workload', 'timestamp'])\n",
    "aggregated_rows = []\n",
    "\n",
    "for (src, dst, ts), group in grouped:\n",
    "    total_new_request = group['new_request'].sum()\n",
    "    success_count = group[group['result'] == 'success']['new_request'].sum()\n",
    "    error_count = total_new_request - success_count\n",
    "    \n",
    "    if total_new_request > 0:\n",
    "        success_rate = success_count / total_new_request\n",
    "        error_rate = 1 - success_rate\n",
    "    else:\n",
    "        success_rate = float('nan')\n",
    "        error_rate = float('nan')\n",
    "    \n",
    "    # Calculer la dur√©e des requ√™tes r√©ussies et erron√©es\n",
    "    duration_success_request = group[group['result'] == 'success']['latency'].sum()\n",
    "    duration_error_request = group[group['result'] == 'error']['latency'].sum()\n",
    "    average_latency = duration_success_request + duration_error_request\n",
    "\n",
    "    # S√©parer new_istio_request_bytes en success et error\n",
    "    new_istio_request_bytes_success = group[group['result'] == 'success']['new_istio_request_bytes'].sum()\n",
    "    new_istio_request_bytes_error = group[group['result'] == 'error']['new_istio_request_bytes'].sum()\n",
    "    istio_request_bytes = new_istio_request_bytes_success+new_istio_request_bytes_error\n",
    "    aggregated_rows.append([ts, src, dst, group['total_request'].max(), total_new_request, success_count, error_count, success_rate, error_rate, duration_success_request, duration_error_request, average_latency, new_istio_request_bytes_success, new_istio_request_bytes_error,istio_request_bytes])\n",
    "\n",
    "# Cr√©er un DataFrame final\n",
    "df_final = pd.DataFrame(aggregated_rows, columns=['timestamp', 'source_workload', 'destination_workload', 'total_request', 'new_request', 'success_count', 'error_count', 'success_rate', 'error_rate', 'duration_success_request', 'duration_error_request', 'average_latency', 'new_istio_request_bytes_success', 'new_istio_request_bytes_error','istio_request_bytes'])\n",
    "\n",
    "# Sauvegarder le fichier\n",
    "output_file = \"results sahra2/aggregated_istio_rates.csv\"\n",
    "df_final.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Traitement termin√©. Fichier sauvegard√© sous {output_file}.\") #Procesamiento completado. Archivo guardado como ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger le fichier\n",
    "file_path = \"results sahra2/aggregated_istio_rates.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convertir timestamp en datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# D√©finir les fen√™tres de temps\n",
    "time_windows = ['15S', '30S', '1min', '5min', '10min']\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les r√©sultats\n",
    "kpi_results = []\n",
    "\n",
    "for window in time_windows:\n",
    "    df_resampled = (df\n",
    "        .groupby(['source_workload', 'destination_workload'])  # Regrouper par workload\n",
    "        .resample(window, on='timestamp', label='right', closed='right')  # Aligner sur la fin de la fen√™tre\n",
    "        .agg({\n",
    "            'total_request': 'max',\n",
    "            'new_request': 'sum',\n",
    "            'success_count': 'sum',\n",
    "            'error_count': 'sum',\n",
    "            'success_rate': 'mean',\n",
    "            'error_rate': 'mean',\n",
    "            'average_latency': 'sum',\n",
    "            'istio_request_bytes': 'sum'\n",
    "        })\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Calcul du throughput\n",
    "    df_resampled['throughput'] = df_resampled['istio_request_bytes'] / pd.to_timedelta(window).total_seconds()\n",
    "    \n",
    "    # Calcul du request rate\n",
    "    df_resampled['request_rate'] = df_resampled['new_request'] / pd.to_timedelta(window).total_seconds()\n",
    "    \n",
    "    df_resampled['time_window'] = window\n",
    "    kpi_results.append(df_resampled)\n",
    "\n",
    "# Concat√©ner tous les r√©sultats\n",
    "df_final = pd.concat(kpi_results)\n",
    "\n",
    "# Supprimer la ligne o√π timestamp == \"2025-03-10 16:09:00\"\n",
    "starting_point = pd.Timestamp(\"2025-03-10 16:09:00\")\n",
    "df_final = df_final[df_final['timestamp'] != starting_point]\n",
    "\n",
    "# Sauvegarder dans un fichier CSV\n",
    "df_final.to_csv(\"results sahra2/kiali_kpi_metrics.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Charger le fichier\n",
    "df = pd.read_csv(\"results sahra2/kiali_kpi_metrics.csv\")\n",
    "\n",
    "# Convertir timestamp en datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# D√©finir les fen√™tres de temps et leurs intervalles respectifs\n",
    "interval_mapping = {\n",
    "    '15S': '1min',  # 15 secondes -> 1 minute\n",
    "    '30S': '2min',  # 30 secondes -> 2 minutes\n",
    "    '1min': '4min',  # 1 minute -> 4 minutes\n",
    "    '5min': '10min',  # 5 minutes -> 10 minutes\n",
    "    '10min': '10min'  # 10 minutes -> 10 minutes\n",
    "}\n",
    "\n",
    "latency_results = []\n",
    "\n",
    "for window, interval in interval_mapping.items():\n",
    "    df_filtered = df[df['time_window'] == window].copy()\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No data for time window: {window}\")\n",
    "        continue\n",
    "    \n",
    "    df_filtered.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    try:\n",
    "        df_grouped = (df_filtered.groupby(['source_workload', 'destination_workload'])\n",
    "                      .resample(interval)\n",
    "                      .agg({col: list for col in df.columns if col not in ['timestamp', 'source_workload', 'destination_workload', 'time_window']})\n",
    "                      .reset_index())\n",
    "    except Exception as e:\n",
    "        print(f\"Error during resampling for window {window}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    for idx, row in df_grouped.iterrows():\n",
    "        values = row.get('average_latency', [])\n",
    "        frequencies = row.get('new_request', [])\n",
    "        \n",
    "        if not values or not frequencies or len(values) != len(frequencies):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            data = np.repeat(values, frequencies)\n",
    "            if data.size == 0:\n",
    "                continue\n",
    "            \n",
    "            percentiles = {\n",
    "                'p50_latency': np.percentile(data, 50),\n",
    "                'p90_latency': np.percentile(data, 90),\n",
    "                'p95_latency': np.percentile(data, 95),\n",
    "                'p99_latency': np.percentile(data, 99)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating percentiles for row {idx} in window {window}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        result = {**row.to_dict(), **percentiles, 'time_window': window}\n",
    "        latency_results.append(result)\n",
    "\n",
    "# Cr√©er un DataFrame final\n",
    "df_latency = pd.DataFrame(latency_results)\n",
    "\n",
    "# Sauvegarder dans un fichier CSV\n",
    "df_latency.to_csv(\"results sahra2/kiali_latency_percentiles.csv\", index=False)\n",
    "\n",
    "print(\"Traitement termin√©. Fichier sauvegard√© sous kiali_latency_percentiles.csv.\") #Procesamiento completado. Archivo guardado como ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"results sahra2/kiali_kpi_metrics.csv\"  # Updated file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "# Define function to classify edge color based on success_rate\n",
    "def get_edge_color(success_rate):\n",
    "    if (success_rate > 0.95):\n",
    "        return \"green\"\n",
    "    elif (success_rate > 0.80):\n",
    "        return \"yellow\"\n",
    "    else:\n",
    "        return \"red\"\n",
    "\n",
    "# Define time windows\n",
    "start_date = data['timestamp'].min()\n",
    "end_date = data['timestamp'].max()\n",
    "window_sizes = [\"1T\", \"5T\", \"10T\", \"30T\"]  # Added more time windows\n",
    "\n",
    "# Generate graphs for each time window\n",
    "for window_size in window_sizes:\n",
    "    date_generated = pd.date_range(start_date, end_date, freq=window_size)\n",
    "    for i in range(len(date_generated) - 1):\n",
    "        start = date_generated[i]\n",
    "        end = date_generated[i + 1]\n",
    "        \n",
    "        # Filter data for the current time window\n",
    "        window_data = data[(data['timestamp'] >= start) & (data['timestamp'] < end)]\n",
    "        \n",
    "        # Create a directed graph\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        # Add edges and nodes with attributes\n",
    "        for _, row in window_data.iterrows():\n",
    "            src = row['source_workload']\n",
    "            dst = row['destination_workload']\n",
    "            success_rate = row['success_rate']\n",
    "            success_count = row['success_count']  # Added success_count\n",
    "            \n",
    "            # Add nodes\n",
    "            G.add_node(src)\n",
    "            G.add_node(dst)\n",
    "            \n",
    "            # Add edge with color and success_count attributes\n",
    "            G.add_edge(src, dst, weight=row['new_request'], color=get_edge_color(success_rate), success_count=success_count)\n",
    "        \n",
    "        # Get edge colors\n",
    "        edge_colors = [G.edges[edge]['color'] for edge in G.edges]\n",
    "        \n",
    "        # Plot the graph\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        pos = nx.spring_layout(G, seed=70,k=20)\n",
    "        nx.draw(G, pos, with_labels=True, node_color=\"lightblue\", edge_color=edge_colors, width=2, alpha=0.7,node_size=2800, font_size=8)\n",
    "        \n",
    "        # Add edge labels for success_count\n",
    "        edge_labels = {(u, v): f\"{d['success_count']}\" for u, v, d in G.edges(data=True)}\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=7)\n",
    "        \n",
    "        plt.title(f\"Graph from {start} to {end} (Window: {window_size})\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1¬™ (completa)\t\n",
    "#### MODELOS: GCN, GAT, SAGE\t \n",
    "#### VALIDACI√ìN CRUZADA: ‚úÖ S√≠\t\n",
    "#### MODULARIDAD: ‚úÖ Alta\n",
    "#### COMPLEJIDAD: üî• Alta\n",
    "#### IDEAL PARA: Comparar modelos, evaluar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Cargar datos\n",
    "data = pd.read_csv(\"results sahra2/kiali_latency_percentiles.csv\")\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "# Preprocesar columnas\n",
    "selected_columns = [\n",
    "    \"p50_latency\", \"p90_latency\", \"p95_latency\", \"p99_latency\",\n",
    "    \"istio_request_bytes\", \"success_rate\", \"throughput\", \"average_latency\"\n",
    "]\n",
    "for col in selected_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# Agrupar y normalizar\n",
    "grouped = data.groupby('source_workload')[selected_columns].mean()\n",
    "grouped.dropna(axis=1, how='all', inplace=True)  # Elimina columnas completamente vac√≠as\n",
    "\n",
    "# Escalar caracter√≠sticas\n",
    "scaler = StandardScaler()\n",
    "node_features_tensor = torch.tensor(scaler.fit_transform(grouped.values), dtype=torch.float)\n",
    "\n",
    "# Mapear nodos\n",
    "node_mapping = {node: idx for idx, node in enumerate(grouped.index)}\n",
    "\n",
    "# Crear edge_index (sin FutureWarning)\n",
    "edge_df = data[['source_workload', 'destination_workload']].copy()\n",
    "edge_df = edge_df[edge_df['source_workload'].isin(node_mapping) & edge_df['destination_workload'].isin(node_mapping)]\n",
    "edge_df['source_workload'] = edge_df['source_workload'].map(node_mapping)\n",
    "edge_df['destination_workload'] = edge_df['destination_workload'].map(node_mapping)\n",
    "edge_index_tensor = torch.tensor(edge_df.values.T, dtype=torch.long)\n",
    "\n",
    "# Etiquetas de anomal√≠a por latencia p99\n",
    "anomaly_series = data.groupby('source_workload')[\"p99_latency\"].mean()\n",
    "anomaly_series = anomaly_series.loc[grouped.index]\n",
    "threshold = anomaly_series.quantile(0.95)\n",
    "anomaly_labels = (anomaly_series > threshold).astype(int)\n",
    "anomaly_labels_tensor = torch.tensor(anomaly_labels.values, dtype=torch.float)\n",
    "\n",
    "# Objeto PyG\n",
    "data = Data(x=node_features_tensor, edge_index=edge_index_tensor)\n",
    "\n",
    "# Definici√≥n de modelo\n",
    "def get_model(model_type, in_channels, hidden_channels, out_channels):\n",
    "    class GNNModel(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            if model_type == \"GCN\":\n",
    "                self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "                self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "            elif model_type == \"GAT\":\n",
    "                self.conv1 = GATConv(in_channels, hidden_channels, heads=4, concat=False)\n",
    "                self.conv2 = GATConv(hidden_channels, out_channels, heads=4, concat=False)\n",
    "            elif model_type == \"GraphSAGE\":\n",
    "                self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "                self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "            else:\n",
    "                raise ValueError(\"Modelo no soportado\")\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            x = self.conv1(x, edge_index).relu()\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return x\n",
    "\n",
    "    return GNNModel()\n",
    "\n",
    "# Entrenamiento y evaluaci√≥n\n",
    "def train_and_evaluate(model_type, train_idx, test_idx, epochs=100, patience=10):\n",
    "    model = get_model(model_type, data.x.size(1), 32, 1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index).squeeze()\n",
    "        loss = F.binary_cross_entropy_with_logits(out[train_idx], anomaly_labels_tensor[train_idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index).squeeze()\n",
    "        probs = torch.sigmoid(out[test_idx])\n",
    "        preds = (probs > 0.5).int().numpy()\n",
    "        true = anomaly_labels_tensor[test_idx].int().numpy()\n",
    "\n",
    "        f1 = f1_score(true, preds)\n",
    "        acc = accuracy_score(true, preds)\n",
    "        try:\n",
    "            auc = roc_auc_score(true, probs.numpy())\n",
    "        except ValueError:\n",
    "            auc = float('nan')\n",
    "\n",
    "    return f1, acc, auc\n",
    "\n",
    "# Validaci√≥n cruzada\n",
    "y_np = anomaly_labels_tensor.numpy()\n",
    "indices = np.arange(len(y_np))\n",
    "\n",
    "minority_class_count = min(np.bincount(y_np.astype(int)))\n",
    "if minority_class_count < 2:\n",
    "    warnings.warn(\n",
    "        f\"No hay suficientes ejemplos en la clase minoritaria ({minority_class_count}) \"\n",
    "        \"para realizar validaci√≥n cruzada. Se omite esta fase.\"\n",
    "    )\n",
    "else:\n",
    "    n_splits = min(5, minority_class_count)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    print(\"\\n--- Validaci√≥n cruzada ---\")\n",
    "    for model_type in [\"GCN\", \"GAT\", \"GraphSAGE\"]:\n",
    "        f1s, accs, aucs = [], [], []\n",
    "        for train_idx, test_idx in skf.split(indices, y_np):\n",
    "            train_idx = torch.tensor(train_idx, dtype=torch.long)\n",
    "            test_idx = torch.tensor(test_idx, dtype=torch.long)\n",
    "            f1, acc, auc = train_and_evaluate(model_type, train_idx, test_idx)\n",
    "            f1s.append(f1)\n",
    "            accs.append(acc)\n",
    "            aucs.append(auc)\n",
    "\n",
    "        print(f\"\\nModelo: {model_type}\")\n",
    "        print(f\"F1 promedio: {np.mean(f1s):.4f}\")\n",
    "        print(f\"Accuracy promedio: {np.mean(accs):.4f}\")\n",
    "        print(f\"AUC-ROC promedio: {np.mean(aucs):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2¬™ (media)\n",
    "#### MODELO: Solo GCN\n",
    "#### VALIDACI√ìN CRUZADA: ‚ùå No\n",
    "#### MODULARIDAD: ‚úÖ Media\n",
    "#### COMPLEJIDAD: ‚≠ê Media\t\n",
    "#### IDEAL PARA: Prueba base de GCN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos\n",
    "data = pd.read_csv(\"results sahra2/kiali_latency_percentiles.csv\")\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "# Seleccionar m√©tricas relevantes\n",
    "selected_columns = [\n",
    "    \"p50_latency\", \"p90_latency\", \"p95_latency\", \"p99_latency\",\n",
    "    \"istio_request_bytes\", \"success_rate\", \"throughput\", \"average_latency\"\n",
    "]\n",
    "for col in selected_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# Agrupar por 'source_workload' y calcular la media\n",
    "grouped = data.groupby('source_workload')[selected_columns].mean()\n",
    "grouped.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "# Normalizar caracter√≠sticas\n",
    "scaler = StandardScaler()\n",
    "node_features = scaler.fit_transform(grouped.values)\n",
    "node_features_tensor = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "# Crear mapeo de nodos\n",
    "node_mapping = {node: idx for idx, node in enumerate(grouped.index)}\n",
    "\n",
    "# Crear edge_index\n",
    "edge_df = data[['source_workload', 'destination_workload']].dropna()\n",
    "edge_df = edge_df[edge_df['source_workload'].isin(node_mapping) & edge_df['destination_workload'].isin(node_mapping)]\n",
    "edge_df['source_workload'] = edge_df['source_workload'].map(node_mapping)\n",
    "edge_df['destination_workload'] = edge_df['destination_workload'].map(node_mapping)\n",
    "edge_index_tensor = torch.tensor(edge_df.values.T, dtype=torch.long)\n",
    "\n",
    "# Etiquetar anomal√≠as basadas en p99_latency\n",
    "anomaly_series = data.groupby('source_workload')[\"p99_latency\"].mean()\n",
    "anomaly_series = anomaly_series.loc[grouped.index]\n",
    "threshold = anomaly_series.quantile(0.95)\n",
    "anomaly_labels = (anomaly_series > threshold).astype(int)\n",
    "anomaly_labels_tensor = torch.tensor(anomaly_labels.values, dtype=torch.float)\n",
    "\n",
    "# Crear objeto Data de PyG\n",
    "graph_data = Data(x=node_features_tensor, edge_index=edge_index_tensor)\n",
    "\n",
    "# Definir modelo GCN (similar a AddGraph)\n",
    "class AddGraphModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Dividir datos en entrenamiento y prueba\n",
    "train_ratio = 0.8\n",
    "num_nodes = len(anomaly_labels_tensor)\n",
    "train_size = int(train_ratio * num_nodes)\n",
    "indices = np.random.permutation(num_nodes)\n",
    "train_idx = torch.tensor(indices[:train_size], dtype=torch.long)\n",
    "test_idx = torch.tensor(indices[train_size:], dtype=torch.long)\n",
    "\n",
    "# Crear modelo\n",
    "model = AddGraphModel(\n",
    "    in_channels=graph_data.x.size(1),\n",
    "    hidden_channels=32,\n",
    "    out_channels=1\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(graph_data.x, graph_data.edge_index).squeeze()\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(out[train_idx], anomaly_labels_tensor[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluar el modelo\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(graph_data.x, graph_data.edge_index).squeeze()\n",
    "    probs = torch.sigmoid(out[test_idx])\n",
    "    preds = (probs > 0.5).int().numpy()\n",
    "    true = anomaly_labels_tensor[test_idx].int().numpy()\n",
    "\n",
    "    f1 = f1_score(true, preds)\n",
    "    acc = accuracy_score(true, preds)\n",
    "    auc = roc_auc_score(true, probs.numpy()) if len(np.unique(true)) > 1 else float('nan')\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}, Accuracy: {acc:.4f}, AUC-ROC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module builds an AddGraph-style anomaly detection model using PyTorch Geometric (PyG). It processes microservice telemetry data (e.g., latency percentiles, throughput, success rate) collected over time.\n",
    "\n",
    "Key steps:\n",
    "\n",
    "Temporal Graph Construction: The system constructs dynamic graph snapshots using 1-minute time windows. Each node represents a microservice (source_workload), and edges represent calls between services.\n",
    "\n",
    "Feature Engineering: Metrics like p50‚Äìp99 latency, success rate, and throughput are averaged and normalized for each node in each snapshot.\n",
    "\n",
    "Anomaly Labeling: Nodes are labeled as anomalous if their p99_latency exceeds the 95th percentile in the global dataset during that window.\n",
    "\n",
    "GCN Model: A 2-layer Graph Convolutional Network (GCN) is trained to detect anomalous nodes based on their temporal behavior and structural context.\n",
    "\n",
    "Evaluation: The model is evaluated using F1 score, accuracy, and ROC AUC on the test snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instalar dependencias necesarias\n",
    "!pip install torch torchvision torchaudio torch-geometric pandas scikit-learn matplotlib tqdm --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Importar librer√≠as\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "# 3. Cargar el dataset\n",
    "df = pd.read_csv(\"results sahra2/kiali_latency_percentiles.csv\")\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# 4. Procesar columnas con listas como strings\n",
    "def parse_and_average(col):\n",
    "    return df[col].apply(lambda x: np.mean(ast.literal_eval(x)) if pd.notnull(x) and isinstance(x, str) else np.nan)\n",
    "\n",
    "list_columns = [\"average_latency\", \"throughput\", \"success_rate\", \"istio_request_bytes\"]\n",
    "for col in list_columns:\n",
    "    df[col + \"_mean\"] = parse_and_average(col)\n",
    "\n",
    "# 5. Definir las features finales\n",
    "features = [\n",
    "    \"p50_latency\", \"p90_latency\", \"p95_latency\", \"p99_latency\",\n",
    "    \"average_latency_mean\", \"throughput_mean\", \"success_rate_mean\", \"istio_request_bytes_mean\"\n",
    "]\n",
    "\n",
    "# Eliminar filas con valores nulos\n",
    "df = df.dropna(subset=[\"source_workload\", \"destination_workload\", \"timestamp\"] + features)\n",
    "\n",
    "# 6. Normalizar las features\n",
    "scaler = StandardScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "# 7. Crear nodos √∫nicos e √≠ndice\n",
    "nodes = pd.unique(df[[\"source_workload\", \"destination_workload\"]].values.ravel())\n",
    "node_map = {n: i for i, n in enumerate(nodes)}\n",
    "\n",
    "# 8. Crear snapshots por ventana de tiempo\n",
    "window_size = timedelta(minutes=1)\n",
    "start = df[\"timestamp\"].min()\n",
    "end = df[\"timestamp\"].max()\n",
    "\n",
    "snapshots = []\n",
    "labels = []\n",
    "\n",
    "while start + window_size <= end:\n",
    "    window = df[(df[\"timestamp\"] >= start) & (df[\"timestamp\"] < start + window_size)]\n",
    "    if len(window) == 0:\n",
    "        start += window_size\n",
    "        continue\n",
    "\n",
    "    src = window[\"source_workload\"].map(node_map).values\n",
    "    dst = window[\"destination_workload\"].map(node_map).values\n",
    "    edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "\n",
    "    grouped = window.groupby(\"source_workload\")[features].mean()\n",
    "    grouped = grouped.reindex(nodes).fillna(0)\n",
    "    x = torch.tensor(grouped.values, dtype=torch.float)\n",
    "\n",
    "    p99_series = window.groupby(\"source_workload\")[\"p99_latency\"].mean()\n",
    "    p99_series = p99_series.reindex(nodes).fillna(0)\n",
    "    threshold = df[\"p99_latency\"].quantile(0.95)\n",
    "    y = (p99_series > threshold).astype(int).values\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "    snapshots.append(Data(x=x, edge_index=edge_index))\n",
    "    labels.append(y)\n",
    "\n",
    "    start += window_size\n",
    "\n",
    "print(f\"Snapshots creados: {len(snapshots)}\")\n",
    "\n",
    "# 9. Definir el modelo GCN\n",
    "class AddGraphGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x.squeeze()\n",
    "\n",
    "model = AddGraphGCN(in_channels=len(features), hidden_channels=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 10. Entrenamiento\n",
    "split = int(0.8 * len(snapshots))\n",
    "train_snapshots = snapshots[:split]\n",
    "test_snapshots = snapshots[split:]\n",
    "test_labels = labels[split:]\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, g in enumerate(train_snapshots):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(g.x, g.edge_index)\n",
    "        target = labels[i]\n",
    "        loss = loss_fn(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")\n",
    "\n",
    "# 11. Evaluaci√≥n\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "for i, g in enumerate(test_snapshots):\n",
    "    with torch.no_grad():\n",
    "        out = model(g.x, g.edge_index)\n",
    "        probs = torch.sigmoid(out)\n",
    "        pred = (probs > 0.5).int().numpy()\n",
    "        true = test_labels[i].int().numpy()\n",
    "\n",
    "        all_preds.extend(pred.tolist())\n",
    "        all_true.extend(true.tolist())\n",
    "\n",
    "f1 = f1_score(all_true, all_preds)\n",
    "acc = accuracy_score(all_true, all_preds)\n",
    "auc = roc_auc_score(all_true, all_preds) if len(np.unique(all_true)) > 1 else float('nan')\n",
    "print(f\"\\nF1 Score: {f1:.4f} | Accuracy: {acc:.4f} | AUC-ROC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-geometric in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (3.11.16)\n",
      "Requirement already satisfied: fsspec in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (2024.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (2.1.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\marti\\appdata\\roaming\\python\\python310\\site-packages (from torch-geometric) (7.0.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (3.2.3)\n",
      "Requirement already satisfied: requests in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->torch-geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->torch-geometric) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->torch-geometric) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->torch-geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->torch-geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->torch-geometric) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->torch-geometric) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->torch-geometric) (1.19.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch-geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torch-geometric) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torch-geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torch-geometric) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torch-geometric) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\marti\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->torch-geometric) (0.4.6)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\marti\\appdata\\roaming\\python\\python310\\site-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.13.1)\n",
      "Requirement already satisfied: torch-geometric-temporal in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric-temporal) (3.4.2)\n",
      "Requirement already satisfied: six in c:\\users\\marti\\appdata\\roaming\\python\\python310\\site-packages (from torch-geometric-temporal) (1.17.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\marti\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\marti\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\marti\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\marti\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Instalar PyTorch (elige la versi√≥n seg√∫n tu sistema; esta es para CPU)\n",
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# Instalar PyTorch Geometric y Torch-Geometric-Temporal\n",
    "!pip install torch-geometric\n",
    "!pip install torch-geometric-temporal\n",
    "\n",
    "# Instalar otras dependencias √∫tiles\n",
    "!pip install pandas scikit-learn matplotlib tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric_temporal.nn import TGNMemory, TemporalConv\n",
    "from torch_geometric_temporal.signal import StaticGraphTemporalSignal, temporal_signal_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Cargar datos\n",
    "df = pd.read_csv(\"results sahra2/kiali_latency_percentiles.csv\")\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# 2. Detectar anomal√≠as por percentil 95 de p99_latency\n",
    "threshold = df[\"p99_latency\"].quantile(0.95)\n",
    "df[\"anomaly\"] = (df[\"p99_latency\"] > threshold).astype(int)\n",
    "\n",
    "# 3. Crear snapshots por minuto\n",
    "snapshots = []\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for ts, group in df.groupby(df['timestamp'].dt.floor(\"min\")):\n",
    "    nodes = pd.concat([group[\"source_workload\"], group[\"destination_workload\"]]).dropna().unique()\n",
    "    node_map = {n: i for i, n in enumerate(nodes)}\n",
    "    \n",
    "    # Crear edge_index\n",
    "    edges = group[[\"source_workload\", \"destination_workload\"]].dropna()\n",
    "    if edges.empty: continue\n",
    "    edge_idx = edges.applymap(node_map.get).dropna().astype(int).values.T\n",
    "    edge_index = torch.tensor(edge_idx, dtype=torch.long)\n",
    "\n",
    "    # edge_attr: usar p99_latency o throughput como atributos\n",
    "    edge_attr_values = group[\"p99_latency\"].fillna(0).values[:edge_index.shape[1]]\n",
    "    edge_attr = torch.tensor(edge_attr_values.reshape(-1, 1), dtype=torch.float)\n",
    "\n",
    "    # Atributos de nodo\n",
    "    node_df = group.groupby(\"source_workload\")[\"p99_latency\"].mean().reindex(nodes).fillna(0).values\n",
    "    x = torch.tensor(scaler.fit_transform(node_df.reshape(-1, 1)), dtype=torch.float)\n",
    "\n",
    "    # Etiquetas (1 si hubo anomal√≠a en nodo)\n",
    "    y_df = group.groupby(\"source_workload\")[\"anomaly\"].max().reindex(nodes).fillna(0).values\n",
    "    y = torch.tensor(y_df.astype(int), dtype=torch.long)\n",
    "\n",
    "    snapshots.append((x, edge_index, edge_attr, y))\n",
    "\n",
    "# 4. Crear dataset temporal\n",
    "dataset = StaticGraphTemporalSignal(\n",
    "    edge_indices=[s[1] for s in snapshots],\n",
    "    edge_features=[s[2] for s in snapshots],\n",
    "    features=[s[0] for s in snapshots],\n",
    "    targets=[s[3] for s in snapshots]\n",
    ")\n",
    "\n",
    "# 5. Dividir en entrenamiento y prueba\n",
    "train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)\n",
    "\n",
    "# 6. Definir modelo temporal\n",
    "class TGN(torch.nn.Module):\n",
    "    def __init__(self, node_features, edge_features, memory_dim, time_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.memory = TGNMemory(\n",
    "            node_features=node_features,\n",
    "            edge_features=edge_features,\n",
    "            memory_dimension=memory_dim,\n",
    "            time_dimension=time_dim,\n",
    "            message_dimension=memory_dim,\n",
    "        )\n",
    "        self.temporal_conv = TemporalConv(in_channels=memory_dim, out_channels=output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, time=None):\n",
    "        mem = self.memory(x, edge_index, edge_attr, time)\n",
    "        return self.temporal_conv(mem, edge_index)\n",
    "\n",
    "# 7. Instanciar modelo\n",
    "model = TGN(node_features=1, edge_features=1, memory_dim=32, time_dim=16, output_dim=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 8. Entrenamiento\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for snapshot in train_dataset:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
    "        loss = F.binary_cross_entropy_with_logits(out.squeeze(), snapshot.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# 9. Evaluaci√≥n\n",
    "model.eval()\n",
    "f1s, accs, aucs = [], [], []\n",
    "\n",
    "for snapshot in test_dataset:\n",
    "    with torch.no_grad():\n",
    "        out = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
    "        probs = torch.sigmoid(out.squeeze())\n",
    "        preds = (probs > 0.5).int()\n",
    "        y_true = snapshot.y.cpu().numpy()\n",
    "        y_pred = preds.cpu().numpy()\n",
    "        y_prob = probs.cpu().numpy()\n",
    "\n",
    "        f1s.append(f1_score(y_true, y_pred, zero_division=0))\n",
    "        accs.append(accuracy_score(y_true, y_pred))\n",
    "        if len(np.unique(y_true)) > 1:\n",
    "            aucs.append(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "print(f\"\\nF1 Score: {np.mean(f1s):.4f}\")\n",
    "print(f\"Accuracy: {np.mean(accs):.4f}\")\n",
    "print(f\"AUC-ROC: {np.mean(aucs) if aucs else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_geometric_temporal.nn.recurrent'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch_geometric_temporal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecurrent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DCRNN\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch_geometric_temporal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m temporal_signal_split\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m f1_score, accuracy_score, roc_auc_score\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric_temporal.nn.recurrent'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric_temporal.nn.recurrent import DCRNN\n",
    "from torch_geometric_temporal.signal import temporal_signal_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Definir el modelo ST-GNN\n",
    "class STGNN(torch.nn.Module):\n",
    "    def __init__(self, node_features, hidden_channels, output_dim):\n",
    "        super(STGNN, self).__init__()\n",
    "        self.recurrent = DCRNN(node_features, hidden_channels, K=2)  # Diffusion Convolutional Recurrent Network\n",
    "        self.linear = torch.nn.Linear(hidden_channels, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        h = self.recurrent(x, edge_index, edge_weight)\n",
    "        out = self.linear(h)\n",
    "        return out\n",
    "\n",
    "# Cargar datos\n",
    "data = pd.read_csv(\"results sahra2/kiali_latency_percentiles.csv\")\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "# Seleccionar m√©tricas relevantes\n",
    "selected_columns = [\n",
    "    \"p50_latency\", \"p90_latency\", \"p95_latency\", \"p99_latency\",\n",
    "    \"istio_request_bytes\", \"success_rate\", \"throughput\", \"average_latency\"\n",
    "]\n",
    "for col in selected_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# Agrupar por 'source_workload' y calcular la media\n",
    "grouped = data.groupby(['timestamp', 'source_workload'])[selected_columns].mean().reset_index()\n",
    "\n",
    "# Crear nodos y aristas para cada snapshot temporal\n",
    "timestamps = grouped['timestamp'].unique()\n",
    "edge_indices, edge_weights, node_features, labels = [], [], [], []\n",
    "\n",
    "for timestamp in timestamps:\n",
    "    snapshot = grouped[grouped['timestamp'] == timestamp]\n",
    "    \n",
    "    # Crear nodos (caracter√≠sticas)\n",
    "    node_features.append(torch.tensor(snapshot[selected_columns].values, dtype=torch.float))\n",
    "    \n",
    "    # Crear aristas (relaciones entre microservicios)\n",
    "    edges = data[data['timestamp'] == timestamp][['source_workload', 'destination_workload']].dropna()\n",
    "    edge_index = torch.tensor(edges.values.T, dtype=torch.long)\n",
    "    edge_indices.append(edge_index)\n",
    "    \n",
    "    # Crear pesos de aristas (por ejemplo, tasa de √©xito)\n",
    "    edge_weight = torch.tensor(edges['success_rate'].values, dtype=torch.float)\n",
    "    edge_weights.append(edge_weight)\n",
    "    \n",
    "    # Etiquetas de anomal√≠as (por ejemplo, basadas en p99_latency)\n",
    "    anomaly_labels = (snapshot['p99_latency'] > snapshot['p99_latency'].quantile(0.95)).astype(int)\n",
    "    labels.append(torch.tensor(anomaly_labels.values, dtype=torch.float))\n",
    "\n",
    "# Crear dataset temporal\n",
    "from torch_geometric_temporal.signal import StaticGraphTemporalSignal\n",
    "dataset = StaticGraphTemporalSignal(edge_indices, edge_weights, node_features, timestamps, labels)\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)\n",
    "\n",
    "# Entrenar el modelo\n",
    "model = STGNN(node_features=len(selected_columns), hidden_channels=32, output_dim=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    for snapshot in train_dataset:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(snapshot.x, snapshot.edge_index, snapshot.edge_weight)\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(out, snapshot.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluar el modelo\n",
    "model.eval()\n",
    "f1_scores, accuracies, aucs = [], [], []\n",
    "for snapshot in test_dataset:\n",
    "    with torch.no_grad():\n",
    "        out = model(snapshot.x, snapshot.edge_index, snapshot.edge_weight)\n",
    "        preds = (torch.sigmoid(out) > 0.5).int()\n",
    "        f1_scores.append(f1_score(snapshot.y, preds))\n",
    "        accuracies.append(accuracy_score(snapshot.y, preds))\n",
    "        aucs.append(roc_auc_score(snapshot.y, torch.sigmoid(out)))\n",
    "\n",
    "print(f\"F1 Score: {np.mean(f1_scores):.4f}, Accuracy: {np.mean(accuracies):.4f}, AUC-ROC: {np.mean(aucs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DCRNN +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric_temporal.nn.recurrent import DCRNN\n",
    "from torch_geometric_temporal.signal import temporal_signal_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Definir el modelo ST-GNN\n",
    "class STGNN(torch.nn.Module):\n",
    "    def __init__(self, node_features, hidden_channels, output_dim):\n",
    "        super(STGNN, self).__init__()\n",
    "        self.recurrent = DCRNN(node_features, hidden_channels, K=2)  # Diffusion Convolutional Recurrent Network\n",
    "        self.linear = torch.nn.Linear(hidden_channels, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        h = self.recurrent(x, edge_index, edge_weight)\n",
    "        out = self.linear(h)\n",
    "        return out\n",
    "\n",
    "# Cargar datos\n",
    "data = pd.read_csv(\"results sahra2/kiali_latency_percentiles.csv\")\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "# Seleccionar m√©tricas relevantes\n",
    "selected_columns = [\n",
    "    \"p50_latency\", \"p90_latency\", \"p95_latency\", \"p99_latency\",\n",
    "    \"istio_request_bytes\", \"success_rate\", \"throughput\", \"average_latency\"\n",
    "]\n",
    "for col in selected_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# Agrupar por 'source_workload' y calcular la media\n",
    "grouped = data.groupby(['timestamp', 'source_workload'])[selected_columns].mean().reset_index()\n",
    "\n",
    "# Crear nodos y aristas para cada snapshot temporal\n",
    "timestamps = grouped['timestamp'].unique()\n",
    "edge_indices, edge_weights, node_features, labels = [], [], [], []\n",
    "\n",
    "for timestamp in timestamps:\n",
    "    snapshot = grouped[grouped['timestamp'] == timestamp]\n",
    "    \n",
    "    # Crear nodos (caracter√≠sticas)\n",
    "    node_features.append(torch.tensor(snapshot[selected_columns].values, dtype=torch.float))\n",
    "    \n",
    "    # Crear aristas (relaciones entre microservicios)\n",
    "    edges = data[data['timestamp'] == timestamp][['source_workload', 'destination_workload']].dropna()\n",
    "    edge_index = torch.tensor(edges.values.T, dtype=torch.long)\n",
    "    edge_indices.append(edge_index)\n",
    "    \n",
    "    # Crear pesos de aristas (por ejemplo, tasa de √©xito)\n",
    "    edge_weight = torch.tensor(edges['success_rate'].values, dtype=torch.float)\n",
    "    edge_weights.append(edge_weight)\n",
    "    \n",
    "    # Etiquetas de anomal√≠as (basadas en p99 y p95)\n",
    "    anomaly_labels_p99 = (snapshot['p99_latency'] > snapshot['p99_latency'].quantile(0.95)).astype(int)\n",
    "    anomaly_labels_p95 = (snapshot['p95_latency'] > snapshot['p95_latency'].quantile(0.95)).astype(int)\n",
    "    combined_labels = (anomaly_labels_p99 | anomaly_labels_p95).astype(int)  # Etiqueta combinada\n",
    "    labels.append(torch.tensor(combined_labels.values, dtype=torch.float))\n",
    "\n",
    "# Crear dataset temporal\n",
    "from torch_geometric_temporal.signal import StaticGraphTemporalSignal\n",
    "dataset = StaticGraphTemporalSignal(edge_indices, edge_weights, node_features, timestamps, labels)\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)\n",
    "\n",
    "# Entrenar el modelo\n",
    "model = STGNN(node_features=len(selected_columns), hidden_channels=32, output_dim=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    for snapshot in train_dataset:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(snapshot.x, snapshot.edge_index, snapshot.edge_weight)\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(out, snapshot.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluar el modelo\n",
    "model.eval()\n",
    "f1_scores, accuracies, aucs = [], [], []\n",
    "for snapshot in test_dataset:\n",
    "    with torch.no_grad():\n",
    "        out = model(snapshot.x, snapshot.edge_index, snapshot.edge_weight)\n",
    "        preds = (torch.sigmoid(out) > 0.5).int()\n",
    "        f1_scores.append(f1_score(snapshot.y, preds))\n",
    "        accuracies.append(accuracy_score(snapshot.y, preds))\n",
    "        aucs.append(roc_auc_score(snapshot.y, torch.sigmoid(out)))\n",
    "\n",
    "print(f\"F1 Score: {np.mean(f1_scores):.4f}, Accuracy: {np.mean(accuracies):.4f}, AUC-ROC: {np.mean(aucs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KGROOT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install dependencies\n",
    "!pip install dgl torch pandas scikit-learn tqdm --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dgl.nn import GraphConv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import timedelta\n",
    "\n",
    "# 3. Load and preprocess dataset\n",
    "df = pd.read_csv(\"istio_request_2.2.csv\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.sort_values(\"timestamp\")\n",
    "\n",
    "# Calculate avg latency\n",
    "df[\"avg_latency\"] = df[\"istio_request_duration_milliseconds_sum\"] / df[\"total_request\"]\n",
    "df[\"avg_latency\"].fillna(0, inplace=True)\n",
    "\n",
    "# Label relation type\n",
    "def classify_relation(row, latency_thres):\n",
    "    if row[\"avg_latency\"] > latency_thres:\n",
    "        return \"high_latency\"\n",
    "    elif row[\"response_code\"] >= 500:\n",
    "        return \"error_5xx\"\n",
    "    elif row[\"response_code\"] >= 400:\n",
    "        return \"error_4xx\"\n",
    "    else:\n",
    "        return \"success\"\n",
    "\n",
    "lat_thres = df[\"avg_latency\"].quantile(0.95)\n",
    "df[\"relation\"] = df.apply(lambda r: classify_relation(r, lat_thres), axis=1)\n",
    "\n",
    "# 4. Encode services and relations\n",
    "all_services = pd.unique(df[[\"source_workload\", \"destination_workload\"]].values.ravel())\n",
    "service2id = {svc: i for i, svc in enumerate(all_services)}\n",
    "relation2id = {\"success\": 0, \"error_4xx\": 1, \"error_5xx\": 2, \"high_latency\": 3}\n",
    "\n",
    "df[\"src_id\"] = df[\"source_workload\"].map(service2id)\n",
    "df[\"dst_id\"] = df[\"destination_workload\"].map(service2id)\n",
    "df[\"rel_id\"] = df[\"relation\"].map(relation2id)\n",
    "\n",
    "# Normalize latency\n",
    "scaler = MinMaxScaler()\n",
    "df[\"latency_norm\"] = scaler.fit_transform(df[[\"avg_latency\"]])\n",
    "\n",
    "# 5. Create graph snapshots (historical and current)\n",
    "snapshots = []\n",
    "labels = []\n",
    "\n",
    "window_size = timedelta(minutes=1)\n",
    "start_time = df[\"timestamp\"].min()\n",
    "end_time = df[\"timestamp\"].max()\n",
    "timepoints = []\n",
    "\n",
    "while start_time + window_size < end_time:\n",
    "    window_df = df[(df[\"timestamp\"] >= start_time) & (df[\"timestamp\"] < start_time + window_size)]\n",
    "    if len(window_df) > 0:\n",
    "        snapshots.append(window_df.copy())\n",
    "        timepoints.append(start_time)\n",
    "    start_time += window_size\n",
    "\n",
    "# Use last snapshot as current graph, previous as historical\n",
    "historical_graphs = snapshots[:-1]\n",
    "current_graph_df = snapshots[-1]\n",
    "\n",
    "# 6. GCN model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hidden_feats)\n",
    "        self.conv2 = GraphConv(hidden_feats, 1)\n",
    "\n",
    "    def forward(self, g, feat):\n",
    "        h = F.relu(self.conv1(g, feat))\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "# 7. Prepare historical training data\n",
    "graphs = []\n",
    "edge_labels = []\n",
    "\n",
    "for snapshot in historical_graphs:\n",
    "    g = dgl.graph((snapshot[\"src_id\"], snapshot[\"dst_id\"]), num_nodes=len(service2id))\n",
    "    edge_feat = torch.tensor(snapshot[\"latency_norm\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "    rel_feat = torch.tensor(snapshot[\"rel_id\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "    g.edata[\"feat\"] = torch.cat([edge_feat, rel_feat], dim=1)\n",
    "    graphs.append(g)\n",
    "\n",
    "# Create training graph as average over history\n",
    "def average_edge_features(graphs):\n",
    "    edge_feats = [g.edata[\"feat\"] for g in graphs]\n",
    "    stacked = torch.stack([F.pad(e, (0, 0, 0, max([g.num_edges() for g in graphs]) - e.size(0))) for e in edge_feats])\n",
    "    return stacked.mean(0)\n",
    "\n",
    "avg_feat = average_edge_features(graphs)\n",
    "train_g = dgl.graph((current_graph_df[\"src_id\"], current_graph_df[\"dst_id\"]), num_nodes=len(service2id))\n",
    "edge_feat = torch.tensor(current_graph_df[\"latency_norm\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "rel_feat = torch.tensor(current_graph_df[\"rel_id\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "train_g.edata[\"feat\"] = torch.cat([edge_feat, rel_feat], dim=1)\n",
    "\n",
    "# 8. Train GCN to reconstruct historical pattern\n",
    "model = GCN(in_feats=2, hidden_feats=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    pred = model(train_g, train_g.edata[\"feat\"])\n",
    "    loss = loss_fn(pred, avg_feat[:len(pred)])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 9. Anomaly score: deviation from historical pattern\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(train_g, train_g.edata[\"feat\"])\n",
    "    scores = torch.abs(pred.squeeze() - avg_feat[:len(pred)].squeeze())\n",
    "    topk = torch.topk(scores, k=10)\n",
    "    print(\"\\nTop-10 Anomalous Interactions:\")\n",
    "    for i in topk.indices:\n",
    "        src = current_graph_df.iloc[i][\"source_workload\"]\n",
    "        dst = current_graph_df.iloc[i][\"destination_workload\"]\n",
    "        r = current_graph_df.iloc[i][\"relation\"]\n",
    "        print(f\"{src} ‚Üí {dst} | relation: {r} | anomaly score: {scores[i].item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n con NetworkX y Matplotlib\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear grafo dirigido\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# A√±adir nodos\n",
    "for node_id in service2id.values():\n",
    "    G.add_node(node_id)\n",
    "\n",
    "# A√±adir aristas con anomal√≠a como atributo\n",
    "for i, row in current_graph_df.iterrows():\n",
    "    src = service2id[row[\"source_workload\"]]\n",
    "    dst = service2id[row[\"destination_workload\"]]\n",
    "    score = scores[i].item()\n",
    "    G.add_edge(src, dst, score=score)\n",
    "\n",
    "# Posiciones para layout\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "# Crear lista de colores para las aristas seg√∫n score\n",
    "edge_colors = []\n",
    "edge_widths = []\n",
    "threshold = torch.quantile(scores, 0.90).item()\n",
    "\n",
    "for u, v in G.edges():\n",
    "    s = G[u][v]['score']\n",
    "    if s > threshold:\n",
    "        edge_colors.append('red')\n",
    "        edge_widths.append(2.5)\n",
    "    else:\n",
    "        edge_colors.append('gray')\n",
    "        edge_widths.append(0.8)\n",
    "\n",
    "# Etiquetas de nodos\n",
    "id2service = {v: k for k, v in service2id.items()}\n",
    "node_labels = {node: id2service[node] for node in G.nodes()}\n",
    "\n",
    "# Dibujar grafo\n",
    "plt.figure(figsize=(14, 10))\n",
    "nx.draw_networkx_nodes(G, pos, node_size=700, node_color='skyblue')\n",
    "nx.draw_networkx_edges(G, pos, edge_color=edge_colors, width=edge_widths, arrows=True)\n",
    "nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=10)\n",
    "\n",
    "plt.title(\"KGroot-based Anomaly Graph\\n(Red edges are anomalous)\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
