{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bce176ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0           timestamp  \\\n",
      "0           0 2025-04-21 20:41:30   \n",
      "1           1 2025-04-21 20:41:31   \n",
      "2           2 2025-04-21 20:41:32   \n",
      "3           3 2025-04-21 20:41:33   \n",
      "4           4 2025-04-21 20:41:34   \n",
      "\n",
      "   adservice-5dc4c759b6-w5p8z_container_cpu_usage_seconds_total  \\\n",
      "0                                           0.014576              \n",
      "1                                           0.014576              \n",
      "2                                           0.014576              \n",
      "3                                           0.014576              \n",
      "4                                           0.014576              \n",
      "\n",
      "   adservice-5dc4c759b6-w5p8z_container_memory_working_set_bytes  \\\n",
      "0                                       1.350542e+06               \n",
      "1                                       1.350542e+06               \n",
      "2                                       1.350542e+06               \n",
      "3                                       1.350542e+06               \n",
      "4                                       1.350542e+06               \n",
      "\n",
      "   adservice-5dc4c759b6-w5p8z_container_network_transmit_packets_total  \\\n",
      "0                                           4.341871                     \n",
      "1                                           4.341871                     \n",
      "2                                           4.341871                     \n",
      "3                                           4.341871                     \n",
      "4                                           4.341871                     \n",
      "\n",
      "  adservice-5dc4c759b6-w5p8z_deployed_at  \\\n",
      "0      aks-agentpool-42554999-vmss000011   \n",
      "1      aks-agentpool-42554999-vmss000011   \n",
      "2      aks-agentpool-42554999-vmss000011   \n",
      "3      aks-agentpool-42554999-vmss000011   \n",
      "4      aks-agentpool-42554999-vmss000011   \n",
      "\n",
      "   cartservice-7f84c9f647-bltfq_container_cpu_usage_seconds_total  \\\n",
      "0                                           0.008088                \n",
      "1                                           0.008088                \n",
      "2                                           0.008088                \n",
      "3                                           0.008088                \n",
      "4                                           0.008246                \n",
      "\n",
      "   cartservice-7f84c9f647-bltfq_container_memory_working_set_bytes  \\\n",
      "0                                       1.749457e+06                 \n",
      "1                                       1.749457e+06                 \n",
      "2                                       1.749457e+06                 \n",
      "3                                       1.749457e+06                 \n",
      "4                                       1.240170e+06                 \n",
      "\n",
      "   cartservice-7f84c9f647-bltfq_container_network_transmit_packets_total  \\\n",
      "0                                           9.461806                       \n",
      "1                                           9.461806                       \n",
      "2                                           9.461806                       \n",
      "3                                           9.461806                       \n",
      "4                                           9.461806                       \n",
      "\n",
      "  cartservice-7f84c9f647-bltfq_deployed_at  ...  \\\n",
      "0        aks-agentpool-42554999-vmss00000y  ...   \n",
      "1        aks-agentpool-42554999-vmss00000y  ...   \n",
      "2        aks-agentpool-42554999-vmss00000y  ...   \n",
      "3        aks-agentpool-42554999-vmss00000y  ...   \n",
      "4        aks-agentpool-42554999-vmss00000y  ...   \n",
      "\n",
      "   loadgenerator-7785849b66-vhwdf_container_network_transmit_packets_total  \\\n",
      "0                                                NaN                         \n",
      "1                                                NaN                         \n",
      "2                                                NaN                         \n",
      "3                                                NaN                         \n",
      "4                                                NaN                         \n",
      "\n",
      "   loadgenerator-7785849b66-vhwdf_deployed_at  \\\n",
      "0                                         NaN   \n",
      "1                                         NaN   \n",
      "2                                         NaN   \n",
      "3                                         NaN   \n",
      "4                                         NaN   \n",
      "\n",
      "   loadgenerator-64fb74c986-62785_container_cpu_usage_seconds_total  \\\n",
      "0                                                NaN                  \n",
      "1                                                NaN                  \n",
      "2                                                NaN                  \n",
      "3                                                NaN                  \n",
      "4                                                NaN                  \n",
      "\n",
      "  loadgenerator-64fb74c986-62785_container_memory_working_set_bytes  \\\n",
      "0                                                NaN                  \n",
      "1                                                NaN                  \n",
      "2                                                NaN                  \n",
      "3                                                NaN                  \n",
      "4                                                NaN                  \n",
      "\n",
      "   loadgenerator-64fb74c986-62785_container_network_transmit_packets_total  \\\n",
      "0                                                NaN                         \n",
      "1                                                NaN                         \n",
      "2                                                NaN                         \n",
      "3                                                NaN                         \n",
      "4                                                NaN                         \n",
      "\n",
      "   loadgenerator-64fb74c986-62785_deployed_at  \\\n",
      "0                                         NaN   \n",
      "1                                         NaN   \n",
      "2                                         NaN   \n",
      "3                                         NaN   \n",
      "4                                         NaN   \n",
      "\n",
      "   loadgenerator-64fb74c986-9fjcn_container_cpu_usage_seconds_total  \\\n",
      "0                                                NaN                  \n",
      "1                                                NaN                  \n",
      "2                                                NaN                  \n",
      "3                                                NaN                  \n",
      "4                                                NaN                  \n",
      "\n",
      "  loadgenerator-64fb74c986-9fjcn_container_memory_working_set_bytes  \\\n",
      "0                                                NaN                  \n",
      "1                                                NaN                  \n",
      "2                                                NaN                  \n",
      "3                                                NaN                  \n",
      "4                                                NaN                  \n",
      "\n",
      "   loadgenerator-64fb74c986-9fjcn_container_network_transmit_packets_total  \\\n",
      "0                                                NaN                         \n",
      "1                                                NaN                         \n",
      "2                                                NaN                         \n",
      "3                                                NaN                         \n",
      "4                                                NaN                         \n",
      "\n",
      "   loadgenerator-64fb74c986-9fjcn_deployed_at  \n",
      "0                                         NaN  \n",
      "1                                         NaN  \n",
      "2                                         NaN  \n",
      "3                                         NaN  \n",
      "4                                         NaN  \n",
      "\n",
      "[5 rows x 265 columns]\n",
      "             timestamp        source_workload   destination_workload  \\\n",
      "0  2025-04-21 20:41:30               frontend              adservice   \n",
      "1  2025-04-21 20:41:30        checkoutservice        currencyservice   \n",
      "2  2025-04-21 20:41:30  recommendationservice  productcatalogservice   \n",
      "3  2025-04-21 20:41:30               frontend        checkoutservice   \n",
      "4  2025-04-21 20:41:30               frontend  recommendationservice   \n",
      "\n",
      "  request_protocol response_flags     reporter  response_code  \\\n",
      "0             grpc              -  destination            200   \n",
      "1             grpc              -  destination            200   \n",
      "2             grpc              -  destination            200   \n",
      "3             grpc              -  destination            200   \n",
      "4             grpc              -  destination            200   \n",
      "\n",
      "   grpc_response_status  total_request  istio_request_bytes_sum  \\\n",
      "0                   0.0          122.0                  21838.5   \n",
      "1                   0.0            7.0                   2430.0   \n",
      "2                   0.0          162.0                  29143.0   \n",
      "3                   0.0            3.0                   2340.0   \n",
      "4                   0.0          165.0                  37940.0   \n",
      "\n",
      "   istio_request_duration_milliseconds_sum   Microservice Experiment  \n",
      "0                                   457.50  adservice_cpu          1  \n",
      "1                                     3.15  adservice_cpu          1  \n",
      "2                                     0.00  adservice_cpu          1  \n",
      "3                                   155.00  adservice_cpu          1  \n",
      "4                                   383.55  adservice_cpu          1  \n",
      "✅ Fichiers 'final_pod_resource_baro_dataset.csv' et 'final_pod_communication_dataset.csv' enregistrés avec succès !\n",
      "📌 Distribution des classes :\n",
      " Abnormality class\n",
      "Normal          3420\n",
      "Packet Delay     912\n",
      "Packet Loss      911\n",
      "CPU HOG          906\n",
      "MEM LEAK         727\n",
      "Name: count, dtype: int64\n",
      "📌 Colonnes disponibles : Index(['Unnamed: 0', 'timestamp',\n",
      "       'adservice-5dc4c759b6-w5p8z_container_cpu_usage_seconds_total',\n",
      "       'adservice-5dc4c759b6-w5p8z_container_memory_working_set_bytes',\n",
      "       'adservice-5dc4c759b6-w5p8z_container_network_transmit_packets_total',\n",
      "       'adservice-5dc4c759b6-w5p8z_deployed_at',\n",
      "       'cartservice-7f84c9f647-bltfq_container_cpu_usage_seconds_total',\n",
      "       'cartservice-7f84c9f647-bltfq_container_memory_working_set_bytes',\n",
      "       'cartservice-7f84c9f647-bltfq_container_network_transmit_packets_total',\n",
      "       'cartservice-7f84c9f647-bltfq_deployed_at',\n",
      "       ...\n",
      "       'loadgenerator-7785849b66-vhwdf_container_network_transmit_packets_total',\n",
      "       'loadgenerator-7785849b66-vhwdf_deployed_at',\n",
      "       'loadgenerator-64fb74c986-62785_container_cpu_usage_seconds_total',\n",
      "       'loadgenerator-64fb74c986-62785_container_memory_working_set_bytes',\n",
      "       'loadgenerator-64fb74c986-62785_container_network_transmit_packets_total',\n",
      "       'loadgenerator-64fb74c986-62785_deployed_at',\n",
      "       'loadgenerator-64fb74c986-9fjcn_container_cpu_usage_seconds_total',\n",
      "       'loadgenerator-64fb74c986-9fjcn_container_memory_working_set_bytes',\n",
      "       'loadgenerator-64fb74c986-9fjcn_container_network_transmit_packets_total',\n",
      "       'loadgenerator-64fb74c986-9fjcn_deployed_at'],\n",
      "      dtype='object', length=265)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2581608487.py:99: DtypeWarning: Columns (29,37,88,92,96,100,128,132,136,140,144,148,152,156,160,164,168,172,176,180,184,188,192,196,200,204,208,212,216,220,224,228,232,236,240,244,248,252,256,260,264) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"final_pod_resource_baro_dataset.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Taille du jeu de train : (5500, 263)\n",
      "✔️ Taille du jeu de test : (1376, 263)\n",
      "\n",
      "🔹 Entraînement du modèle Random Forest...\n",
      "📌 Modèle : Random Forest\n",
      "🔹 Accuracy : 1.0000\n",
      "🔹 Rapport de classification : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     CPU HOG       1.00      1.00      1.00       181\n",
      "    MEM LEAK       1.00      1.00      1.00       146\n",
      "      Normal       1.00      1.00      1.00       684\n",
      "Packet Delay       1.00      1.00      1.00       183\n",
      " Packet Loss       1.00      1.00      1.00       182\n",
      "\n",
      "    accuracy                           1.00      1376\n",
      "   macro avg       1.00      1.00      1.00      1376\n",
      "weighted avg       1.00      1.00      1.00      1376\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "🔹 Entraînement du modèle Decision Tree...\n",
      "📌 Modèle : Decision Tree\n",
      "🔹 Accuracy : 1.0000\n",
      "🔹 Rapport de classification : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     CPU HOG       1.00      1.00      1.00       181\n",
      "    MEM LEAK       1.00      1.00      1.00       146\n",
      "      Normal       1.00      1.00      1.00       684\n",
      "Packet Delay       1.00      1.00      1.00       183\n",
      " Packet Loss       1.00      1.00      1.00       182\n",
      "\n",
      "    accuracy                           1.00      1376\n",
      "   macro avg       1.00      1.00      1.00      1376\n",
      "weighted avg       1.00      1.00      1.00      1376\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "🔹 Entraînement du modèle SVM...\n",
      "📌 Modèle : SVM\n",
      "🔹 Accuracy : 0.9847\n",
      "🔹 Rapport de classification : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     CPU HOG       0.99      0.99      0.99       181\n",
      "    MEM LEAK       0.99      1.00      1.00       146\n",
      "      Normal       0.98      0.99      0.98       684\n",
      "Packet Delay       0.98      0.99      0.99       183\n",
      " Packet Loss       1.00      0.93      0.97       182\n",
      "\n",
      "    accuracy                           0.98      1376\n",
      "   macro avg       0.99      0.98      0.99      1376\n",
      "weighted avg       0.98      0.98      0.98      1376\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "🔹 Entraînement du modèle KNN...\n",
      "📌 Modèle : KNN\n",
      "🔹 Accuracy : 0.9906\n",
      "🔹 Rapport de classification : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     CPU HOG       0.99      0.99      0.99       181\n",
      "    MEM LEAK       0.99      0.98      0.99       146\n",
      "      Normal       0.99      0.99      0.99       684\n",
      "Packet Delay       0.98      1.00      0.99       183\n",
      " Packet Loss       1.00      0.98      0.99       182\n",
      "\n",
      "    accuracy                           0.99      1376\n",
      "   macro avg       0.99      0.99      0.99      1376\n",
      "weighted avg       0.99      0.99      0.99      1376\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "🔹 Entraînement du modèle Naive Bayes...\n",
      "📌 Modèle : Naive Bayes\n",
      "🔹 Accuracy : 0.5116\n",
      "🔹 Rapport de classification : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     CPU HOG       0.63      1.00      0.77       181\n",
      "    MEM LEAK       0.39      1.00      0.56       146\n",
      "      Normal       1.00      0.02      0.04       684\n",
      "Packet Delay       0.49      1.00      0.66       183\n",
      " Packet Loss       0.55      0.99      0.71       182\n",
      "\n",
      "    accuracy                           0.51      1376\n",
      "   macro avg       0.61      0.80      0.55      1376\n",
      "weighted avg       0.76      0.51      0.36      1376\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            timestamp                           instance  \\\n",
      "0 2025-04-21 20:44:13  aks-agentpool-42554999-vmss00000x   \n",
      "1 2025-04-21 20:44:14  aks-agentpool-42554999-vmss00000x   \n",
      "2 2025-04-21 20:44:15  aks-agentpool-42554999-vmss00000x   \n",
      "3 2025-04-21 20:44:16  aks-agentpool-42554999-vmss00000x   \n",
      "4 2025-04-21 20:44:17  aks-agentpool-42554999-vmss00000x   \n",
      "\n",
      "                              pod  container_cpu_usage_seconds_total  \\\n",
      "0  loadgenerator-64fb74c986-zxn5f                           0.040401   \n",
      "1  loadgenerator-64fb74c986-zxn5f                           0.042157   \n",
      "2  loadgenerator-64fb74c986-zxn5f                           0.043912   \n",
      "3  loadgenerator-64fb74c986-zxn5f                           0.045668   \n",
      "4  loadgenerator-64fb74c986-zxn5f                           0.040660   \n",
      "\n",
      "   container_cpu_system_seconds_total  container_memory_working_set_bytes  \\\n",
      "0                            0.003729                        2.435206e+06   \n",
      "1                            0.003872                        2.525402e+06   \n",
      "2                            0.004014                        2.615598e+06   \n",
      "3                            0.004156                        2.705795e+06   \n",
      "4                            0.003752                        2.176102e+06   \n",
      "\n",
      "   container_memory_rss  container_network_receive_bytes_total  \\\n",
      "0          2.437397e+06                           32725.837968   \n",
      "1          2.537665e+06                           34031.688346   \n",
      "2          2.637933e+06                           35337.538725   \n",
      "3          2.738202e+06                           36643.389103   \n",
      "4          2.119190e+06                           37949.239482   \n",
      "\n",
      "   container_network_transmit_packets_total Abnormality class   Microservice  \\\n",
      "0                                  9.400021            Normal  adservice_cpu   \n",
      "1                                  9.773806            Normal  adservice_cpu   \n",
      "2                                 10.147592            Normal  adservice_cpu   \n",
      "3                                 10.521377            Normal  adservice_cpu   \n",
      "4                                 10.895162            Normal  adservice_cpu   \n",
      "\n",
      "  Experiment  \n",
      "0          1  \n",
      "1          1  \n",
      "2          1  \n",
      "3          1  \n",
      "4          1  \n",
      "✅ Fichier 'final2_dataset.csv' enregistré avec succès !\n",
      "Les colonnes 'instance' ont été modifiées et le fichier a été trié par 'timestamp'. Le fichier a été enregistré sous 'final2_modified_sorted.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 📁 Dossier principal contenant les expériences\n",
    "root_dir = \"C:/Users/wassi/OneDrive/Bureau/experience\"\n",
    "\n",
    "# 📌 Liste des classes valides\n",
    "valid_classes = [\"CPU HOG\", \"MEM LEAK\", \"Packet Loss\", \"Packet Delay\"]\n",
    "\n",
    "# 📌 Liste pour stocker les DataFrames de chaque fichier\n",
    "df_resource_baro_list = []\n",
    "df_communication_list = []\n",
    "\n",
    "# 🔍 Parcours des sous-dossiers (microservices)\n",
    "for microservice in os.listdir(root_dir):\n",
    "    microservice_path = os.path.join(root_dir, microservice)\n",
    "    \n",
    "    if os.path.isdir(microservice_path):  # Vérifier si c'est un dossier\n",
    "        # Déterminer la classe en fonction du suffixe (_cpu, _mem, _loss, _delay)\n",
    "        if microservice.endswith(\"_cpu\"):\n",
    "            failure_class = \"CPU HOG\"\n",
    "        elif microservice.endswith(\"_mem\"):\n",
    "            failure_class = \"MEM LEAK\"\n",
    "        elif microservice.endswith(\"_loss\"):\n",
    "            failure_class = \"Packet Loss\"\n",
    "        elif microservice.endswith(\"_delay\"):\n",
    "            failure_class = \"Packet Delay\"\n",
    "        else:\n",
    "            failure_class = \"Normal\"  # Si aucun suffixe spécifique n'est trouvé\n",
    "        \n",
    "        # 🔍 Parcours des expériences (1,2,3,4,5)\n",
    "        for experiment in os.listdir(microservice_path):\n",
    "            experiment_path = os.path.join(microservice_path, experiment)\n",
    "            \n",
    "            if os.path.isdir(experiment_path): \n",
    "                # 📌 Charger le fichier \"pod_resource_baro.csv\" s'il existe\n",
    "                data_file = os.path.join(experiment_path, \"pod_resource_baro.csv\")\n",
    "                \n",
    "                if os.path.exists(data_file):\n",
    "                    df = pd.read_csv(data_file)\n",
    "                    # Convertir la colonne \"timestamp\" en format datetime avec le format ISO 8601\n",
    "                    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                    # Récupérer le timestamp de la première ligne et ajouter 3 minutes (180 secondes)\n",
    "                    first_timestamp = df[\"timestamp\"].iloc[0]\n",
    "                    inject_time = first_timestamp + timedelta(minutes=3)\n",
    "                    \n",
    "                    # 🏷 Ajouter la classe en fonction de l'injection\n",
    "                    df[\"Abnormality class\"] = df[\"timestamp\"].apply(\n",
    "                        lambda x: \"Normal\" if x < inject_time else failure_class)\n",
    "                    \n",
    "                    # 🏷 Ajouter des colonnes pour identifier l'expérience\n",
    "                    df[\"Microservice\"] = microservice\n",
    "                    df[\"Experiment\"] = experiment\n",
    "                    \n",
    "                    # 📌 Ajouter le DataFrame à la liste des fichiers pod_resource_baro\n",
    "                    df_resource_baro_list.append(df)\n",
    "\n",
    "                # 📌 Charger et concaténer tous les fichiers \"pod_communication.csv\" sans modification\n",
    "                comm_file = os.path.join(experiment_path, \"pod_communication.csv\")\n",
    "                if os.path.exists(comm_file):\n",
    "                    comm_df = pd.read_csv(comm_file)\n",
    "                    comm_df[\"Microservice\"] = microservice\n",
    "                    comm_df[\"Experiment\"] = experiment\n",
    "                    # 📌 Ajouter le DataFrame à la liste des fichiers pod_communication\n",
    "                    df_communication_list.append(comm_df)\n",
    "\n",
    "# 🏗 Fusionner tous les DataFrames en un seul pour pod_resource_baro\n",
    "df_resource_baro_all = pd.concat(df_resource_baro_list, ignore_index=True)\n",
    "\n",
    "# 🏗 Fusionner tous les DataFrames en un seul pour pod_communication\n",
    "df_communication_all = pd.concat(df_communication_list, ignore_index=True)\n",
    "\n",
    "# 🔎 Affichage de quelques lignes pour vérification\n",
    "print(df_resource_baro_all.head())\n",
    "print(df_communication_all.head())\n",
    "\n",
    "# 📂 Sauvegarde des DataFrames finaux\n",
    "df_resource_baro_all.to_csv(\"final_pod_resource_baro_dataset.csv\", index=False)\n",
    "df_communication_all.to_csv(\"final_pod_communication_dataset.csv\", index=False)\n",
    "\n",
    "print(\"✅ Fichiers 'final_pod_resource_baro_dataset.csv' et 'final_pod_communication_dataset.csv' enregistrés avec succès !\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 📂 Charger le dataset\n",
    "df = pd.read_csv(\"final_pod_resource_baro_dataset.csv\")\n",
    "\n",
    "# 🔎 Vérifier la distribution des classes\n",
    "print(\"📌 Distribution des classes :\\n\", df[\"Abnormality class\"].value_counts())\n",
    "\n",
    "# 🎯 Définir X (features) et y (target)\n",
    "print(\"📌 Colonnes disponibles :\", df.columns)\n",
    "X = df.drop(columns=[\"Abnormality class\", \"Experiment\"], errors=\"ignore\")\n",
    "y = df[\"Abnormality class\"]\n",
    "\n",
    "# 🎭 Encodage des labels (y)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# 🔄 Convert non-numeric columns to numeric\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "# 🚨 Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# 🎲 Séparer en train & test (80% train, 20% test) avec stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# 🔬 Normalisation des features (important pour SVM et KNN)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# ✅ Vérification\n",
    "print(f\"✔️ Taille du jeu de train : {X_train.shape}\")\n",
    "print(f\"✔️ Taille du jeu de test : {X_test.shape}\")\n",
    "\n",
    "# 📌 Initialisation des modèles\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "# 🔥 Entraînement et évaluation des modèles\n",
    "accuracies = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n🔹 Entraînement du modèle {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 🎯 Stocker l'accuracy\n",
    "    accuracies[name] = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # 📊 Affichage des résultats\n",
    "    print(f\"📌 Modèle : {name}\")\n",
    "    print(f\"🔹 Accuracy : {accuracies[name]:.4f}\")\n",
    "    print(f\"🔹 Rapport de classification : \\n{classification_report(y_test, y_pred, target_names=label_encoder.classes_)}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 📊 Tracer les résultats\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(accuracies.keys(), accuracies.values(), color=['blue', 'orange', 'green', 'red', 'purple'])\n",
    "plt.xlabel(\"Modèles\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Performance des Modèles de Machine Learning\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 📁 Dossier principal contenant les expériences\n",
    "root_dir = \"C:/Users/wassi/OneDrive/Bureau/experience\"\n",
    "\n",
    "# 📌 Liste des classes valides\n",
    "valid_classes = [\"CPU HOG\", \"MEM LEAK\", \"Packet Loss\", \"Packet Delay\"]\n",
    "\n",
    "# 📌 Liste pour stocker les DataFrames de chaque fichier\n",
    "df_list = []\n",
    "\n",
    "# 🔍 Parcours des sous-dossiers (microservices)\n",
    "for microservice in os.listdir(root_dir):\n",
    "    microservice_path = os.path.join(root_dir, microservice)\n",
    "    \n",
    "    if os.path.isdir(microservice_path):  # Vérifier si c'est un dossier\n",
    "        # Déterminer la classe en fonction du suffixe (_cpu, _mem, _loss, _delay)\n",
    "        if microservice.endswith(\"_cpu\"):\n",
    "            failure_class = \"CPU HOG\"\n",
    "        elif microservice.endswith(\"_mem\"):\n",
    "            failure_class = \"MEM LEAK\"\n",
    "        elif microservice.endswith(\"_loss\"):\n",
    "            failure_class = \"Packet Loss\"\n",
    "        elif microservice.endswith(\"_delay\"):\n",
    "            failure_class = \"Packet Delay\"\n",
    "        else:\n",
    "            failure_class = \"Normal\"  # Si aucun suffixe spécifique n'est trouvé\n",
    "        \n",
    "        # 🔍 Parcours des expériences (1,2,3,4,5)\n",
    "        for experiment in os.listdir(microservice_path):\n",
    "            experiment_path = os.path.join(microservice_path, experiment)\n",
    "            \n",
    "            if os.path.isdir(experiment_path): \n",
    "                # 📌 Charger le fichier \"data.csv\" s'il existe\n",
    "                data_file = os.path.join(experiment_path, \"pod_resource_consumption.csv\")\n",
    "                \n",
    "                if os.path.exists(data_file):\n",
    "                    df = pd.read_csv(data_file)\n",
    "                    # Convertir la colonne \"timestamp\" en format datetime avec le format ISO 8601\n",
    "                    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "                    # Récupérer le timestamp de la première ligne et ajouter 3 minutes (180 secondes)\n",
    "                    first_timestamp = df[\"timestamp\"].iloc[0]\n",
    "                    inject_time = first_timestamp + timedelta(minutes=3)\n",
    "                    \n",
    "                    # 🏷 Ajouter la classe en fonction de l'injection\n",
    "                    df[\"Abnormality class\"] = df[\"timestamp\"].apply(\n",
    "                        lambda x: \"Normal\" if x < inject_time else failure_class)\n",
    "                    \n",
    "                    # 🏷 Ajouter des colonnes pour identifier l'expérience\n",
    "                    df[\"Microservice\"] = microservice\n",
    "                    df[\"Experiment\"] = experiment\n",
    "                    \n",
    "                    # 📌 Ajouter le DataFrame à la liste\n",
    "                    df_list.append(df)\n",
    "\n",
    "# 🏗 Fusionner tous les DataFrames en un seul\n",
    "df_all = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# 🔎 Affichage de quelques lignes pour vérification\n",
    "print(df_all.head())\n",
    "\n",
    "# 📂 Sauvegarde du DataFrame final\n",
    "df_all.to_csv(\"final2_dataset.csv\", index=False)\n",
    "print(\"✅ Fichier 'final2_dataset.csv' enregistré avec succès !\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Charger le fichier CSV\n",
    "df = pd.read_csv('final2_dataset.csv')\n",
    "\n",
    "# Parcourir toutes les colonnes qui se terminent par \"_deployed_at\"\n",
    "for col in df.columns:\n",
    "    if col.endswith('instance'):\n",
    "        # Parcourir toutes les lignes de la colonne et modifier chaque valeur\n",
    "        for i in range(len(df)):\n",
    "            current_value = df.at[i, col]  # Récupérer la valeur actuelle de la cellule\n",
    "            if pd.notna(current_value):  # Vérifier si la cellule n'est pas vide (NaN)\n",
    "                current_value_str = str(current_value)  # Convertir en chaîne si nécessaire\n",
    "                # Modifier la valeur en \"node\" + les deux derniers caractères de la valeur\n",
    "                df.at[i, col] = 'node' + current_value_str[-2:]\n",
    "\n",
    "# Trier le DataFrame par la colonne 'timestamp'\n",
    "df = df.sort_values(by='timestamp')\n",
    "\n",
    "# Sauvegarder le fichier avec les nouvelles modifications et trié par 'timestamp'\n",
    "df.to_csv('final2_modified_sorted.csv', index=False)\n",
    "\n",
    "print(\"Les colonnes 'instance' ont été modifiées et le fichier a été trié par 'timestamp'. Le fichier a été enregistré sous 'final2_modified_sorted.csv'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c2278a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['grpc_response_status'].fillna(0, inplace=True)\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:48: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_success['latency'].fillna(0, inplace=True)\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:75: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_error['latency'].fillna(0, inplace=True)\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:75: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_error['latency'].fillna(0, inplace=True)\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:75: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_error['latency'].fillna(0, inplace=True)\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:75: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_error['latency'].fillna(0, inplace=True)\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:75: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_error['latency'].fillna(0, inplace=True)\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:98: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_error['latency'].fillna(0, inplace=True)\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:98: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_error['latency'].fillna(0, inplace=True)\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:98: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_error['latency'].fillna(0, inplace=True)\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:98: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_error['latency'].fillna(0, inplace=True)\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:98: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_error['latency'].fillna(0, inplace=True)\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:98: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_error['latency'].fillna(0, inplace=True)\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:98: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_error['latency'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement terminé. Fichier sauvegardé sous 'new_request_istio_data.csv'.\n",
      "Traitement terminé. Fichier sauvegardé sous aggregated_istio_rates.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:195: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  .resample(window, on='timestamp', label='right', closed='right')\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:235: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_real_durations)\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:195: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  .resample(window, on='timestamp', label='right', closed='right')\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:235: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_real_durations)\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:235: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_real_durations)\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:235: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_real_durations)\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2960753219.py:235: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_real_durations)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger le fichier CSV\n",
    "file_path = \"final_pod_communication_dataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df['grpc_response_status'].fillna(0, inplace=True)\n",
    "\n",
    "df['response_flags'] = df['response_flags'].astype(str).str.strip()  # Convertir en string et enlever espaces\n",
    "\n",
    "# Ajouter une colonne 'result' avec 'success' ou 'error'\n",
    "df['result'] = df.apply(\n",
    "    lambda row: 'success' if row['response_code'] == 200 and row['grpc_response_status'] == 0 and row['response_flags'] == '-' else 'error',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Réorganiser les données par 'source_workload', 'destination_workload' et 'timestamp'\n",
    "df_sorted = df.sort_values(by=['source_workload', 'destination_workload', 'timestamp'])\n",
    "\n",
    "# Sauvegarder le fichier résultant\n",
    "df_sorted.to_csv(\"aggregated_istio_data.csv\", index=False)\n",
    "\n",
    "# Charger le fichier\n",
    "file_path = \"aggregated_istio_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convertir timestamp en datetime pour le tri\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Trier avant la séparation\n",
    "df.sort_values(by=['source_workload', 'destination_workload', 'timestamp'], inplace=True)\n",
    "\n",
    "# Séparer les succès\n",
    "df_success = df[df['result'] == 'success'].copy()\n",
    "\n",
    "# Calculer new_request, new_istio_request_bytes et new_istio_request_duration_milliseconds avec 0 pour la première ligne\n",
    "df_success['new_request'] = df_success.groupby(['source_workload', 'destination_workload'])['total_request'].diff().fillna(0)\n",
    "df_success['new_istio_request_bytes'] = df_success.groupby(['source_workload', 'destination_workload'])['istio_request_bytes_sum'].diff().fillna(0)\n",
    "df_success['new_istio_request_duration_milliseconds'] = df_success.groupby(['source_workload', 'destination_workload'])['istio_request_duration_milliseconds_sum'].diff().fillna(0)\n",
    "\n",
    "# Appliquer la condition si new_request == 0\n",
    "df_success.loc[df_success['new_request'] == 0, ['new_istio_request_bytes', 'new_istio_request_duration_milliseconds']] = 0\n",
    "\n",
    "# Calculer latency\n",
    "df_success['latency'] = df_success['new_istio_request_duration_milliseconds'] / df_success['new_request']\n",
    "df_success['latency'].fillna(0, inplace=True)\n",
    "\n",
    "# Sauvegarder les succès dans un fichier\n",
    "df_success.to_csv(\"success_istio_data.csv\", index=False)\n",
    "\n",
    "# Séparer les erreurs HTTP et gRPC\n",
    "df_http_errors = df[(df['result'] == 'error') & (df['request_protocol'] == 'http')].copy()\n",
    "df_grpc_errors = df[(df['result'] == 'error') & (df['request_protocol'] == 'grpc')].copy()\n",
    "\n",
    "error_files = []  # Liste des fichiers d'erreur générés\n",
    "\n",
    "# Traitement des erreurs HTTP\n",
    "http_groups = df_http_errors.groupby(['request_protocol', 'response_code', 'grpc_response_status', 'response_flags'])\n",
    "\n",
    "for (request_protocol, response_code, grpc_status, response_flags), df_error in http_groups:\n",
    "    df_error = df_error.copy()\n",
    "    \n",
    "    # Calculer new_request, new_istio_request_bytes et new_istio_request_duration_milliseconds avec 0 pour la première ligne\n",
    "    df_error['new_request'] = df_error.groupby(['source_workload', 'destination_workload'])['total_request'].diff().fillna(0)\n",
    "    df_error['new_istio_request_bytes'] = df_error.groupby(['source_workload', 'destination_workload'])['istio_request_bytes_sum'].diff().fillna(0)\n",
    "    df_error['new_istio_request_duration_milliseconds'] = df_error.groupby(['source_workload', 'destination_workload'])['istio_request_duration_milliseconds_sum'].diff().fillna(0)\n",
    "    \n",
    "    # Appliquer la condition si new_request == 0\n",
    "    df_error.loc[df_error['new_request'] == 0, ['new_istio_request_bytes', 'new_istio_request_duration_milliseconds']] = 0\n",
    "    \n",
    "    # Calculer latency\n",
    "    df_error['latency'] = df_error['new_istio_request_duration_milliseconds'] / df_error['new_request']\n",
    "    df_error['latency'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Nommer le fichier selon l'erreur\n",
    "    file_name = f\"error_{request_protocol}_{response_code}_{response_flags}.csv\"\n",
    "    df_error.to_csv(file_name, index=False)\n",
    "    error_files.append(df_error)\n",
    "\n",
    "# Traitement des erreurs gRPC\n",
    "grpc_groups = df_grpc_errors.groupby(['request_protocol', 'response_code', 'grpc_response_status', 'response_flags'])\n",
    "\n",
    "for (request_protocol, response_code, grpc_status, response_flags), df_error in grpc_groups:\n",
    "    df_error = df_error.copy()\n",
    "    \n",
    "    # Calculer new_request, new_istio_request_bytes et new_istio_request_duration_milliseconds avec 0 pour la première ligne\n",
    "    df_error['new_request'] = df_error.groupby(['source_workload', 'destination_workload'])['total_request'].diff().fillna(0)\n",
    "    df_error['new_istio_request_bytes'] = df_error.groupby(['source_workload', 'destination_workload'])['istio_request_bytes_sum'].diff().fillna(0)\n",
    "    df_error['new_istio_request_duration_milliseconds'] = df_error.groupby(['source_workload', 'destination_workload'])['istio_request_duration_milliseconds_sum'].diff().fillna(0)\n",
    "    \n",
    "    # Appliquer la condition si new_request == 0\n",
    "    df_error.loc[df_error['new_request'] == 0, ['new_istio_request_bytes', 'new_istio_request_duration_milliseconds']] = 0\n",
    "    \n",
    "    # Calculer latency\n",
    "    df_error['latency'] = df_error['new_istio_request_duration_milliseconds'] / df_error['new_request']\n",
    "    df_error['latency'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Nommer le fichier selon l'erreur\n",
    "    file_name = f\"error_{request_protocol}_{response_code}_{grpc_status}_{response_flags}.csv\"\n",
    "    df_error.to_csv(file_name, index=False)\n",
    "    error_files.append(df_error)\n",
    "\n",
    "# Fusionner tous les fichiers (success + errors)\n",
    "df_final = pd.concat([df_success] + error_files).sort_values(by=['source_workload', 'destination_workload', 'timestamp'])\n",
    "\n",
    "# Sauvegarder le fichier final\n",
    "df_final.to_csv(\"new_request_istio_data.csv\", index=False)\n",
    "\n",
    "print(\"Traitement terminé. Fichier sauvegardé sous 'new_request_istio_data.csv'.\")\n",
    "\n",
    "\n",
    "# Charger le fichier\n",
    "file_path = \"new_request_istio_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convertir timestamp en datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Trier les données\n",
    "df.sort_values(by=['source_workload', 'destination_workload', 'timestamp'], inplace=True)\n",
    "\n",
    "# Calculer success rate, error rate, etc.\n",
    "grouped = df.groupby(['source_workload', 'destination_workload', 'timestamp'])\n",
    "aggregated_rows = []\n",
    "\n",
    "for (src, dst, ts), group in grouped:\n",
    "    total_new_request = group['new_request'].sum()\n",
    "    success_count = group[group['result'] == 'success']['new_request'].sum()\n",
    "    error_count = total_new_request - success_count\n",
    "    \n",
    "    if total_new_request > 0:\n",
    "        success_rate = success_count / total_new_request\n",
    "        error_rate = 1 - success_rate\n",
    "    else:\n",
    "        success_rate = float('nan')\n",
    "        error_rate = float('nan')\n",
    "    \n",
    "    duration_success_request = group[group['result'] == 'success']['latency'].sum()\n",
    "    duration_error_request = group[group['result'] == 'error']['latency'].sum()\n",
    "    average_latency = duration_success_request + duration_error_request\n",
    "\n",
    "    new_istio_request_bytes_success = group[group['result'] == 'success']['new_istio_request_bytes'].sum()\n",
    "    new_istio_request_bytes_error = group[group['result'] == 'error']['new_istio_request_bytes'].sum()\n",
    "    istio_request_bytes = new_istio_request_bytes_success + new_istio_request_bytes_error\n",
    "    istio_request_duration_milliseconds = group['new_istio_request_duration_milliseconds'].sum()\n",
    "\n",
    "    # Récupérer la valeur du node (par exemple le premier)\n",
    "    node = group['node'].iloc[0] if 'node' in group.columns else None\n",
    "\n",
    "    aggregated_rows.append([\n",
    "        ts, src, dst, node, group['total_request'].max(), total_new_request,\n",
    "        success_count, error_count, success_rate, error_rate,\n",
    "        duration_success_request, duration_error_request, average_latency,\n",
    "        new_istio_request_bytes_success, new_istio_request_bytes_error,\n",
    "        istio_request_bytes, istio_request_duration_milliseconds\n",
    "    ])\n",
    "\n",
    "# Créer un DataFrame final\n",
    "df_final = pd.DataFrame(aggregated_rows, columns=[\n",
    "    'timestamp', 'source_workload', 'destination_workload', 'node', 'total_request', 'new_request',\n",
    "    'success_count', 'error_count', 'success_rate', 'error_rate',\n",
    "    'duration_success_request', 'duration_error_request', 'average_latency',\n",
    "    'new_istio_request_bytes_success', 'new_istio_request_bytes_error',\n",
    "    'istio_request_bytes', 'duration_milliseconds'\n",
    "])\n",
    "\n",
    "# Sauvegarder le fichier\n",
    "output_file = \"aggregated_istio_rates.csv\"\n",
    "df_final.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Traitement terminé. Fichier sauvegardé sous {output_file}.\")\n",
    "\n",
    "\n",
    "\n",
    "# Charger le fichier\n",
    "file_path = \"aggregated_istio_rates.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convertir timestamp en datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Définir les fenêtres de temps\n",
    "time_windows = ['15S', '30S', '1min', '5min', '10min']\n",
    "\n",
    "# Initialiser un tableau pour stocker les résultats\n",
    "kpi_results = []\n",
    "\n",
    "for window in time_windows:\n",
    "    # Resample par fenêtre de temps\n",
    "    df_resampled = (\n",
    "        df\n",
    "        .groupby(['source_workload', 'destination_workload'])\n",
    "        .resample(window, on='timestamp', label='right', closed='right')\n",
    "        .agg({\n",
    "            'total_request': 'max',\n",
    "            'new_request': 'sum',\n",
    "            'success_count': 'sum',\n",
    "            'error_count': 'sum',\n",
    "            'success_rate': 'mean',\n",
    "            'error_rate': 'mean',\n",
    "            'average_latency': 'sum',\n",
    "            'istio_request_bytes': 'sum',\n",
    "            'duration_milliseconds': 'sum'\n",
    "        })\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Pour chaque pair source-destination, calculer la vraie durée entre deux points non nuls\n",
    "    def compute_real_durations(group):\n",
    "        # Garder le timestamp courant pour référence\n",
    "        last_time = None\n",
    "        last_index = None\n",
    "        durations = []\n",
    "        \n",
    "        for idx, row in group.iterrows():\n",
    "            if row['new_request'] > 0:\n",
    "                if last_time is not None:\n",
    "                    duration = (row['timestamp'] - last_time).total_seconds()\n",
    "                    durations.append(duration)\n",
    "                else:\n",
    "                    durations.append(np.nan)\n",
    "                last_time = row['timestamp']\n",
    "                last_index = idx\n",
    "            else:\n",
    "                durations.append(np.nan)\n",
    "        # Remplir les valeurs manquantes en regardant en avant\n",
    "        return pd.Series(durations, index=group.index)\n",
    "\n",
    "    # Appliquer la fonction à chaque groupe\n",
    "    df_resampled['real_duration'] = (\n",
    "        df_resampled\n",
    "        .groupby(['source_workload', 'destination_workload'])\n",
    "        .apply(compute_real_durations)\n",
    "        .reset_index(level=[0,1], drop=True)\n",
    "    )\n",
    "\n",
    "    # Calcul des KPI\n",
    "    df_resampled['throughput'] = df_resampled['istio_request_bytes'] / df_resampled['real_duration']\n",
    "    df_resampled['request_rate'] = df_resampled['new_request'] / df_resampled['real_duration']\n",
    "\n",
    "    # Ajouter la fenêtre utilisée\n",
    "    df_resampled['time_window'] = window\n",
    "\n",
    "    kpi_results.append(df_resampled)\n",
    "\n",
    "# Concaténer tous les résultats\n",
    "df_final = pd.concat(kpi_results)\n",
    "\n",
    "# Supprimer la ligne où timestamp == \"2025-04-02 15:21:00\"\n",
    "starting_point = pd.Timestamp(\"2025-04-08 00:15:00\")\n",
    "df_final = df_final[df_final['timestamp'] != starting_point]\n",
    "\n",
    "# Sauvegarder dans un fichier CSV\n",
    "df_final.to_csv(\"kiali_kpi_metrics.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a9158c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Entraînement du modèle GAE...\n",
      "Epoch 20, Loss: 1.2514\n",
      "Epoch 40, Loss: 1.2229\n",
      "Epoch 60, Loss: 1.1305\n",
      "Epoch 80, Loss: 1.0917\n",
      "Epoch 100, Loss: 1.0395\n",
      "Epoch 120, Loss: 1.1759\n",
      "Epoch 140, Loss: 0.9896\n",
      "Epoch 160, Loss: 1.1517\n",
      "Epoch 180, Loss: 1.0310\n",
      "Epoch 200, Loss: 0.9258\n",
      "✅ Fichier anomaly_scores_filtered.csv généré.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import remove_self_loops, to_undirected\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.nn import GCNConv, GAE\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 🔄 Chargement des données\n",
    "df = pd.read_csv(\"kiali_kpi_metrics.csv\")\n",
    "\n",
    "# 🧽 Nettoyage de time_window\n",
    "df['time_window'] = df['time_window'].astype(str).str.strip()\n",
    "df = df[df['time_window'] == \"15S\"].copy()\n",
    "\n",
    "if df.empty:\n",
    "    raise ValueError(\"🚨 Aucun enregistrement avec time_window == '15s'. Vérifiez le fichier CSV.\")\n",
    "\n",
    "# ✅ Nettoyage des colonnes numériques\n",
    "df['error_rate'] = pd.to_numeric(df['error_rate'], errors='coerce').fillna(0.0)\n",
    "\n",
    "# 🏷️ Attribution du statut\n",
    "def assign_status(er):\n",
    "    if er == 0.0:\n",
    "        return \"Healthy\"\n",
    "    elif er < 0.15:\n",
    "        return \"Degraded\"\n",
    "    else:\n",
    "        return \"Unavailable\"\n",
    "\n",
    "df['status'] = df['error_rate'].apply(assign_status)\n",
    "\n",
    "# 🔍 Préparation des données pour le modèle (uniquement anomalies)\n",
    "df_anomalies = df[df['status'] != \"Healthy\"].copy()\n",
    "\n",
    "# Création des nœuds\n",
    "nodes = pd.unique(df[['source_workload', 'destination_workload']].values.ravel())\n",
    "node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "idx_to_node = {v: k for k, v in node_to_idx.items()}\n",
    "\n",
    "# Création de edge_index (pour les anomalies uniquement)\n",
    "edge_index = torch.tensor([\n",
    "    [node_to_idx[src] for src in df_anomalies['source_workload']],\n",
    "    [node_to_idx[dst] for dst in df_anomalies['destination_workload']],\n",
    "], dtype=torch.long)\n",
    "\n",
    "edge_index, _ = remove_self_loops(edge_index)\n",
    "edge_index = to_undirected(edge_index)\n",
    "\n",
    "# Features des nœuds (moyenne de error_rate par source)\n",
    "features = df.groupby('source_workload')['error_rate'].mean().reindex(nodes).fillna(0)\n",
    "scaler = StandardScaler()\n",
    "x = torch.tensor(scaler.fit_transform(features.values.reshape(-1, 1)), dtype=torch.float)\n",
    "x = torch.nan_to_num(x)\n",
    "\n",
    "# 🧱 Données pour le GCN\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# === GCN Autoencoder ===\n",
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
    "        self.conv2 = GCNConv(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "# Modèle GAE\n",
    "model = GAE(GCNEncoder(data.num_node_features, 16))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Entraînement\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    loss = model.recon_loss(z, data.edge_index)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "print(\"🔁 Entraînement du modèle GAE...\")\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Encodage final\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    probs = model.decoder.forward_all(z).sigmoid()\n",
    "\n",
    "# Calcul des scores d'anomalie\n",
    "scores = []\n",
    "for idx, row in df.iterrows():\n",
    "    src = node_to_idx.get(row['source_workload'])\n",
    "    dst = node_to_idx.get(row['destination_workload'])\n",
    "\n",
    "    if row['status'] == \"Healthy\":\n",
    "        score = 0.0  # Pas d'anomalie pour les communications saines\n",
    "    elif src is not None and dst is not None:\n",
    "        score = 1.0 - probs[src, dst].item()  # Calcul du score d'anomalie basé sur la reconstruction\n",
    "    else:\n",
    "        score = 1.0  # par défaut en cas de noeud inconnu\n",
    "    scores.append(score)\n",
    "\n",
    "df['anomaly_score'] = scores\n",
    "df.to_csv(\"anomaly_scores_filtered.csv\", index=False)\n",
    "print(\"✅ Fichier anomaly_scores_filtered.csv généré.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c0a48fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\2878623855.py:27: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  df['minute'] = df['timestamp'].dt.floor('T')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GIF enregistré sous 'temporal_anomaly_graph.gif' !\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.animation as animation\n",
    "import os\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv(\"anomaly_scores_filtered.csv\")\n",
    "\n",
    "# Vérification des colonnes nécessaires\n",
    "required_columns = ['timestamp', 'source_workload', 'destination_workload', 'status', 'anomaly_score', 'time_window']\n",
    "if not all(col in df.columns for col in required_columns):\n",
    "    raise ValueError(f\"Le fichier CSV doit contenir les colonnes suivantes : {', '.join(required_columns)}\")\n",
    "\n",
    "# Nettoyage\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "df['time_window'] = df['time_window'].astype(str).str.strip()\n",
    "\n",
    "# Filtrage time_window contenant \"15\"\n",
    "df = df[df['time_window'].str.contains(\"15\", case=False, na=False)].copy()\n",
    "if df.empty:\n",
    "    raise ValueError(\"🚨 Aucun enregistrement avec time_window contenant '15'.\")\n",
    "\n",
    "# Ajouter la minute\n",
    "df['minute'] = df['timestamp'].dt.floor('T')\n",
    "\n",
    "# Fonction pour colorier les arêtes\n",
    "def get_color(status):\n",
    "    return {\n",
    "        'Healthy': 'green',\n",
    "        'Degraded': 'blue',\n",
    "        'Unavailable': 'red'\n",
    "    }.get(status, 'gray')\n",
    "\n",
    "# Initialisation\n",
    "grouped = df.groupby('minute')\n",
    "frames = []  # pour stocker les images de chaque frame\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "# Disposition fixe pour tout (optionnel) pour éviter saut de nœuds\n",
    "fixed_pos = {}\n",
    "\n",
    "# Construire chaque minute\n",
    "minutes = sorted(grouped.groups.keys())\n",
    "\n",
    "def build_graph(minute):\n",
    "    G = nx.DiGraph()\n",
    "    group = grouped.get_group(minute)\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        src = row['source_workload']\n",
    "        dst = row['destination_workload']\n",
    "        color = get_color(row['status'])\n",
    "        score = row['anomaly_score']\n",
    "        label = f\"{score:.2f}\" if score > 0.0 else \"\"\n",
    "\n",
    "        G.add_node(src)\n",
    "        G.add_node(dst)\n",
    "        G.add_edge(src, dst, color=color, score=score, label=label)\n",
    "\n",
    "    return G\n",
    "\n",
    "# Fonction d'animation\n",
    "def update(i):\n",
    "    ax.clear()\n",
    "    minute = minutes[i]\n",
    "    G = build_graph(minute)\n",
    "\n",
    "    global fixed_pos\n",
    "    if not fixed_pos:\n",
    "        fixed_pos = nx.spring_layout(G, seed=84, k=0.5, iterations=50)\n",
    "\n",
    "    nx.draw_networkx_nodes(G, fixed_pos, node_size=600, node_color='skyblue', ax=ax)\n",
    "\n",
    "    edge_colors = [G[u][v]['color'] for u, v in G.edges()]\n",
    "    nx.draw_networkx_edges(G, fixed_pos, edge_color=edge_colors, width=2, ax=ax)\n",
    "\n",
    "    nx.draw_networkx_labels(G, fixed_pos, font_size=10, font_weight='bold', ax=ax)\n",
    "\n",
    "    edge_labels = {\n",
    "        (u, v): f\"{G[u][v]['label']}\" for u, v in G.edges()\n",
    "        if G[u][v]['label']\n",
    "    }\n",
    "    label_pos = {k: (v[0], v[1] + 0.03) for k, v in fixed_pos.items()}\n",
    "    nx.draw_networkx_edge_labels(G, pos=label_pos, edge_labels=edge_labels, font_size=9, font_color='black', ax=ax)\n",
    "\n",
    "    legend = [\n",
    "        Line2D([0], [0], color='green', lw=2, label='Healthy'),\n",
    "        Line2D([0], [0], color='blue', lw=2, label='Degraded'),\n",
    "        Line2D([0], [0], color='red', lw=2, label='Unavailable'),\n",
    "    ]\n",
    "    ax.legend(handles=legend, loc='upper right')\n",
    "\n",
    "    ax.set_title(f\"Graphe minute {minute.strftime('%Y-%m-%d %H:%M')}\", fontsize=14)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Créer l'animation\n",
    "ani = animation.FuncAnimation(fig, update, frames=len(minutes), interval=1000, repeat=False)\n",
    "\n",
    "# Sauvegarder en GIF\n",
    "ani.save('temporal_anomaly_graph.gif', writer='pillow', fps=1)\n",
    "\n",
    "print(\"✅ GIF enregistré sous 'temporal_anomaly_graph.gif' !\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bd17eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "C:\\Users\\wassi\\AppData\\Local\\Temp\\ipykernel_15516\\3932223751.py:78: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GIF enregistré sous : knowledge_graph_evolution.gif\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# ✅ Charger les données\n",
    "df = pd.read_csv('final2_modified_sorted.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['Microservice'] = df['Microservice'].astype(str)\n",
    "\n",
    "# ✅ Créer dossier pour images\n",
    "os.makedirs('frames', exist_ok=True)\n",
    "frame_files = []  # stocke les noms d'images\n",
    "\n",
    "# ✅ Fonction pour construire un graphe pour une période donnée\n",
    "def create_knowledge_graph(df, start_time, end_time):\n",
    "    filtered_df = df[(df['timestamp'] >= start_time) & (df['timestamp'] < end_time)]\n",
    "    G = nx.DiGraph()\n",
    "    microservice_name = None\n",
    "    attack_type = None\n",
    "\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        if pd.isna(row['Microservice']):\n",
    "            continue\n",
    "\n",
    "        microservice = row['Microservice']\n",
    "        try:\n",
    "            microservice_name, attack_type = microservice.split('_')\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        instance = row['instance']\n",
    "        pod = row['pod']\n",
    "        G.add_node(instance, type='instance')\n",
    "        G.add_node(pod, type='pod')\n",
    "\n",
    "        if row['Abnormality class'] == 'Normal':\n",
    "            edge_color = 'green'\n",
    "        else:\n",
    "            edge_color = 'red'\n",
    "\n",
    "        # Ajouter arête générale\n",
    "        G.add_edge(instance, pod, color='green', anomaly='Normal')\n",
    "\n",
    "        # Ajouter arête rouge si anomalie détectée sur le bon pod\n",
    "        if microservice_name in pod:\n",
    "            G.add_edge(instance, pod, color=edge_color, anomaly=row['Abnormality class'])\n",
    "\n",
    "    return G, microservice_name, attack_type, row['Abnormality class']\n",
    "\n",
    "# ✅ Fonction pour tracer et enregistrer une image\n",
    "def save_graph(G, start_time, end_time, microservice_name, attack_type, anomaly_class, frame_id):\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    edge_colors = [G[u][v]['color'] for u, v in G.edges()]\n",
    "    pos = nx.spring_layout(G, seed=84)\n",
    "\n",
    "    nx.draw(G, pos, with_labels=True, node_size=1000, node_color='lightblue',\n",
    "            font_size=7, edge_color=edge_colors, width=2, font_weight='bold')\n",
    "\n",
    "    plt.title(f\"Graph from {start_time} to {end_time}\", fontsize=14)\n",
    "\n",
    "    # Légende\n",
    "    red_patch = plt.Line2D([0], [0], marker='o', color='w', label='Anomalie', markerfacecolor='red', markersize=10)\n",
    "    green_patch = plt.Line2D([0], [0], marker='o', color='w', label='Normal', markerfacecolor='green', markersize=10)\n",
    "    plt.legend(handles=[red_patch, green_patch])\n",
    "\n",
    "    # Texte sous le graphe\n",
    "    if anomaly_class != 'Normal':\n",
    "        plt.figtext(0.5, 0.01, f\"Microservice attaqué : {microservice_name}, Type : {attack_type.upper()}\",\n",
    "                    wrap=True, horizontalalignment='center', fontsize=12)\n",
    "    else:\n",
    "        plt.figtext(0.5, 0.01, \"Aucune attaque détectée\",\n",
    "                    wrap=True, horizontalalignment='center', fontsize=12)\n",
    "\n",
    "    # Bien cadrer le texte\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Sauvegarde image\n",
    "    filename = f'frames/frame_{frame_id:03}.png'\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    frame_files.append(filename)\n",
    "\n",
    "# ✅ Identifier les expériences\n",
    "df['microservice_change'] = df['Microservice'].ne(df['Microservice'].shift()).cumsum()\n",
    "\n",
    "# ✅ Générer tous les graphes\n",
    "frame_id = 0\n",
    "for exp_id, exp_df in df.groupby('microservice_change'):\n",
    "    start_time = exp_df['timestamp'].min()\n",
    "    end_time = start_time + timedelta(minutes=6)\n",
    "\n",
    "    # Avant injection\n",
    "    G1, microservice_name, attack_type, anomaly_class = create_knowledge_graph(\n",
    "        exp_df, start_time, start_time + timedelta(minutes=3))\n",
    "    save_graph(G1, start_time, start_time + timedelta(minutes=3),\n",
    "               microservice_name, attack_type, anomaly_class, frame_id)\n",
    "    frame_id += 1\n",
    "\n",
    "    # Après injection\n",
    "    G2, _, _, _ = create_knowledge_graph(\n",
    "        exp_df, start_time + timedelta(minutes=3), end_time)\n",
    "    save_graph(G2, start_time + timedelta(minutes=3), end_time,\n",
    "               microservice_name, attack_type, anomaly_class, frame_id)\n",
    "    frame_id += 1\n",
    "\n",
    "# ✅ Générer le GIF\n",
    "images = [Image.open(f) for f in frame_files]\n",
    "images[0].save('knowledge_graph_evolution.gif', save_all=True,\n",
    "               append_images=images[1:], duration=1500, loop=0)\n",
    "\n",
    "print(\"✅ GIF enregistré sous : knowledge_graph_evolution.gif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e712056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GIF saved as 'combined_graph6.gif'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.animation as animation\n",
    "import os\n",
    "\n",
    "# Load and preprocess data\n",
    "df1 = pd.read_csv(\"anomaly_scores_filtered.csv\")\n",
    "df2 = pd.read_csv(\"final2_modified_sorted.csv\")\n",
    "\n",
    "# Required columns\n",
    "required_cols1 = ['source_workload', 'destination_workload', 'timestamp', 'status', 'anomaly_score']\n",
    "required_cols2 = ['timestamp', 'instance', 'pod', 'Abnormality class', 'Microservice']\n",
    "if not all(col in df1.columns for col in required_cols1):\n",
    "    raise ValueError(f\"anomaly_scores_filtered.csv must contain: {', '.join(required_cols1)}\")\n",
    "if not all(col in df2.columns for col in required_cols2):\n",
    "    raise ValueError(f\"final2_modified_sorted.csv must contain: {', '.join(required_cols2)}\")\n",
    "\n",
    "# Convert timestamps to datetime and remove timezone\n",
    "df1['timestamp'] = pd.to_datetime(df1['timestamp'], errors='coerce').dt.tz_localize(None)\n",
    "df2['timestamp'] = pd.to_datetime(df2['timestamp'], errors='coerce').dt.tz_localize(None)\n",
    "\n",
    "# Normalize pod names in df2 by taking the part before the first '-'\n",
    "df2['pod_normalized'] = df2['pod'].apply(lambda x: x.split('-')[0] if isinstance(x, str) and '-' in x else x)\n",
    "\n",
    "# Find common timestamps (exact match down to seconds)\n",
    "common_timestamps = sorted(set(df1['timestamp']).intersection(set(df2['timestamp'])))\n",
    "if not common_timestamps:\n",
    "    raise ValueError(\"No common timestamps found between the two datasets.\")\n",
    "\n",
    "# Collect all unique nodes (pods and instances)\n",
    "all_pods = set(df1['source_workload']).union(set(df1['destination_workload'])).union(set(df2['pod_normalized']))\n",
    "all_instances = set(df2['instance'])\n",
    "all_nodes = all_pods.union(all_instances)\n",
    "\n",
    "# Create a graph with all nodes to compute fixed positions\n",
    "G_all = nx.DiGraph()\n",
    "for node in all_nodes:\n",
    "    node_type = 'instance' if node in all_instances else 'pod'\n",
    "    G_all.add_node(node, type=node_type, label=node)\n",
    "\n",
    "# Compute fixed positions for all nodes\n",
    "fixed_pos = nx.spring_layout(G_all, seed=84, k=0.5, iterations=50)\n",
    "\n",
    "# Function to get edge color for pod-to-pod communication\n",
    "def get_communication_color(status):\n",
    "    return {\n",
    "        'Healthy': 'green',\n",
    "        'Degraded': 'blue',\n",
    "        'Unavailable': 'red'\n",
    "    }.get(status, 'gray')\n",
    "\n",
    "# Function to get edge color for instance-to-pod relationship\n",
    "def get_anomaly_color(abnormality_class):\n",
    "    return 'red' if abnormality_class != 'Normal' else 'green'\n",
    "\n",
    "# Function to build the combined graph for a given timestamp\n",
    "def build_combined_graph(timestamp):\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Process pod-to-pod communications (from df1)\n",
    "    df1_window = df1[df1['timestamp'] == timestamp]\n",
    "    for _, row in df1_window.iterrows():\n",
    "        src = row['source_workload']\n",
    "        dst = row['destination_workload']\n",
    "        status = row['status']\n",
    "        score = row['anomaly_score']\n",
    "        color = get_communication_color(status)\n",
    "        label = f\"{score:.2f}\" if score > 0.0 else \"\"\n",
    "        \n",
    "        G.add_node(src, type='pod', label=src)\n",
    "        G.add_node(dst, type='pod', label=dst)\n",
    "        G.add_edge(src, dst, type='communication', color=color, label=label, score=score)\n",
    "    \n",
    "    # Process instance-to-pod relationships (from df2)\n",
    "    df2_window = df2[df2['timestamp'] == timestamp]\n",
    "    microservice_name = None\n",
    "    attack_type = None\n",
    "    anomaly_class = 'Normal'\n",
    "    attacked_pod = None  # Track the pod under attack\n",
    "    \n",
    "    for _, row in df2_window.iterrows():\n",
    "        instance = row['instance']\n",
    "        pod = row['pod_normalized']  # Use normalized pod name\n",
    "        abnormality = row['Abnormality class']\n",
    "        microservice = row['Microservice']\n",
    "        color = get_anomaly_color(abnormality)\n",
    "        \n",
    "        # If abnormality is not normal, extract the pod affected by the attack\n",
    "        if abnormality != 'Normal':\n",
    "            microservice_name, attack_type = microservice.split('_', 1)\n",
    "            attacked_pod = microservice_name  # The pod affected by the attack is the microservice part\n",
    "            \n",
    "        G.add_node(instance, type='instance', label=instance)\n",
    "        G.add_node(pod, type='pod', label=pod)\n",
    "        \n",
    "        # Add deployment edge; color it based on anomaly\n",
    "        if pod == attacked_pod:\n",
    "            # Color the edge red if the pod is attacked\n",
    "            G.add_edge(instance, pod, type='deployment', color='red', anomaly=abnormality)\n",
    "        else:\n",
    "            # Default color is green (normal)\n",
    "            G.add_edge(instance, pod, type='deployment', color='green', anomaly=abnormality)\n",
    "    \n",
    "    return G, microservice_name, attack_type, anomaly_class, attacked_pod\n",
    "\n",
    "# Initialize plot\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "# Animation update function\n",
    "def update(i):\n",
    "    ax.clear()\n",
    "    timestamp = common_timestamps[i]\n",
    "    G, microservice_name, attack_type, anomaly_class, attacked_pod = build_combined_graph(timestamp)\n",
    "    \n",
    "    # Draw nodes (pods as circles, instances as squares)\n",
    "    pod_nodes = [n for n, d in G.nodes(data=True) if d['type'] == 'pod']\n",
    "    instance_nodes = [n for n, d in G.nodes(data=True) if d['type'] == 'instance']\n",
    "    nx.draw_networkx_nodes(G, fixed_pos, nodelist=pod_nodes, node_size=600, node_color='skyblue', \n",
    "                          node_shape='o', ax=ax)\n",
    "    nx.draw_networkx_nodes(G, fixed_pos, nodelist=instance_nodes, node_size=600, node_color='lightgreen', \n",
    "                          node_shape='s', ax=ax)\n",
    "    \n",
    "    # Draw edges (communication as solid, deployment as dashed)\n",
    "    comm_edges = [(u, v) for u, v, d in G.edges(data=True) if d['type'] == 'communication']\n",
    "    deploy_edges = [(u, v) for u, v, d in G.edges(data=True) if d['type'] == 'deployment']\n",
    "\n",
    "    # Apply the same edge coloring logic for deployment edges\n",
    "    deploy_colors = [\n",
    "        'red' if (u, v) == (attacked_pod, v) or (u, v) == (u, attacked_pod) else 'green'\n",
    "        for u, v in deploy_edges\n",
    "    ]\n",
    "    \n",
    "    comm_colors = [G[u][v]['color'] for u, v in comm_edges]\n",
    "    \n",
    "    # Draw the edges\n",
    "    nx.draw_networkx_edges(G, fixed_pos, edgelist=comm_edges, edge_color=comm_colors, width=2, ax=ax)\n",
    "    nx.draw_networkx_edges(G, fixed_pos, edgelist=deploy_edges, edge_color=deploy_colors, width=2, \n",
    "                          style='dashed', ax=ax)\n",
    "    \n",
    "    # Draw node labels\n",
    "    nx.draw_networkx_labels(G, fixed_pos, labels=nx.get_node_attributes(G, 'label'), \n",
    "                           font_size=10, font_weight='bold', ax=ax)\n",
    "    \n",
    "    # Draw edge labels for communications\n",
    "    edge_labels = {(u, v): d['label'] for u, v, d in G.edges(data=True) \n",
    "                   if d['type'] == 'communication' and d['label']}\n",
    "    label_pos = {k: (v[0], v[1] + 0.03) for k, v in fixed_pos.items()}\n",
    "    nx.draw_networkx_edge_labels(G, pos=label_pos, edge_labels=edge_labels, \n",
    "                                font_size=9, font_color='black', ax=ax)\n",
    "    \n",
    "    # Legends\n",
    "    comm_legend = [\n",
    "        Line2D([0], [0], color='green', lw=2, label='Healthy'),\n",
    "        Line2D([0], [0], color='blue', lw=2, label='Degraded'),\n",
    "        Line2D([0], [0], color='red', lw=2, label='Unavailable')\n",
    "    ]\n",
    "    anomaly_legend = [\n",
    "        Line2D([0], [0], color='green', lw=2, linestyle='--', label='Normal'),\n",
    "        Line2D([0], [0], color='red', lw=2, linestyle='--', label='Anomaly')\n",
    "    ]\n",
    "    ax.legend(handles=comm_legend + anomaly_legend, loc='upper right')\n",
    "    \n",
    "    # Title with exact timestamp\n",
    "    ax.set_title(f\"Combined Graph for {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\", fontsize=14)\n",
    "    #if anomaly_class != 'Normal' and microservice_name:\n",
    "     #   plt.figtext(0.5, 0.01, f\"Microservice attaqué : {microservice_name}, Type : {attack_type.upper()}\",\n",
    "      #              wrap=True, horizontalalignment='center', fontsize=12)\n",
    "    #else:\n",
    "     #   plt.figtext(0.5, 0.01, \"Aucune attaque détectée\",\n",
    "      #              wrap=True, horizontalalignment='center', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    ax.axis('off')\n",
    "\n",
    "# Create animation\n",
    "ani = animation.FuncAnimation(fig, update, frames=len(common_timestamps), interval=1000, repeat=False)\n",
    "\n",
    "# Save as GIF\n",
    "ani.save('combined_graph6.gif', writer='pillow', fps=1)\n",
    "\n",
    "print(\"✅ GIF saved as 'combined_graph6.gif'\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0489baa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport networkx as nx\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.lines import Line2D\\nimport matplotlib.animation as animation\\nimport os\\n\\n# Load and preprocess data\\ndf1 = pd.read_csv(\"anomaly_scores_filtered.csv\")\\ndf2 = pd.read_csv(\"final2_modified_sorted.csv\")\\n\\n# Required columns\\nrequired_cols1 = [\\'source_workload\\', \\'destination_workload\\', \\'timestamp\\', \\'status\\', \\'anomaly_score\\']\\nrequired_cols2 = [\\'timestamp\\', \\'instance\\', \\'pod\\', \\'Abnormality class\\', \\'Microservice\\']\\nif not all(col in df1.columns for col in required_cols1):\\n    raise ValueError(f\"anomaly_scores_filtered.csv must contain: {\\', \\'.join(required_cols1)}\")\\nif not all(col in df2.columns for col in required_cols2):\\n    raise ValueError(f\"final2_modified_sorted.csv must contain: {\\', \\'.join(required_cols2)}\")\\n\\n# Convert timestamps to datetime and remove timezone\\ndf1[\\'timestamp\\'] = pd.to_datetime(df1[\\'timestamp\\'], errors=\\'coerce\\').dt.tz_localize(None)\\ndf2[\\'timestamp\\'] = pd.to_datetime(df2[\\'timestamp\\'], errors=\\'coerce\\').dt.tz_localize(None)\\n\\n# Normalize pod names in df2 by taking the part before the first \\'-\\'\\ndf2[\\'pod_normalized\\'] = df2[\\'pod\\'].apply(lambda x: x.split(\\'-\\')[0] if isinstance(x, str) and \\'-\\' in x else x)\\n\\n# Find common timestamps (exact match down to seconds)\\ncommon_timestamps = sorted(set(df1[\\'timestamp\\']).intersection(set(df2[\\'timestamp\\'])))\\nif not common_timestamps:\\n    raise ValueError(\"No common timestamps found between the two datasets.\")\\n\\n# Debug: Print common timestamps\\nprint(\"Common timestamps in both datasets:\")\\nfor ts in common_timestamps:\\n    print(ts)\\n\\n# Collect all unique nodes (pods and instances)\\nall_pods = set(df1[\\'source_workload\\']).union(set(df1[\\'destination_workload\\'])).union(set(df2[\\'pod_normalized\\']))\\nall_instances = set(df2[\\'instance\\'])\\nall_nodes = all_pods.union(all_instances)\\n\\n# Create a graph with all nodes to compute fixed positions\\nG_all = nx.DiGraph()\\nfor node in all_nodes:\\n    node_type = \\'instance\\' if node in all_instances else \\'pod\\'\\n    G_all.add_node(node, type=node_type, label=node)\\n\\n# Compute fixed positions for all nodes\\nfixed_pos = nx.spring_layout(G_all, seed=84, k=0.5, iterations=50)\\n\\n# Function to get edge color for pod-to-pod communication\\ndef get_communication_color(status):\\n    return {\\n        \\'Healthy\\': \\'green\\',\\n        \\'Degraded\\': \\'blue\\',\\n        \\'Unavailable\\': \\'red\\'\\n    }.get(status, \\'gray\\')\\n\\n# Function to build the combined graph for a given timestamp\\ndef build_combined_graph(timestamp):\\n    G = nx.DiGraph()\\n    \\n    # Process pod-to-pod communications (from df1)\\n    df1_window = df1[df1[\\'timestamp\\'] == timestamp]\\n    for _, row in df1_window.iterrows():\\n        src = row[\\'source_workload\\']\\n        dst = row[\\'destination_workload\\']\\n        status = row[\\'status\\']\\n        score = row[\\'anomaly_score\\']\\n        color = get_communication_color(status)\\n        label = f\"{score:.2f}\" if score > 0.0 else \"\"\\n        \\n        G.add_node(src, type=\\'pod\\', label=src)\\n        G.add_node(dst, type=\\'pod\\', label=dst)\\n        G.add_edge(src, dst, type=\\'communication\\', color=color, label=label, score=score)\\n    \\n    # Process instance-to-pod relationships (from df2)\\n    df2_window = df2[df2[\\'timestamp\\'] == timestamp]\\n    # Debug: Print df2_window contents\\n    print(f\"df2 window at {timestamp}:\\n\", df2_window[[\\'instance\\', \\'pod\\', \\'pod_normalized\\', \\'Microservice\\', \\'Abnormality class\\']])\\n    \\n    microservice_name = None\\n    attack_type = None\\n    has_anomaly = False\\n    \\n    for _, row in df2_window.iterrows():\\n        instance = row[\\'instance\\']\\n        pod = row[\\'pod_normalized\\']  # Use normalized pod name\\n        abnormality = row[\\'Abnormality class\\']\\n        microservice = row[\\'Microservice\\']\\n        \\n        # Default edge color is green\\n        edge_color = \\'green\\'\\n        edge_anomaly = \\'Normal\\'\\n        \\n        # Check for anomaly and update edge color and anomaly status\\n        try:\\n            microservice_name_temp, attack_type_temp = microservice.split(\\'_\\', 1)\\n            # Check if microservice_name_temp matches pod (case-insensitive partial match)\\n            if abnormality != \\'Normal\\' and microservice_name_temp.lower() in pod.lower():\\n                edge_color = \\'red\\'\\n                edge_anomaly = abnormality\\n                has_anomaly = True\\n                microservice_name = microservice_name_temp  # Update every time a red edge is added\\n                attack_type = attack_type_temp\\n                print(f\"Red edge added at {timestamp}: {instance} -> {pod}, microservice={microservice_name_temp}, attack_type={attack_type_temp}, abnormality={abnormality}\")\\n        except:\\n            microservice_name_temp = microservice\\n            attack_type_temp = \\'\\'\\n            if abnormality != \\'Normal\\' and microservice_name_temp.lower() in pod.lower():\\n                edge_color = \\'red\\'\\n                edge_anomaly = abnormality\\n                has_anomaly = True\\n                microservice_name = microservice_name_temp  # Update every time a red edge is added\\n                attack_type = attack_type_temp\\n                print(f\"Red edge added at {timestamp}: {instance} -> {pod}, microservice={microservice_name_temp}, attack_type={attack_type_temp}, abnormality={abnormality}\")\\n        \\n        G.add_node(instance, type=\\'instance\\', label=instance)\\n        G.add_node(pod, type=\\'pod\\', label=pod)\\n        G.add_edge(instance, pod, type=\\'deployment\\', color=edge_color, anomaly=edge_anomaly)\\n    \\n    # Debug: Final state before return\\n    print(f\"Returning from build_combined_graph at {timestamp}: has_anomaly={has_anomaly}, microservice_name={microservice_name}, attack_type={attack_type}\")\\n    \\n    return G, microservice_name, attack_type, has_anomaly\\n\\n# Initialize plot\\nfig, ax = plt.subplots(figsize=(20, 10))\\n\\n# Animation update function\\ndef update(i):\\n    ax.clear()\\n    timestamp = common_timestamps[i]\\n    G, microservice_name, attack_type, has_anomaly = build_combined_graph(timestamp)\\n    \\n    # Debug: Print current timestamp and anomaly status\\n    print(f\"Processing timestamp: {timestamp}, has_anomaly={has_anomaly}, microservice_name={microservice_name}\")\\n    \\n    # Draw nodes (pods as circles, instances as squares)\\n    pod_nodes = [n for n, d in G.nodes(data=True) if d[\\'type\\'] == \\'pod\\']\\n    instance_nodes = [n for n, d in G.nodes(data=True) if d[\\'type\\'] == \\'instance\\']\\n    nx.draw_networkx_nodes(G, fixed_pos, nodelist=pod_nodes, node_size=600, node_color=\\'skyblue\\', \\n                          node_shape=\\'o\\', ax=ax)\\n    nx.draw_networkx_nodes(G, fixed_pos, nodelist=instance_nodes, node_size=600, node_color=\\'lightgreen\\', \\n                          node_shape=\\'s\\', ax=ax)\\n    \\n    # Draw edges (communication as solid, deployment as dashed)\\n    comm_edges = [(u, v) for u, v, d in G.edges(data=True) if d[\\'type\\'] == \\'communication\\']\\n    deploy_edges = [(u, v) for u, v, d in G.edges(data=True) if d[\\'type\\'] == \\'deployment\\']\\n    comm_colors = [G[u][v][\\'color\\'] for u, v in comm_edges]\\n    deploy_colors = [G[u][v][\\'color\\'] for u, v in deploy_edges]\\n    nx.draw_networkx_edges(G, fixed_pos, edgelist=comm_edges, edge_color=comm_colors, width=2, ax=ax)\\n    nx.draw_networkx_edges(G, fixed_pos, edgelist=deploy_edges, edge_color=deploy_colors, width=2, \\n                          style=\\'dashed\\', ax=ax)\\n    \\n    # Draw node labels\\n    nx.draw_networkx_labels(G, fixed_pos, labels=nx.get_node_attributes(G, \\'label\\'), \\n                           font_size=10, font_weight=\\'bold\\', ax=ax)\\n    \\n    # Draw edge labels for communications\\n    edge_labels = {(u, v): d[\\'label\\'] for u, v, d in G.edges(data=True) \\n                   if d[\\'type\\'] == \\'communication\\' and d[\\'label\\']}\\n    label_pos = {k: (v[0], v[1] + 0.03) for k, v in fixed_pos.items()}\\n    nx.draw_networkx_edge_labels(G, pos=label_pos, edge_labels=edge_labels, \\n                                font_size=9, font_color=\\'black\\', ax=ax)\\n    \\n    # Legends\\n    comm_legend = [\\n        Line2D([0], [0], color=\\'green\\', lw=2, label=\\'Healthy\\'),\\n        Line2D([0], [0], color=\\'blue\\', lw=2, label=\\'Degraded\\'),\\n        Line2D([0], [0], color=\\'red\\', lw=2, label=\\'Unavailable\\')\\n    ]\\n    anomaly_legend = [\\n        Line2D([0], [0], color=\\'green\\', lw=2, linestyle=\\'--\\', label=\\'Normal\\'),\\n        Line2D([0], [0], color=\\'red\\', lw=2, linestyle=\\'--\\', label=\\'Anomaly\\')\\n    ]\\n    ax.legend(handles=comm_legend + anomaly_legend, loc=\\'upper right\\')\\n    \\n    # Title with exact timestamp\\n    ax.set_title(f\"Combined Graph for {timestamp.strftime(\\'%Y-%m-%d %H:%M:%S\\')}\", fontsize=14)\\n    \\n    # Display attack information if anomaly detected\\n    if has_anomaly:\\n        plt.figtext(0.5, 0.01, f\"Microservice attaqué : {microservice_name or \\'UNKNOWN\\'}, Type : {attack_type.upper() if attack_type else \\'UNKNOWN\\'}\",\\n                    wrap=True, horizontalalignment=\\'center\\', fontsize=12)\\n    else:\\n        plt.figtext(0.5, 0.01, \"Aucune attaque détectée\",\\n                    wrap=True, horizontalalignment=\\'center\\', fontsize=12)\\n    \\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\\n    ax.axis(\\'off\\')\\n    \\n    # Display the plot in Visual Studio Code\\n    plt.show()\\n\\n# Create animation\\nani = animation.FuncAnimation(fig, update, frames=len(common_timestamps), interval=1000, repeat=False)\\n\\n# Save as GIF\\nani.save(\\'combined_graph7.gif\\', writer=\\'pillow\\', fps=1)\\n\\nprint(\"✅ GIF saved as \\'combined_graph7.gif\\'\")\\nplt.close()'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.animation as animation\n",
    "import os\n",
    "\n",
    "# Load and preprocess data\n",
    "df1 = pd.read_csv(\"anomaly_scores_filtered.csv\")\n",
    "df2 = pd.read_csv(\"final2_modified_sorted.csv\")\n",
    "\n",
    "# Required columns\n",
    "required_cols1 = ['source_workload', 'destination_workload', 'timestamp', 'status', 'anomaly_score']\n",
    "required_cols2 = ['timestamp', 'instance', 'pod', 'Abnormality class', 'Microservice']\n",
    "if not all(col in df1.columns for col in required_cols1):\n",
    "    raise ValueError(f\"anomaly_scores_filtered.csv must contain: {', '.join(required_cols1)}\")\n",
    "if not all(col in df2.columns for col in required_cols2):\n",
    "    raise ValueError(f\"final2_modified_sorted.csv must contain: {', '.join(required_cols2)}\")\n",
    "\n",
    "# Convert timestamps to datetime and remove timezone\n",
    "df1['timestamp'] = pd.to_datetime(df1['timestamp'], errors='coerce').dt.tz_localize(None)\n",
    "df2['timestamp'] = pd.to_datetime(df2['timestamp'], errors='coerce').dt.tz_localize(None)\n",
    "\n",
    "# Normalize pod names in df2 by taking the part before the first '-'\n",
    "df2['pod_normalized'] = df2['pod'].apply(lambda x: x.split('-')[0] if isinstance(x, str) and '-' in x else x)\n",
    "\n",
    "# Find common timestamps (exact match down to seconds)\n",
    "common_timestamps = sorted(set(df1['timestamp']).intersection(set(df2['timestamp'])))\n",
    "if not common_timestamps:\n",
    "    raise ValueError(\"No common timestamps found between the two datasets.\")\n",
    "\n",
    "# Debug: Print common timestamps\n",
    "print(\"Common timestamps in both datasets:\")\n",
    "for ts in common_timestamps:\n",
    "    print(ts)\n",
    "\n",
    "# Collect all unique nodes (pods and instances)\n",
    "all_pods = set(df1['source_workload']).union(set(df1['destination_workload'])).union(set(df2['pod_normalized']))\n",
    "all_instances = set(df2['instance'])\n",
    "all_nodes = all_pods.union(all_instances)\n",
    "\n",
    "# Create a graph with all nodes to compute fixed positions\n",
    "G_all = nx.DiGraph()\n",
    "for node in all_nodes:\n",
    "    node_type = 'instance' if node in all_instances else 'pod'\n",
    "    G_all.add_node(node, type=node_type, label=node)\n",
    "\n",
    "# Compute fixed positions for all nodes\n",
    "fixed_pos = nx.spring_layout(G_all, seed=84, k=0.5, iterations=50)\n",
    "\n",
    "# Function to get edge color for pod-to-pod communication\n",
    "def get_communication_color(status):\n",
    "    return {\n",
    "        'Healthy': 'green',\n",
    "        'Degraded': 'blue',\n",
    "        'Unavailable': 'red'\n",
    "    }.get(status, 'gray')\n",
    "\n",
    "# Function to build the combined graph for a given timestamp\n",
    "def build_combined_graph(timestamp):\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Process pod-to-pod communications (from df1)\n",
    "    df1_window = df1[df1['timestamp'] == timestamp]\n",
    "    for _, row in df1_window.iterrows():\n",
    "        src = row['source_workload']\n",
    "        dst = row['destination_workload']\n",
    "        status = row['status']\n",
    "        score = row['anomaly_score']\n",
    "        color = get_communication_color(status)\n",
    "        label = f\"{score:.2f}\" if score > 0.0 else \"\"\n",
    "        \n",
    "        G.add_node(src, type='pod', label=src)\n",
    "        G.add_node(dst, type='pod', label=dst)\n",
    "        G.add_edge(src, dst, type='communication', color=color, label=label, score=score)\n",
    "    \n",
    "    # Process instance-to-pod relationships (from df2)\n",
    "    df2_window = df2[df2['timestamp'] == timestamp]\n",
    "    # Debug: Print df2_window contents\n",
    "    print(f\"df2 window at {timestamp}:\\n\", df2_window[['instance', 'pod', 'pod_normalized', 'Microservice', 'Abnormality class']])\n",
    "    \n",
    "    microservice_name = None\n",
    "    attack_type = None\n",
    "    has_anomaly = False\n",
    "    \n",
    "    for _, row in df2_window.iterrows():\n",
    "        instance = row['instance']\n",
    "        pod = row['pod_normalized']  # Use normalized pod name\n",
    "        abnormality = row['Abnormality class']\n",
    "        microservice = row['Microservice']\n",
    "        \n",
    "        # Default edge color is green\n",
    "        edge_color = 'green'\n",
    "        edge_anomaly = 'Normal'\n",
    "        \n",
    "        # Check for anomaly and update edge color and anomaly status\n",
    "        try:\n",
    "            microservice_name_temp, attack_type_temp = microservice.split('_', 1)\n",
    "            # Check if microservice_name_temp matches pod (case-insensitive partial match)\n",
    "            if abnormality != 'Normal' and microservice_name_temp.lower() in pod.lower():\n",
    "                edge_color = 'red'\n",
    "                edge_anomaly = abnormality\n",
    "                has_anomaly = True\n",
    "                microservice_name = microservice_name_temp  # Update every time a red edge is added\n",
    "                attack_type = attack_type_temp\n",
    "                print(f\"Red edge added at {timestamp}: {instance} -> {pod}, microservice={microservice_name_temp}, attack_type={attack_type_temp}, abnormality={abnormality}\")\n",
    "        except:\n",
    "            microservice_name_temp = microservice\n",
    "            attack_type_temp = ''\n",
    "            if abnormality != 'Normal' and microservice_name_temp.lower() in pod.lower():\n",
    "                edge_color = 'red'\n",
    "                edge_anomaly = abnormality\n",
    "                has_anomaly = True\n",
    "                microservice_name = microservice_name_temp  # Update every time a red edge is added\n",
    "                attack_type = attack_type_temp\n",
    "                print(f\"Red edge added at {timestamp}: {instance} -> {pod}, microservice={microservice_name_temp}, attack_type={attack_type_temp}, abnormality={abnormality}\")\n",
    "        \n",
    "        G.add_node(instance, type='instance', label=instance)\n",
    "        G.add_node(pod, type='pod', label=pod)\n",
    "        G.add_edge(instance, pod, type='deployment', color=edge_color, anomaly=edge_anomaly)\n",
    "    \n",
    "    # Debug: Final state before return\n",
    "    print(f\"Returning from build_combined_graph at {timestamp}: has_anomaly={has_anomaly}, microservice_name={microservice_name}, attack_type={attack_type}\")\n",
    "    \n",
    "    return G, microservice_name, attack_type, has_anomaly\n",
    "\n",
    "# Initialize plot\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "# Animation update function\n",
    "def update(i):\n",
    "    ax.clear()\n",
    "    timestamp = common_timestamps[i]\n",
    "    G, microservice_name, attack_type, has_anomaly = build_combined_graph(timestamp)\n",
    "    \n",
    "    # Debug: Print current timestamp and anomaly status\n",
    "    print(f\"Processing timestamp: {timestamp}, has_anomaly={has_anomaly}, microservice_name={microservice_name}\")\n",
    "    \n",
    "    # Draw nodes (pods as circles, instances as squares)\n",
    "    pod_nodes = [n for n, d in G.nodes(data=True) if d['type'] == 'pod']\n",
    "    instance_nodes = [n for n, d in G.nodes(data=True) if d['type'] == 'instance']\n",
    "    nx.draw_networkx_nodes(G, fixed_pos, nodelist=pod_nodes, node_size=600, node_color='skyblue', \n",
    "                          node_shape='o', ax=ax)\n",
    "    nx.draw_networkx_nodes(G, fixed_pos, nodelist=instance_nodes, node_size=600, node_color='lightgreen', \n",
    "                          node_shape='s', ax=ax)\n",
    "    \n",
    "    # Draw edges (communication as solid, deployment as dashed)\n",
    "    comm_edges = [(u, v) for u, v, d in G.edges(data=True) if d['type'] == 'communication']\n",
    "    deploy_edges = [(u, v) for u, v, d in G.edges(data=True) if d['type'] == 'deployment']\n",
    "    comm_colors = [G[u][v]['color'] for u, v in comm_edges]\n",
    "    deploy_colors = [G[u][v]['color'] for u, v in deploy_edges]\n",
    "    nx.draw_networkx_edges(G, fixed_pos, edgelist=comm_edges, edge_color=comm_colors, width=2, ax=ax)\n",
    "    nx.draw_networkx_edges(G, fixed_pos, edgelist=deploy_edges, edge_color=deploy_colors, width=2, \n",
    "                          style='dashed', ax=ax)\n",
    "    \n",
    "    # Draw node labels\n",
    "    nx.draw_networkx_labels(G, fixed_pos, labels=nx.get_node_attributes(G, 'label'), \n",
    "                           font_size=10, font_weight='bold', ax=ax)\n",
    "    \n",
    "    # Draw edge labels for communications\n",
    "    edge_labels = {(u, v): d['label'] for u, v, d in G.edges(data=True) \n",
    "                   if d['type'] == 'communication' and d['label']}\n",
    "    label_pos = {k: (v[0], v[1] + 0.03) for k, v in fixed_pos.items()}\n",
    "    nx.draw_networkx_edge_labels(G, pos=label_pos, edge_labels=edge_labels, \n",
    "                                font_size=9, font_color='black', ax=ax)\n",
    "    \n",
    "    # Legends\n",
    "    comm_legend = [\n",
    "        Line2D([0], [0], color='green', lw=2, label='Healthy'),\n",
    "        Line2D([0], [0], color='blue', lw=2, label='Degraded'),\n",
    "        Line2D([0], [0], color='red', lw=2, label='Unavailable')\n",
    "    ]\n",
    "    anomaly_legend = [\n",
    "        Line2D([0], [0], color='green', lw=2, linestyle='--', label='Normal'),\n",
    "        Line2D([0], [0], color='red', lw=2, linestyle='--', label='Anomaly')\n",
    "    ]\n",
    "    ax.legend(handles=comm_legend + anomaly_legend, loc='upper right')\n",
    "    \n",
    "    # Title with exact timestamp\n",
    "    ax.set_title(f\"Combined Graph for {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\", fontsize=14)\n",
    "    \n",
    "    # Display attack information if anomaly detected\n",
    "    if has_anomaly:\n",
    "        plt.figtext(0.5, 0.01, f\"Microservice attaqué : {microservice_name or 'UNKNOWN'}, Type : {attack_type.upper() if attack_type else 'UNKNOWN'}\",\n",
    "                    wrap=True, horizontalalignment='center', fontsize=12)\n",
    "    else:\n",
    "        plt.figtext(0.5, 0.01, \"Aucune attaque détectée\",\n",
    "                    wrap=True, horizontalalignment='center', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Display the plot in Visual Studio Code\n",
    "    plt.show()\n",
    "\n",
    "# Create animation\n",
    "ani = animation.FuncAnimation(fig, update, frames=len(common_timestamps), interval=1000, repeat=False)\n",
    "\n",
    "# Save as GIF\n",
    "ani.save('combined_graph7.gif', writer='pillow', fps=1)\n",
    "\n",
    "print(\"✅ GIF saved as 'combined_graph7.gif'\")\n",
    "plt.close()'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
