{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38a9d0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_28144\\1763659225.py:85: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 | Loss: 0.8456\n",
      "Epoch 020 | Loss: 0.7659\n",
      "Epoch 030 | Loss: 0.6720\n",
      "Epoch 040 | Loss: 0.3365\n",
      "Epoch 050 | Loss: 0.7221\n",
      "Epoch 060 | Loss: 0.5454\n",
      "Epoch 070 | Loss: 0.7334\n",
      "Epoch 080 | Loss: 0.9527\n",
      "Epoch 090 | Loss: 0.3042\n",
      "Epoch 100 | Loss: 0.1452\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     healthy       0.92      1.00      0.96       357\n",
      "    degraded       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.92       388\n",
      "   macro avg       0.46      0.50      0.48       388\n",
      "weighted avg       0.85      0.92      0.88       388\n",
      "\n",
      "Macro F1-score: 0.4791946308724832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import classification_report, f1_score  # Added f1_score import\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "# üîÑ Chargement des donn√©es\n",
    "df = pd.read_csv(\"kiali_kpi_metrics.csv\")\n",
    "\n",
    "# üßΩ Nettoyage de time_window\n",
    "df['time_window'] = df['time_window'].astype(str).str.strip()\n",
    "df = df[df['time_window'] == \"15S\"].copy()\n",
    "\n",
    "if df.empty:\n",
    "    raise ValueError(\"üö® Aucun enregistrement avec time_window == '15s'. V√©rifiez le fichier CSV.\")\n",
    "\n",
    "# ‚úÖ Nettoyage des colonnes num√©riques\n",
    "df['error_rate'] = pd.to_numeric(df['error_rate'], errors='coerce').fillna(0.0)\n",
    "\n",
    "# üè∑Ô∏è Attribution du statut\n",
    "def assign_status(er):\n",
    "    if er < 0.01:\n",
    "        return 0\n",
    "    elif er < 0.1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def assign_status(er):\n",
    "    if er < 0.15:\n",
    "        return 0\n",
    "    \n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "\n",
    "df['status'] = df['error_rate'].apply(assign_status)\n",
    "df.drop(df[df['istio_request_bytes'] == 0.0].index, inplace = True)\n",
    "df.drop(df[df['new_request'] < 0.0].index, inplace = True)\n",
    "df = df.dropna(subset=[\"request_rate\"])\n",
    "df.to_csv(\"data2.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MicroserviceDataset:\n",
    "    def __init__(self, df, node_map, features, target):\n",
    "        self.df = df\n",
    "        self.node_map = node_map\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1  # We'll treat the whole dataset as one graph\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        edge_index = []\n",
    "        edge_attr = []\n",
    "        edge_labels = []\n",
    "        \n",
    "        # Create edges with features and labels\n",
    "        for _, row in self.df.iterrows():\n",
    "            src = self.node_map[row['source_workload']]\n",
    "            tgt = self.node_map[row['destination_workload']]\n",
    "            edge_index.append([src, tgt])\n",
    "            edge_attr.append(row[self.features].values)\n",
    "            edge_labels.append(row[self.target])\n",
    "        \n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "        edge_labels = torch.tensor(edge_labels, dtype=torch.long)\n",
    "        \n",
    "        # Create node features (using random features since we don't have node features)\n",
    "        num_nodes = len(self.node_map)\n",
    "        x = torch.randn((num_nodes, 16))  # 16-dimensional random features\n",
    "        \n",
    "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=edge_labels)\n",
    "\n",
    "class EdgePredictorGNN(nn.Module):\n",
    "    def __init__(self, node_features, edge_features, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(node_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.edge_predictor = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim + edge_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # Node embeddings\n",
    "        x = self.conv1(data.x, data.edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, data.edge_index)\n",
    "        \n",
    "        # Edge prediction\n",
    "        src, dst = data.edge_index\n",
    "        h_src = x[src]\n",
    "        h_dst = x[dst]\n",
    "        \n",
    "        # Combine node embeddings and edge features\n",
    "        edge_emb = torch.cat([h_src, h_dst, data.edge_attr], dim=1)\n",
    "        return self.edge_predictor(edge_emb)\n",
    "\n",
    "def train():\n",
    "    # Load and preprocess data\n",
    "    df = pd.read_csv('data2.csv')\n",
    "    \n",
    "\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']).astype(int) / 10**9  # Convert to seconds\n",
    "\n",
    "\n",
    "    # Prepare features and target\n",
    "    features = ['throughput', 'duration_milliseconds', 'request_rate','istio_request_bytes','average_latency','istio_request_bytes','new_request','timestamp']  # Excluding error_rate\n",
    "    target = 'status'\n",
    "    scaler = MinMaxScaler()\n",
    "    df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "    #df[features] = StandardScaler().fit_transform(df[features])\n",
    "    \n",
    "    # Create node mapping\n",
    "    all_nodes = sorted(list(set(df['source_workload'].unique()).union(set(df['destination_workload'].unique()))))\n",
    "    node_map = {node: i for i, node in enumerate(all_nodes)}\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = MicroserviceDataset(df, node_map, features, target)\n",
    "    \n",
    "    # Since we have one graph, we'll use a single DataLoader with batch_size=1\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = EdgePredictorGNN(\n",
    "        node_features=16,  # Matches our random node features\n",
    "        edge_features=len(features),\n",
    "        hidden_dim=64,\n",
    "        num_classes=2  # healthy, degraded, error\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1, 101):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Print training stats\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch:03d} | Loss: {total_loss/len(loader):.4f}')\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data = dataset[0].to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1).cpu().numpy()\n",
    "        truth = data.y.cpu().numpy()\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(truth, pred, target_names=['healthy', 'degraded']))\n",
    "        print(\"Macro F1-score:\", f1_score(truth, pred, average='macro'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1af92bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 1.1142\n",
      "Epoch 2 | Loss: 1.0303\n",
      "Epoch 3 | Loss: 0.9176\n",
      "Epoch 4 | Loss: 0.8178\n",
      "Epoch 5 | Loss: 0.7378\n",
      "Epoch 6 | Loss: 0.7224\n",
      "Epoch 7 | Loss: 0.7221\n",
      "Epoch 8 | Loss: 0.7077\n",
      "Epoch 9 | Loss: 0.7270\n",
      "Epoch 10 | Loss: 0.6740\n",
      "Epoch 11 | Loss: 0.6735\n",
      "Epoch 12 | Loss: 0.6391\n",
      "Epoch 13 | Loss: 0.6165\n",
      "Epoch 14 | Loss: 0.6067\n",
      "Epoch 15 | Loss: 0.5945\n",
      "Epoch 16 | Loss: 0.5808\n",
      "Epoch 17 | Loss: 0.5707\n",
      "Epoch 18 | Loss: 0.5553\n",
      "Epoch 19 | Loss: 0.5358\n",
      "Epoch 20 | Loss: 0.5249\n",
      "Epoch 21 | Loss: 0.5094\n",
      "Epoch 22 | Loss: 0.5066\n",
      "Epoch 23 | Loss: 0.5156\n",
      "Epoch 24 | Loss: 0.5145\n",
      "Epoch 25 | Loss: 0.5041\n",
      "Epoch 26 | Loss: 0.5084\n",
      "Epoch 27 | Loss: 0.4858\n",
      "Epoch 28 | Loss: 0.4872\n",
      "Epoch 29 | Loss: 0.4876\n",
      "Epoch 30 | Loss: 0.4910\n",
      "Epoch 31 | Loss: 0.4874\n",
      "Epoch 32 | Loss: 0.4892\n",
      "Epoch 33 | Loss: 0.4781\n",
      "Epoch 34 | Loss: 0.4846\n",
      "Epoch 35 | Loss: 0.4791\n",
      "Epoch 36 | Loss: 0.4806\n",
      "Epoch 37 | Loss: 0.4765\n",
      "Epoch 38 | Loss: 0.4792\n",
      "Epoch 39 | Loss: 0.4740\n",
      "Epoch 40 | Loss: 0.4763\n",
      "Epoch 41 | Loss: 0.4726\n",
      "Epoch 42 | Loss: 0.4724\n",
      "Epoch 43 | Loss: 0.4717\n",
      "Epoch 44 | Loss: 0.4716\n",
      "Epoch 45 | Loss: 0.4704\n",
      "Epoch 46 | Loss: 0.4693\n",
      "Epoch 47 | Loss: 0.4685\n",
      "Epoch 48 | Loss: 0.4664\n",
      "Epoch 49 | Loss: 0.4665\n",
      "Epoch 50 | Loss: 0.4646\n",
      "Epoch 51 | Loss: 0.4641\n",
      "Epoch 52 | Loss: 0.4624\n",
      "Epoch 53 | Loss: 0.4626\n",
      "Epoch 54 | Loss: 0.4612\n",
      "Epoch 55 | Loss: 0.4606\n",
      "Epoch 56 | Loss: 0.4591\n",
      "Epoch 57 | Loss: 0.4588\n",
      "Epoch 58 | Loss: 0.4579\n",
      "Epoch 59 | Loss: 0.4568\n",
      "Epoch 60 | Loss: 0.4558\n",
      "Epoch 61 | Loss: 0.4549\n",
      "Epoch 62 | Loss: 0.4542\n",
      "Epoch 63 | Loss: 0.4533\n",
      "Epoch 64 | Loss: 0.4527\n",
      "Epoch 65 | Loss: 0.4521\n",
      "Epoch 66 | Loss: 0.4525\n",
      "Epoch 67 | Loss: 0.4558\n",
      "Epoch 68 | Loss: 0.4644\n",
      "Epoch 69 | Loss: 0.4879\n",
      "Epoch 70 | Loss: 0.4608\n",
      "Epoch 71 | Loss: 0.4470\n",
      "Epoch 72 | Loss: 0.4554\n",
      "Epoch 73 | Loss: 0.4604\n",
      "Epoch 74 | Loss: 0.4552\n",
      "Epoch 75 | Loss: 0.4455\n",
      "Epoch 76 | Loss: 0.4589\n",
      "Epoch 77 | Loss: 0.4592\n",
      "Epoch 78 | Loss: 0.4453\n",
      "Epoch 79 | Loss: 0.4636\n",
      "Epoch 80 | Loss: 0.4552\n",
      "Epoch 81 | Loss: 0.4472\n",
      "Epoch 82 | Loss: 0.4568\n",
      "Epoch 83 | Loss: 0.4409\n",
      "Epoch 84 | Loss: 0.4511\n",
      "Epoch 85 | Loss: 0.4413\n",
      "Epoch 86 | Loss: 0.4469\n",
      "Epoch 87 | Loss: 0.4420\n",
      "Epoch 88 | Loss: 0.4435\n",
      "Epoch 89 | Loss: 0.4423\n",
      "Epoch 90 | Loss: 0.4397\n",
      "Epoch 91 | Loss: 0.4418\n",
      "Epoch 92 | Loss: 0.4381\n",
      "Epoch 93 | Loss: 0.4414\n",
      "Epoch 94 | Loss: 0.4367\n",
      "Epoch 95 | Loss: 0.4396\n",
      "Epoch 96 | Loss: 0.4360\n",
      "Epoch 97 | Loss: 0.4382\n",
      "Epoch 98 | Loss: 0.4360\n",
      "Epoch 99 | Loss: 0.4363\n",
      "Epoch 100 | Loss: 0.4358\n",
      "Epoch 101 | Loss: 0.4345\n",
      "Epoch 102 | Loss: 0.4355\n",
      "Epoch 103 | Loss: 0.4334\n",
      "Epoch 104 | Loss: 0.4346\n",
      "Epoch 105 | Loss: 0.4334\n",
      "Epoch 106 | Loss: 0.4328\n",
      "Epoch 107 | Loss: 0.4333\n",
      "Epoch 108 | Loss: 0.4321\n",
      "Epoch 109 | Loss: 0.4317\n",
      "Epoch 110 | Loss: 0.4322\n",
      "Epoch 111 | Loss: 0.4311\n",
      "Epoch 112 | Loss: 0.4306\n",
      "Epoch 113 | Loss: 0.4312\n",
      "Epoch 114 | Loss: 0.4309\n",
      "Epoch 115 | Loss: 0.4298\n",
      "Epoch 116 | Loss: 0.4295\n",
      "Epoch 117 | Loss: 0.4301\n",
      "Epoch 118 | Loss: 0.4300\n",
      "Epoch 119 | Loss: 0.4299\n",
      "Epoch 120 | Loss: 0.4292\n",
      "Epoch 121 | Loss: 0.4286\n",
      "Epoch 122 | Loss: 0.4280\n",
      "Epoch 123 | Loss: 0.4277\n",
      "Epoch 124 | Loss: 0.4275\n",
      "Epoch 125 | Loss: 0.4271\n",
      "Epoch 126 | Loss: 0.4269\n",
      "Epoch 127 | Loss: 0.4269\n",
      "Epoch 128 | Loss: 0.4273\n",
      "Epoch 129 | Loss: 0.4305\n",
      "Epoch 130 | Loss: 0.4447\n",
      "Epoch 131 | Loss: 0.5035\n",
      "Epoch 132 | Loss: 0.4511\n",
      "Epoch 133 | Loss: 0.4264\n",
      "Epoch 134 | Loss: 0.4431\n",
      "Epoch 135 | Loss: 0.4309\n",
      "Epoch 136 | Loss: 0.4319\n",
      "Epoch 137 | Loss: 0.4303\n",
      "Epoch 138 | Loss: 0.4303\n",
      "Epoch 139 | Loss: 0.4277\n",
      "Epoch 140 | Loss: 0.4332\n",
      "Epoch 141 | Loss: 0.4274\n",
      "Epoch 142 | Loss: 0.4303\n",
      "Epoch 143 | Loss: 0.4273\n",
      "Epoch 144 | Loss: 0.4280\n",
      "Epoch 145 | Loss: 0.4281\n",
      "Epoch 146 | Loss: 0.4268\n",
      "Epoch 147 | Loss: 0.4273\n",
      "Epoch 148 | Loss: 0.4263\n",
      "Epoch 149 | Loss: 0.4261\n",
      "Epoch 150 | Loss: 0.4250\n",
      "Epoch 151 | Loss: 0.4251\n",
      "Epoch 152 | Loss: 0.4243\n",
      "Epoch 153 | Loss: 0.4244\n",
      "Epoch 154 | Loss: 0.4234\n",
      "Epoch 155 | Loss: 0.4233\n",
      "Epoch 156 | Loss: 0.4235\n",
      "Epoch 157 | Loss: 0.4224\n",
      "Epoch 158 | Loss: 0.4225\n",
      "Epoch 159 | Loss: 0.4220\n",
      "Epoch 160 | Loss: 0.4217\n",
      "Epoch 161 | Loss: 0.4211\n",
      "Epoch 162 | Loss: 0.4212\n",
      "Epoch 163 | Loss: 0.4209\n",
      "Epoch 164 | Loss: 0.4202\n",
      "Epoch 165 | Loss: 0.4201\n",
      "Epoch 166 | Loss: 0.4197\n",
      "Epoch 167 | Loss: 0.4198\n",
      "Epoch 168 | Loss: 0.4190\n",
      "Epoch 169 | Loss: 0.4184\n",
      "Epoch 170 | Loss: 0.4181\n",
      "Epoch 171 | Loss: 0.4177\n",
      "Epoch 172 | Loss: 0.4174\n",
      "Epoch 173 | Loss: 0.4170\n",
      "Epoch 174 | Loss: 0.4170\n",
      "Epoch 175 | Loss: 0.4173\n",
      "Epoch 176 | Loss: 0.4190\n",
      "Epoch 177 | Loss: 0.4238\n",
      "Epoch 178 | Loss: 0.4436\n",
      "Epoch 179 | Loss: 0.4508\n",
      "Epoch 180 | Loss: 0.4658\n",
      "Epoch 181 | Loss: 0.4180\n",
      "Epoch 182 | Loss: 0.4381\n",
      "Epoch 183 | Loss: 0.4661\n",
      "Epoch 184 | Loss: 0.4268\n",
      "Epoch 185 | Loss: 0.4600\n",
      "Epoch 186 | Loss: 0.4455\n",
      "Epoch 187 | Loss: 0.4378\n",
      "Epoch 188 | Loss: 0.4354\n",
      "Epoch 189 | Loss: 0.4234\n",
      "Epoch 190 | Loss: 0.4416\n",
      "Epoch 191 | Loss: 0.4238\n",
      "Epoch 192 | Loss: 0.4372\n",
      "Epoch 193 | Loss: 0.4234\n",
      "‚èπ Early stopping triggered\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nClassification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   saludable       1.00      0.75      0.85       326\n",
      "   degradado       0.25      0.87      0.39        31\n",
      " fallo grave       0.70      0.84      0.76        31\n",
      "\n",
      "    accuracy                           0.77       388\n",
      "   macro avg       0.65      0.82      0.67       388\n",
      "weighted avg       0.91      0.77      0.81       388\n",
      "\n",
      "Macro F1-score: 0.671169149010033\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# üì• Cargar datos\n",
    "df = pd.read_csv(\"kiali_kpi_metrics.csv\")\n",
    "df['time_window'] = df['time_window'].astype(str).str.strip()\n",
    "df = df[df['time_window'] == \"15S\"].copy()\n",
    "df['error_rate'] = pd.to_numeric(df['error_rate'], errors='coerce').fillna(0.0)\n",
    "df = df[df['istio_request_bytes'] != 0.0]\n",
    "df = df[df['new_request'] >= 0.0]\n",
    "df = df.dropna(subset=[\"request_rate\"])\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp']).astype(int) / 10**9\n",
    "\n",
    "# üè∑ Etiquetas multiclase\n",
    "def assign_status(er):\n",
    "    if er < 0.01:\n",
    "        return 0\n",
    "    elif er < 0.15:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "df['status'] = df['error_rate'].apply(assign_status)\n",
    "\n",
    "# üéõ Normalizaci√≥n\n",
    "features = ['throughput', 'duration_milliseconds', 'request_rate',\n",
    "            'istio_request_bytes', 'average_latency', 'new_request', 'timestamp']\n",
    "df[features] = MinMaxScaler().fit_transform(df[features])\n",
    "target = 'status'\n",
    "\n",
    "# üîÄ Mapeo de nodos y estad√≠sticas como embeddings\n",
    "all_nodes = sorted(set(df['source_workload']).union(df['destination_workload']))\n",
    "node_map = {node: i for i, node in enumerate(all_nodes)}\n",
    "\n",
    "# Crear embeddings reales de nodos\n",
    "node_stats = df.groupby('source_workload')[features].mean()\n",
    "x_features = np.zeros((len(node_map), len(features)))\n",
    "for node, idx in node_map.items():\n",
    "    if node in node_stats.index:\n",
    "        x_features[idx] = node_stats.loc[node].values\n",
    "    else:\n",
    "        x_features[idx] = np.random.rand(len(features))  # fallback aleatorio\n",
    "\n",
    "x_tensor = torch.tensor(x_features, dtype=torch.float)\n",
    "\n",
    "# üîÅ Dataset\n",
    "class MicroserviceDataset:\n",
    "    def __init__(self, df, node_map, features, target):\n",
    "        self.df = df\n",
    "        self.node_map = node_map\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self): return 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        edge_index, edge_attr, edge_labels = [], [], []\n",
    "\n",
    "        for _, row in self.df.iterrows():\n",
    "            src = self.node_map[row['source_workload']]\n",
    "            tgt = self.node_map[row['destination_workload']]\n",
    "            edge_index.append([src, tgt])\n",
    "            edge_attr.append(row[self.features].values)\n",
    "            edge_labels.append(row[self.target])\n",
    "\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "        edge_labels = torch.tensor(edge_labels, dtype=torch.long)\n",
    "\n",
    "        return Data(x=x_tensor, edge_index=edge_index, edge_attr=edge_attr, y=edge_labels)\n",
    "\n",
    "# üß† Modelo\n",
    "class EdgePredictorGNN(nn.Module):\n",
    "    def __init__(self, node_features, edge_features, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(node_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.edge_predictor = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim + edge_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.conv1(data.x, data.edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, data.edge_index)\n",
    "        src, dst = data.edge_index\n",
    "        h_src = x[src]\n",
    "        h_dst = x[dst]\n",
    "        edge_emb = torch.cat([h_src, h_dst, data.edge_attr], dim=1)\n",
    "        return self.edge_predictor(edge_emb)\n",
    "\n",
    "# ‚öôÔ∏è Configuraci√≥n y entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = MicroserviceDataset(df, node_map, features, target)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "model = EdgePredictorGNN(7, 7, 64, 3).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# üßÆ Pesos de clases\n",
    "class_counts = Counter(df['status'])\n",
    "total = sum(class_counts.values())\n",
    "weights = [total / class_counts[i] for i in range(3)]\n",
    "weights_tensor = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "# üöÇ Entrenamiento + logging\n",
    "losses = []\n",
    "best_loss = float(\"inf\")\n",
    "patience = 20\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(1, 301):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "    print(f\"Epoch {epoch} | Loss: {total_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"‚èπ Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# üìà Gr√°fico de p√©rdida\n",
    "plt.plot(losses)\n",
    "plt.title(\"P√©rdida por √©poca\")\n",
    "plt.xlabel(\"√âpoca\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# üìä Evaluaci√≥n\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    data = dataset[0].to(device)\n",
    "    out = model(data)\n",
    "    pred = out.argmax(dim=1).cpu().numpy()\n",
    "    truth = data.y.cpu().numpy()\n",
    "    print(\"\\\\nClassification Report:\")\n",
    "    print(classification_report(truth, pred, target_names=['saludable', 'degradado', 'fallo grave']))\n",
    "    print(\"Macro F1-score:\", f1_score(truth, pred, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a809ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dgl\n",
      "  Downloading dgl-2.2.1-cp310-cp310-win_amd64.whl.metadata (595 bytes)\n",
      "Collecting numpy>=1.14.0 (from dgl)\n",
      "  Downloading numpy-2.2.6-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting scipy>=1.1.0 (from dgl)\n",
      "  Downloading scipy-1.15.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting networkx>=2.1 (from dgl)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting requests>=2.19.0 (from dgl)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tqdm (from dgl)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting psutil>=5.8.0 (from dgl)\n",
      "  Downloading psutil-7.0.0-cp37-abi3-win_amd64.whl.metadata (23 kB)\n",
      "Collecting torchdata>=0.5.0 (from dgl)\n",
      "  Downloading torchdata-0.11.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting pandas (from dgl)\n",
      "  Downloading pandas-2.3.0-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.19.0->dgl)\n",
      "  Downloading charset_normalizer-3.4.2-cp310-cp310-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.19.0->dgl)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->dgl)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.19.0->dgl)\n",
      "  Downloading certifi-2025.6.15-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting torch>=2 (from torchdata>=0.5.0->dgl)\n",
      "  Downloading torch-2.7.1-cp310-cp310-win_amd64.whl.metadata (28 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->dgl)\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->dgl)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->dgl)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting colorama (from tqdm->dgl)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->dgl)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting filelock (from torch>=2->torchdata>=0.5.0->dgl)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch>=2->torchdata>=0.5.0->dgl)\n",
      "  Downloading typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=2->torchdata>=0.5.0->dgl)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting jinja2 (from torch>=2->torchdata>=0.5.0->dgl)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch>=2->torchdata>=0.5.0->dgl)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=2->torchdata>=0.5.0->dgl)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=2->torchdata>=0.5.0->dgl)\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Downloading dgl-2.2.1-cp310-cp310-win_amd64.whl (5.3 MB)\n",
      "   ---------------------------------------- 0.0/5.3 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/5.3 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.8/5.3 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 2.6/5.3 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.4/5.3 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.2/5.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.0/5.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.3/5.3 MB 4.1 MB/s eta 0:00:00\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Downloading numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.9 MB 2.8 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.3/12.9 MB 3.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.1/12.9 MB 3.7 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.6/12.9 MB 3.4 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 3.4/12.9 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 4.2/12.9 MB 3.5 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 4.7/12.9 MB 3.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 5.5/12.9 MB 3.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 6.6/12.9 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 7.3/12.9 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 8.4/12.9 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 9.4/12.9 MB 4.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.7/12.9 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.1/12.9 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 4.4 MB/s eta 0:00:00\n",
      "Downloading psutil-7.0.0-cp37-abi3-win_amd64.whl (244 kB)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Downloading scipy-1.15.3-cp310-cp310-win_amd64.whl (41.3 MB)\n",
      "   ---------------------------------------- 0.0/41.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/41.3 MB 7.2 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 2.4/41.3 MB 6.4 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 3.7/41.3 MB 6.2 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 5.0/41.3 MB 6.0 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 6.3/41.3 MB 5.9 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 7.9/41.3 MB 6.2 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 9.2/41.3 MB 6.3 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 9.7/41.3 MB 5.7 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 11.0/41.3 MB 5.8 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 12.3/41.3 MB 5.8 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 13.6/41.3 MB 5.8 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 14.7/41.3 MB 5.7 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 16.3/41.3 MB 5.8 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 17.8/41.3 MB 5.9 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 19.1/41.3 MB 5.9 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 20.4/41.3 MB 5.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 21.5/41.3 MB 5.9 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 22.5/41.3 MB 5.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 24.1/41.3 MB 5.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 25.4/41.3 MB 5.9 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 26.5/41.3 MB 5.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 27.8/41.3 MB 5.9 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 29.1/41.3 MB 5.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 30.4/41.3 MB 5.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 31.7/41.3 MB 6.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 33.3/41.3 MB 6.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 34.3/41.3 MB 6.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 35.4/41.3 MB 6.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 36.7/41.3 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 37.5/41.3 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 39.1/41.3 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.6/41.3 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.2/41.3 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.3/41.3 MB 5.8 MB/s eta 0:00:00\n",
      "Downloading torchdata-0.11.0-py3-none-any.whl (61 kB)\n",
      "Downloading pandas-2.3.0-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/11.1 MB 7.4 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.9/11.1 MB 7.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.5/11.1 MB 7.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.8/11.1 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.8/11.1 MB 7.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.1/11.1 MB 6.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.2/11.1 MB 6.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.0/11.1 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 6.0 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp310-cp310-win_amd64.whl (105 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading torch-2.7.1-cp310-cp310-win_amd64.whl (216.1 MB)\n",
      "   ---------------------------------------- 0.0/216.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/216.1 MB 3.7 MB/s eta 0:00:58\n",
      "   ---------------------------------------- 2.1/216.1 MB 5.3 MB/s eta 0:00:41\n",
      "    --------------------------------------- 2.9/216.1 MB 4.8 MB/s eta 0:00:45\n",
      "    --------------------------------------- 4.2/216.1 MB 5.0 MB/s eta 0:00:43\n",
      "    --------------------------------------- 5.2/216.1 MB 5.1 MB/s eta 0:00:42\n",
      "   - -------------------------------------- 7.1/216.1 MB 5.7 MB/s eta 0:00:37\n",
      "   - -------------------------------------- 8.1/216.1 MB 5.7 MB/s eta 0:00:37\n",
      "   - -------------------------------------- 8.9/216.1 MB 5.5 MB/s eta 0:00:38\n",
      "   - -------------------------------------- 10.0/216.1 MB 5.6 MB/s eta 0:00:37\n",
      "   -- ------------------------------------- 11.3/216.1 MB 5.4 MB/s eta 0:00:39\n",
      "   -- ------------------------------------- 12.1/216.1 MB 5.3 MB/s eta 0:00:39\n",
      "   -- ------------------------------------- 13.4/216.1 MB 5.5 MB/s eta 0:00:37\n",
      "   -- ------------------------------------- 13.4/216.1 MB 5.5 MB/s eta 0:00:37\n",
      "   -- ------------------------------------- 14.4/216.1 MB 5.0 MB/s eta 0:00:41\n",
      "   -- ------------------------------------- 15.5/216.1 MB 5.1 MB/s eta 0:00:40\n",
      "   --- ------------------------------------ 16.3/216.1 MB 4.9 MB/s eta 0:00:41\n",
      "   --- ------------------------------------ 16.5/216.1 MB 4.9 MB/s eta 0:00:41\n",
      "   --- ------------------------------------ 17.6/216.1 MB 4.8 MB/s eta 0:00:42\n",
      "   --- ------------------------------------ 18.1/216.1 MB 4.6 MB/s eta 0:00:43\n",
      "   --- ------------------------------------ 19.1/216.1 MB 4.6 MB/s eta 0:00:43\n",
      "   --- ------------------------------------ 20.2/216.1 MB 4.6 MB/s eta 0:00:43\n",
      "   --- ------------------------------------ 21.2/216.1 MB 4.7 MB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 22.3/216.1 MB 4.6 MB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 22.8/216.1 MB 4.5 MB/s eta 0:00:43\n",
      "   ---- ----------------------------------- 23.1/216.1 MB 4.4 MB/s eta 0:00:44\n",
      "   ---- ----------------------------------- 24.1/216.1 MB 4.5 MB/s eta 0:00:44\n",
      "   ---- ----------------------------------- 24.9/216.1 MB 4.4 MB/s eta 0:00:44\n",
      "   ---- ----------------------------------- 24.9/216.1 MB 4.4 MB/s eta 0:00:44\n",
      "   ---- ----------------------------------- 25.4/216.1 MB 4.2 MB/s eta 0:00:46\n",
      "   ---- ----------------------------------- 26.5/216.1 MB 4.2 MB/s eta 0:00:45\n",
      "   ----- ---------------------------------- 27.5/216.1 MB 4.3 MB/s eta 0:00:44\n",
      "   ----- ---------------------------------- 28.6/216.1 MB 4.2 MB/s eta 0:00:45\n",
      "   ----- ---------------------------------- 30.1/216.1 MB 4.3 MB/s eta 0:00:43\n",
      "   ----- ---------------------------------- 31.5/216.1 MB 4.4 MB/s eta 0:00:42\n",
      "   ------ --------------------------------- 32.5/216.1 MB 4.4 MB/s eta 0:00:42\n",
      "   ------ --------------------------------- 33.8/216.1 MB 4.5 MB/s eta 0:00:41\n",
      "   ------ --------------------------------- 34.6/216.1 MB 4.5 MB/s eta 0:00:41\n",
      "   ------ --------------------------------- 35.4/216.1 MB 4.4 MB/s eta 0:00:41\n",
      "   ------ --------------------------------- 36.7/216.1 MB 4.5 MB/s eta 0:00:41\n",
      "   ------- -------------------------------- 38.0/216.1 MB 4.5 MB/s eta 0:00:40\n",
      "   ------- -------------------------------- 39.1/216.1 MB 4.5 MB/s eta 0:00:40\n",
      "   ------- -------------------------------- 40.1/216.1 MB 4.5 MB/s eta 0:00:39\n",
      "   ------- -------------------------------- 40.9/216.1 MB 4.5 MB/s eta 0:00:39\n",
      "   ------- -------------------------------- 41.7/216.1 MB 4.5 MB/s eta 0:00:39\n",
      "   ------- -------------------------------- 43.0/216.1 MB 4.5 MB/s eta 0:00:39\n",
      "   -------- ------------------------------- 43.5/216.1 MB 4.5 MB/s eta 0:00:39\n",
      "   -------- ------------------------------- 43.8/216.1 MB 4.4 MB/s eta 0:00:39\n",
      "   -------- ------------------------------- 44.6/216.1 MB 4.4 MB/s eta 0:00:39\n",
      "   -------- ------------------------------- 45.6/216.1 MB 4.4 MB/s eta 0:00:39\n",
      "   -------- ------------------------------- 46.9/216.1 MB 4.5 MB/s eta 0:00:38\n",
      "   -------- ------------------------------- 48.2/216.1 MB 4.5 MB/s eta 0:00:38\n",
      "   --------- ------------------------------ 49.3/216.1 MB 4.5 MB/s eta 0:00:38\n",
      "   --------- ------------------------------ 50.6/216.1 MB 4.5 MB/s eta 0:00:37\n",
      "   --------- ------------------------------ 51.9/216.1 MB 4.6 MB/s eta 0:00:37\n",
      "   --------- ------------------------------ 53.2/216.1 MB 4.6 MB/s eta 0:00:36\n",
      "   ---------- ----------------------------- 54.5/216.1 MB 4.6 MB/s eta 0:00:35\n",
      "   ---------- ----------------------------- 55.6/216.1 MB 4.7 MB/s eta 0:00:35\n",
      "   ---------- ----------------------------- 56.1/216.1 MB 4.6 MB/s eta 0:00:35\n",
      "   ---------- ----------------------------- 57.1/216.1 MB 4.6 MB/s eta 0:00:35\n",
      "   ---------- ----------------------------- 58.7/216.1 MB 4.7 MB/s eta 0:00:34\n",
      "   ----------- ---------------------------- 59.8/216.1 MB 4.7 MB/s eta 0:00:34\n",
      "   ----------- ---------------------------- 60.3/216.1 MB 4.7 MB/s eta 0:00:34\n",
      "   ----------- ---------------------------- 61.9/216.1 MB 4.7 MB/s eta 0:00:33\n",
      "   ----------- ---------------------------- 62.9/216.1 MB 4.7 MB/s eta 0:00:33\n",
      "   ----------- ---------------------------- 63.7/216.1 MB 4.7 MB/s eta 0:00:33\n",
      "   ----------- ---------------------------- 64.5/216.1 MB 4.7 MB/s eta 0:00:33\n",
      "   ------------ --------------------------- 65.8/216.1 MB 4.7 MB/s eta 0:00:33\n",
      "   ------------ --------------------------- 67.1/216.1 MB 4.7 MB/s eta 0:00:32\n",
      "   ------------ --------------------------- 68.4/216.1 MB 4.7 MB/s eta 0:00:32\n",
      "   ------------ --------------------------- 70.0/216.1 MB 4.8 MB/s eta 0:00:31\n",
      "   ------------- -------------------------- 71.3/216.1 MB 4.8 MB/s eta 0:00:31\n",
      "   ------------- -------------------------- 72.9/216.1 MB 4.8 MB/s eta 0:00:30\n",
      "   ------------- -------------------------- 74.4/216.1 MB 4.9 MB/s eta 0:00:30\n",
      "   ------------- -------------------------- 75.5/216.1 MB 4.9 MB/s eta 0:00:29\n",
      "   -------------- ------------------------- 77.1/216.1 MB 4.9 MB/s eta 0:00:29\n",
      "   -------------- ------------------------- 78.6/216.1 MB 4.9 MB/s eta 0:00:28\n",
      "   -------------- ------------------------- 79.7/216.1 MB 4.9 MB/s eta 0:00:28\n",
      "   -------------- ------------------------- 80.7/216.1 MB 4.9 MB/s eta 0:00:28\n",
      "   --------------- ------------------------ 82.1/216.1 MB 5.0 MB/s eta 0:00:28\n",
      "   --------------- ------------------------ 83.4/216.1 MB 5.0 MB/s eta 0:00:27\n",
      "   --------------- ------------------------ 84.4/216.1 MB 5.0 MB/s eta 0:00:27\n",
      "   --------------- ------------------------ 85.7/216.1 MB 5.0 MB/s eta 0:00:27\n",
      "   ---------------- ----------------------- 86.8/216.1 MB 5.0 MB/s eta 0:00:26\n",
      "   ---------------- ----------------------- 88.1/216.1 MB 5.0 MB/s eta 0:00:26\n",
      "   ---------------- ----------------------- 89.1/216.1 MB 5.0 MB/s eta 0:00:26\n",
      "   ---------------- ----------------------- 90.4/216.1 MB 5.0 MB/s eta 0:00:25\n",
      "   ---------------- ----------------------- 91.5/216.1 MB 5.0 MB/s eta 0:00:25\n",
      "   ----------------- ---------------------- 92.3/216.1 MB 5.0 MB/s eta 0:00:25\n",
      "   ----------------- ---------------------- 93.1/216.1 MB 5.0 MB/s eta 0:00:25\n",
      "   ----------------- ---------------------- 93.8/216.1 MB 5.0 MB/s eta 0:00:25\n",
      "   ----------------- ---------------------- 94.9/216.1 MB 5.0 MB/s eta 0:00:25\n",
      "   ----------------- ---------------------- 95.9/216.1 MB 5.0 MB/s eta 0:00:25\n",
      "   ----------------- ---------------------- 96.7/216.1 MB 5.0 MB/s eta 0:00:24\n",
      "   ------------------ --------------------- 98.3/216.1 MB 5.0 MB/s eta 0:00:24\n",
      "   ------------------ --------------------- 99.6/216.1 MB 5.0 MB/s eta 0:00:24\n",
      "   ------------------ --------------------- 100.7/216.1 MB 5.0 MB/s eta 0:00:23\n",
      "   ------------------ --------------------- 102.0/216.1 MB 5.0 MB/s eta 0:00:23\n",
      "   ------------------- -------------------- 102.8/216.1 MB 5.0 MB/s eta 0:00:23\n",
      "   ------------------- -------------------- 103.8/216.1 MB 5.0 MB/s eta 0:00:23\n",
      "   ------------------- -------------------- 104.9/216.1 MB 5.0 MB/s eta 0:00:23\n",
      "   ------------------- -------------------- 105.6/216.1 MB 5.0 MB/s eta 0:00:22\n",
      "   ------------------- -------------------- 106.4/216.1 MB 5.0 MB/s eta 0:00:22\n",
      "   ------------------- -------------------- 107.7/216.1 MB 5.0 MB/s eta 0:00:22\n",
      "   -------------------- ------------------- 108.3/216.1 MB 5.0 MB/s eta 0:00:22\n",
      "   -------------------- ------------------- 109.1/216.1 MB 5.0 MB/s eta 0:00:22\n",
      "   -------------------- ------------------- 109.6/216.1 MB 5.0 MB/s eta 0:00:22\n",
      "   -------------------- ------------------- 110.1/216.1 MB 5.0 MB/s eta 0:00:22\n",
      "   -------------------- ------------------- 110.9/216.1 MB 4.9 MB/s eta 0:00:22\n",
      "   -------------------- ------------------- 112.2/216.1 MB 4.9 MB/s eta 0:00:22\n",
      "   -------------------- ------------------- 112.7/216.1 MB 4.9 MB/s eta 0:00:21\n",
      "   -------------------- ------------------- 113.2/216.1 MB 4.9 MB/s eta 0:00:21\n",
      "   --------------------- ------------------ 114.0/216.1 MB 4.9 MB/s eta 0:00:21\n",
      "   --------------------- ------------------ 114.8/216.1 MB 4.9 MB/s eta 0:00:21\n",
      "   --------------------- ------------------ 115.3/216.1 MB 4.9 MB/s eta 0:00:21\n",
      "   --------------------- ------------------ 116.1/216.1 MB 4.9 MB/s eta 0:00:21\n",
      "   --------------------- ------------------ 117.2/216.1 MB 4.9 MB/s eta 0:00:21\n",
      "   --------------------- ------------------ 118.2/216.1 MB 4.8 MB/s eta 0:00:21\n",
      "   ---------------------- ----------------- 119.0/216.1 MB 4.8 MB/s eta 0:00:21\n",
      "   ---------------------- ----------------- 119.8/216.1 MB 4.9 MB/s eta 0:00:20\n",
      "   ---------------------- ----------------- 120.3/216.1 MB 4.8 MB/s eta 0:00:20\n",
      "   ---------------------- ----------------- 121.1/216.1 MB 4.8 MB/s eta 0:00:20\n",
      "   ---------------------- ----------------- 122.4/216.1 MB 4.8 MB/s eta 0:00:20\n",
      "   ---------------------- ----------------- 123.7/216.1 MB 4.8 MB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 125.0/216.1 MB 4.8 MB/s eta 0:00:19\n",
      "   ----------------------- ---------------- 125.8/216.1 MB 4.8 MB/s eta 0:00:19\n",
      "   ----------------------- ---------------- 126.4/216.1 MB 4.8 MB/s eta 0:00:19\n",
      "   ----------------------- ---------------- 127.1/216.1 MB 4.8 MB/s eta 0:00:19\n",
      "   ----------------------- ---------------- 127.9/216.1 MB 4.8 MB/s eta 0:00:19\n",
      "   ----------------------- ---------------- 129.0/216.1 MB 4.8 MB/s eta 0:00:19\n",
      "   ------------------------ --------------- 130.0/216.1 MB 4.8 MB/s eta 0:00:18\n",
      "   ------------------------ --------------- 131.3/216.1 MB 4.8 MB/s eta 0:00:18\n",
      "   ------------------------ --------------- 132.6/216.1 MB 4.8 MB/s eta 0:00:18\n",
      "   ------------------------ --------------- 134.2/216.1 MB 4.8 MB/s eta 0:00:17\n",
      "   ------------------------- -------------- 135.8/216.1 MB 4.9 MB/s eta 0:00:17\n",
      "   ------------------------- -------------- 137.4/216.1 MB 4.9 MB/s eta 0:00:17\n",
      "   ------------------------- -------------- 137.9/216.1 MB 4.9 MB/s eta 0:00:17\n",
      "   ------------------------- -------------- 139.2/216.1 MB 4.9 MB/s eta 0:00:16\n",
      "   ------------------------- -------------- 140.2/216.1 MB 4.9 MB/s eta 0:00:16\n",
      "   -------------------------- ------------- 141.6/216.1 MB 4.9 MB/s eta 0:00:16\n",
      "   -------------------------- ------------- 142.9/216.1 MB 4.9 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 144.2/216.1 MB 4.9 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 145.2/216.1 MB 4.9 MB/s eta 0:00:15\n",
      "   --------------------------- ------------ 146.3/216.1 MB 4.9 MB/s eta 0:00:15\n",
      "   --------------------------- ------------ 147.3/216.1 MB 4.9 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 148.1/216.1 MB 4.9 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 149.4/216.1 MB 4.9 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 149.9/216.1 MB 4.9 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 151.0/216.1 MB 4.9 MB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 152.0/216.1 MB 4.9 MB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 152.8/216.1 MB 4.9 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 153.9/216.1 MB 4.9 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 154.4/216.1 MB 4.9 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 155.7/216.1 MB 4.9 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 156.5/216.1 MB 4.9 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 157.0/216.1 MB 4.8 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 157.8/216.1 MB 4.8 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 158.9/216.1 MB 4.9 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 159.6/216.1 MB 4.9 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 160.7/216.1 MB 4.8 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 161.7/216.1 MB 4.8 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 162.8/216.1 MB 4.9 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 163.8/216.1 MB 4.9 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 164.9/216.1 MB 4.9 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 166.5/216.1 MB 4.9 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 167.5/216.1 MB 4.9 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 168.8/216.1 MB 4.9 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 170.1/216.1 MB 4.9 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 171.4/216.1 MB 5.0 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 172.5/216.1 MB 5.0 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 173.8/216.1 MB 5.0 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 174.9/216.1 MB 5.0 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 176.2/216.1 MB 5.1 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 177.5/216.1 MB 5.1 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 178.8/216.1 MB 5.1 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 180.1/216.1 MB 5.1 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 181.4/216.1 MB 5.1 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 182.5/216.1 MB 5.1 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 183.8/216.1 MB 5.1 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 185.1/216.1 MB 5.1 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 186.4/216.1 MB 5.1 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 187.4/216.1 MB 5.1 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 188.7/216.1 MB 5.1 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 190.1/216.1 MB 5.1 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 191.4/216.1 MB 5.1 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 192.4/216.1 MB 5.1 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 194.0/216.1 MB 5.1 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 195.0/216.1 MB 5.2 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 196.3/216.1 MB 5.2 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 197.7/216.1 MB 5.2 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 199.0/216.1 MB 5.2 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 200.3/216.1 MB 5.2 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 201.3/216.1 MB 5.2 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 202.6/216.1 MB 5.3 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 203.9/216.1 MB 5.3 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 205.0/216.1 MB 5.3 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 206.6/216.1 MB 5.3 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 207.6/216.1 MB 5.3 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 208.9/216.1 MB 5.3 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 210.5/216.1 MB 5.3 MB/s eta 0:00:02\n",
      "   ---------------------------------------  211.8/216.1 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  212.6/216.1 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  213.6/216.1 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  213.9/216.1 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  214.7/216.1 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  216.0/216.1 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  216.0/216.1 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 216.1/216.1 MB 5.2 MB/s eta 0:00:00\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 1.3/6.3 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.4/6.3 MB 6.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.4/6.3 MB 5.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 5.5 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 5.8 MB/s eta 0:00:00\n",
      "Installing collected packages: pytz, mpmath, urllib3, tzdata, typing-extensions, sympy, six, psutil, numpy, networkx, MarkupSafe, idna, fsspec, filelock, colorama, charset_normalizer, certifi, tqdm, scipy, requests, python-dateutil, jinja2, torch, pandas, torchdata, dgl\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2025.2\n",
      "    Uninstalling pytz-2025.2:\n",
      "      Successfully uninstalled pytz-2025.2\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.3.0\n",
      "    Uninstalling mpmath-1.3.0:\n",
      "      Successfully uninstalled mpmath-1.3.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.13\n",
      "    Uninstalling urllib3-1.26.13:\n",
      "      Successfully uninstalled urllib3-1.26.13\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2025.2\n",
      "    Uninstalling tzdata-2025.2:\n",
      "      Successfully uninstalled tzdata-2025.2\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.13.1\n",
      "    Uninstalling typing_extensions-4.13.1:\n",
      "      Successfully uninstalled typing_extensions-4.13.1\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.17.0\n",
      "    Uninstalling six-1.17.0:\n",
      "      Successfully uninstalled six-1.17.0\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 7.0.0\n",
      "    Uninstalling psutil-7.0.0:\n",
      "      Successfully uninstalled psutil-7.0.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.2\n",
      "    Uninstalling numpy-2.1.2:\n",
      "      Successfully uninstalled numpy-2.1.2\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.4.2\n",
      "    Uninstalling networkx-3.4.2:\n",
      "      Successfully uninstalled networkx-3.4.2\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.5\n",
      "    Uninstalling MarkupSafe-2.1.5:\n",
      "      Successfully uninstalled MarkupSafe-2.1.5\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.13.1\n",
      "    Uninstalling filelock-3.13.1:\n",
      "      Successfully uninstalled filelock-3.13.1\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.6\n",
      "    Uninstalling colorama-0.4.6:\n",
      "      Successfully uninstalled colorama-0.4.6\n",
      "  Attempting uninstall: charset_normalizer\n",
      "    Found existing installation: charset-normalizer 2.1.1\n",
      "    Uninstalling charset-normalizer-2.1.1:\n",
      "      Successfully uninstalled charset-normalizer-2.1.1\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2022.12.7\n",
      "    Uninstalling certifi-2022.12.7:\n",
      "      Successfully uninstalled certifi-2022.12.7\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.2\n",
      "    Uninstalling scipy-1.15.2:\n",
      "      Successfully uninstalled scipy-1.15.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.1\n",
      "    Uninstalling requests-2.28.1:\n",
      "      Successfully uninstalled requests-2.28.1\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.4\n",
      "    Uninstalling Jinja2-3.1.4:\n",
      "      Successfully uninstalled Jinja2-3.1.4\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.0+cu118\n",
      "    Uninstalling torch-2.2.0+cu118:\n",
      "      Successfully uninstalled torch-2.2.0+cu118\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Uninstalling pandas-2.2.3:\n",
      "      Successfully uninstalled pandas-2.2.3\n",
      "  Attempting uninstall: torchdata\n",
      "    Found existing installation: torchdata 0.11.0\n",
      "    Uninstalling torchdata-0.11.0:\n",
      "      Successfully uninstalled torchdata-0.11.0\n",
      "  Attempting uninstall: dgl\n",
      "    Found existing installation: dgl 2.2.1+cu121\n",
      "    Uninstalling dgl-2.2.1+cu121:\n",
      "      Successfully uninstalled dgl-2.2.1+cu121\n",
      "Successfully installed MarkupSafe-3.0.2 certifi-2025.6.15 charset_normalizer-3.4.2 colorama-0.4.6 dgl-2.2.1 filelock-3.18.0 fsspec-2025.5.1 idna-3.10 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.6 pandas-2.3.0 psutil-7.0.0 python-dateutil-2.9.0.post0 pytz-2025.2 requests-2.32.4 scipy-1.15.3 six-1.17.0 sympy-1.14.0 torch-2.7.1 torchdata-0.11.0 tqdm-4.67.1 typing-extensions-4.14.0 tzdata-2025.2 urllib3-2.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch-geometric-temporal 0.54.0 requires pandas<=1.3.5, but you have pandas 2.3.0 which is incompatible.\n",
      "torchaudio 2.2.0+cu118 requires torch==2.2.0+cu118, but you have torch 2.7.1 which is incompatible.\n",
      "torchvision 0.17.0+cu118 requires torch==2.2.0+cu118, but you have torch 2.7.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall dgl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3f03bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\marti\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "563e0551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: dgl 2.2.1\n",
      "Uninstalling dgl-2.2.1:\n",
      "  Successfully uninstalled dgl-2.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 180, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_internal\\commands\\uninstall.py\", line 110, in run\n",
      "    uninstall_pathset.commit()\n",
      "  File \"c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_internal\\req\\req_uninstall.py\", line 432, in commit\n",
      "    self._moved_paths.commit()\n",
      "  File \"c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_internal\\req\\req_uninstall.py\", line 278, in commit\n",
      "    save_dir.cleanup()\n",
      "  File \"c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_internal\\utils\\temp_dir.py\", line 173, in cleanup\n",
      "    rmtree(self._path)\n",
      "  File \"c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\tenacity\\__init__.py\", line 291, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\tenacity\\__init__.py\", line 381, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\tenacity\\__init__.py\", line 327, in iter\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\tenacity\\__init__.py\", line 160, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\tenacity\\__init__.py\", line 384, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_internal\\utils\\misc.py\", line 128, in rmtree\n",
      "    shutil.rmtree(dir, ignore_errors=ignore_errors, onexc=rmtree_errorhandler)\n",
      "  File \"c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\shutil.py\", line 796, in rmtree\n",
      "    return _rmtree_unsafe(path, onexc)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\shutil.py\", line 636, in _rmtree_unsafe\n",
      "    onexc(os.unlink, fullname, err)\n",
      "  File \"c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\shutil.py\", line 634, in _rmtree_unsafe\n",
      "    os.unlink(fullname)\n",
      "PermissionError: [WinError 5] Acceso denegado: 'C:\\\\Users\\\\marti\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\~gl\\\\dgl.dll'\n"
     ]
    }
   ],
   "source": [
    "pip uninstall dgl -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "084e56ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dgl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# DGL is already installed in a previous cell using `!pip install --upgrade --force-reinstall dgl`\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdgl\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dgl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "# DGL is already installed in a previous cell using `!pip install --upgrade --force-reinstall dgl`\n",
    "import dgl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dgl.nn import GraphConv\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30bd63fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fused_nodes.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Cargar datasets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m nodes_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused_nodes.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m edges_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused_edges.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Mapear nodos a √≠ndices\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fused_nodes.csv'"
     ]
    }
   ],
   "source": [
    "# Cargar datasets\n",
    "nodes_df = pd.read_csv(\"fused_nodes.csv\")\n",
    "edges_df = pd.read_csv(\"fused_edges.csv\")\n",
    "\n",
    "# Mapear nodos a √≠ndices\n",
    "node_ids = {name: idx for idx, name in enumerate(nodes_df['id'].unique())}\n",
    "nodes_df['idx'] = nodes_df['id'].map(node_ids)\n",
    "\n",
    "# Codificar la clase\n",
    "le = LabelEncoder()\n",
    "nodes_df['label'] = le.fit_transform(nodes_df['Abnormality class'].astype(str))\n",
    "\n",
    "# Crear features\n",
    "feature_cols = ['success_rate', 'error_rate', 'request_rate', 'throughput', 'pagerank', 'motif_1', 'motif_2', 'motif_3']\n",
    "feature_cols = [col for col in feature_cols if col in nodes_df.columns]\n",
    "X = nodes_df[feature_cols].fillna(0)\n",
    "X = torch.tensor(StandardScaler().fit_transform(X), dtype=torch.float32)\n",
    "y = torch.tensor(nodes_df['label'].values, dtype=torch.long)\n",
    "\n",
    "# Crear aristas\n",
    "edges_df = edges_df.dropna(subset=[\"src\", \"dst\"])\n",
    "edges_df = edges_df[edges_df[\"src\"].isin(node_ids) & edges_df[\"dst\"].isin(node_ids)]\n",
    "src = edges_df[\"src\"].map(node_ids).tolist()\n",
    "dst = edges_df[\"dst\"].map(node_ids).tolist()\n",
    "\n",
    "# Crear grafo\n",
    "g = dgl.graph((src, dst), num_nodes=len(nodes_df))\n",
    "g.ndata['feat'] = X\n",
    "g.ndata['label'] = y\n",
    "\n",
    "\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, h_feats)\n",
    "        self.classify = nn.Linear(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        h = F.relu(self.conv1(g, x))\n",
    "        h = F.dropout(h, 0.3, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        return self.classify(h)\n",
    "\n",
    "\n",
    "# Split en train/test\n",
    "idx = list(range(len(nodes_df)))\n",
    "train_idx, test_idx = train_test_split(idx, stratify=y.numpy(), test_size=0.3, random_state=42)\n",
    "\n",
    "train_mask = torch.zeros(len(nodes_df), dtype=torch.bool)\n",
    "test_mask = torch.zeros(len(nodes_df), dtype=torch.bool)\n",
    "train_mask[train_idx] = True\n",
    "test_mask[test_idx] = True\n",
    "\n",
    "# Instanciar modelo\n",
    "model = GNNModel(in_feats=X.shape[1], h_feats=64, num_classes=len(le.classes_))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Entrenar\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    out = model(g, g.ndata['feat'])\n",
    "    loss = loss_fn(out[train_mask], y[train_mask])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch} | Loss {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(g, g.ndata['feat'])[test_mask].argmax(dim=1)\n",
    "    print(\"Accuracy:\", accuracy_score(y[test_mask].numpy(), pred.numpy()))\n",
    "    print(classification_report(y[test_mask].numpy(), pred.numpy(), target_names=le.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
